{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9cf6cd9",
   "metadata": {},
   "source": [
    "# Complete MLflow Tutorial: Learn Machine Learning Experiment Tracking\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "By the end of this notebook, you'll understand:\n",
    "- How to set up and use MLflow for experiment tracking\n",
    "- How to log parameters, metrics, and artifacts\n",
    "- How to manage model versions with MLflow Model Registry\n",
    "- How to compare experiments and make data-driven decisions\n",
    "- Best practices for MLflow in production workflows\n",
    "\n",
    "## ğŸ“– What is MLflow?\n",
    "MLflow is an open-source platform for managing the machine learning lifecycle, including:\n",
    "- **Tracking**: Record and query experiments (code, data, config, results)\n",
    "- **Projects**: Package data science code in a reusable, reproducible form\n",
    "- **Models**: Manage and deploy models from various ML libraries\n",
    "- **Registry**: Centralized model store for collaborative model management\n",
    "\n",
    "Let's dive into each component step by step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e454db8",
   "metadata": {},
   "source": [
    "## 1. Import MLflow and Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries. Each import serves a specific purpose in our ML workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e96648e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Farhad/Documents/ML_project/MLFLOW/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n",
      "MLflow version: 3.1.4\n"
     ]
    }
   ],
   "source": [
    "# MLflow - Main library for experiment tracking and model management\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature  # For automatically inferring model signatures\n",
    "from mlflow.tracking import MlflowClient    # Client for programmatic access to MLflow\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd                         # For data manipulation and analysis\n",
    "import numpy as np                          # For numerical operations\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn import datasets               # For loading sample datasets\n",
    "from sklearn.model_selection import train_test_split  # For splitting data\n",
    "from sklearn.linear_model import LogisticRegression   # Our ML model\n",
    "from sklearn.ensemble import RandomForestClassifier   # Alternative model for comparison\n",
    "from sklearn.metrics import (               # For calculating model performance metrics\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt             # For creating plots\n",
    "import seaborn as sns                       # For advanced statistical visualizations\n",
    "\n",
    "# System and utility libraries\n",
    "import os                                   # For file system operations\n",
    "import warnings                             # For handling warnings\n",
    "warnings.filterwarnings('ignore')          # Suppress warnings for cleaner output\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(f\"MLflow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58034a9",
   "metadata": {},
   "source": [
    "## 2. Set Up MLflow Tracking Server Connection\n",
    "\n",
    "MLflow can store tracking data in different locations:\n",
    "- **Local filesystem**: `file:./mlruns` (default)\n",
    "- **Database**: `sqlite:///mlflow.db` or `postgresql://...`\n",
    "- **Remote server**: `http://localhost:5000` or `http://your-server:5000`\n",
    "\n",
    "We'll configure our setup to connect to a local MLflow tracking server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "427be4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— MLflow tracking URI: file:./mlruns\n",
      "âœ… MLflow tracking setup complete!\n",
      "ğŸ“Š Found 1 existing experiments\n",
      "   - iris-classification-tutorial (ID: 636605946707093340)\n"
     ]
    }
   ],
   "source": [
    "# Set the MLflow tracking URI\n",
    "# This tells MLflow where to store experiment data and artifacts\n",
    "# Options:\n",
    "# 1. Local file system: \"file:./mlruns\" \n",
    "# 2. Local server: \"http://127.0.0.1:8080\"\n",
    "# 3. Remote server: \"http://your-mlflow-server.com:5000\"\n",
    "\n",
    "# For this tutorial, we'll use local file-based tracking for simplicity\n",
    "# You can change this to \"http://127.0.0.1:8080\" if you have the server running\n",
    "tracking_uri = \"file:./mlruns\"\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "\n",
    "# Verify the tracking URI is set correctly\n",
    "current_uri = mlflow.get_tracking_uri()\n",
    "print(f\"ğŸ”— MLflow tracking URI: {current_uri}\")\n",
    "\n",
    "# Create an MLflow client for programmatic access\n",
    "# This allows us to interact with MLflow programmatically\n",
    "client = MlflowClient()\n",
    "\n",
    "print(\"âœ… MLflow tracking setup complete!\")\n",
    "\n",
    "# Let's check if we can connect and list any existing experiments\n",
    "try:\n",
    "    # Use search_experiments() instead of list_experiments() for newer MLflow versions\n",
    "    experiments = client.search_experiments()\n",
    "    print(f\"ğŸ“Š Found {len(experiments)} existing experiments\")\n",
    "    for exp in experiments:\n",
    "        print(f\"   - {exp.name} (ID: {exp.experiment_id})\")\n",
    "except AttributeError:\n",
    "    # Fallback for older MLflow versions\n",
    "    try:\n",
    "        experiments = client.list_experiments()\n",
    "        print(f\"ğŸ“Š Found {len(experiments)} existing experiments\")\n",
    "        for exp in experiments:\n",
    "            print(f\"   - {exp.name} (ID: {exp.experiment_id})\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Note: Could not list experiments: {e}\")\n",
    "        print(\"   This is normal if no experiments exist yet or if using file-based tracking.\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Note: Could not connect to MLflow: {e}\")\n",
    "    print(\"   This is normal if no experiments exist yet or if using file-based tracking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4f69f1",
   "metadata": {},
   "source": [
    "## 3. Create and Start an MLflow Experiment\n",
    "\n",
    "**Experiments** in MLflow are containers for organizing related runs. Think of them as project folders where you group related model training attempts.\n",
    "\n",
    "### Best Practices for Experiment Naming:\n",
    "- Use descriptive names: `iris-classification-comparison`\n",
    "- Include version or date: `model-v2-2024`\n",
    "- Separate by model type: `logistic-regression-experiments`\n",
    "- Group by business objective: `customer-churn-prediction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ccf4761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Experiment: 'iris-classification-tutorial'\n",
      "ğŸ“ Experiment ID: 636605946707093340\n",
      "ğŸ“‚ Artifact Location: file:///Users/Farhad/Documents/ML_project/MLFLOW/mlruns/636605946707093340\n",
      "ğŸ·ï¸  Tag: project = ml-tutorial\n",
      "ğŸ·ï¸  Tag: dataset = iris\n",
      "ğŸ·ï¸  Tag: task = classification\n",
      "ğŸ·ï¸  Tag: team = data-science\n",
      "ğŸ·ï¸  Tag: priority = learning\n",
      "\n",
      "âœ… Experiment setup complete!\n",
      "ğŸ’¡ All runs in this notebook will be tracked under experiment: 'iris-classification-tutorial'\n"
     ]
    }
   ],
   "source": [
    "# Define experiment name with descriptive naming\n",
    "experiment_name = \"iris-classification-tutorial\"\n",
    "\n",
    "# Create or get the experiment\n",
    "# mlflow.set_experiment() either:\n",
    "# 1. Creates a new experiment if it doesn't exist\n",
    "# 2. Sets the active experiment if it already exists\n",
    "experiment = mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"ğŸ§ª Experiment: '{experiment_name}'\")\n",
    "print(f\"ğŸ“ Experiment ID: {experiment.experiment_id}\")\n",
    "print(f\"ğŸ“‚ Artifact Location: {experiment.artifact_location}\")\n",
    "\n",
    "# You can also create experiments with additional metadata\n",
    "# This is useful for providing context and description\n",
    "experiment_description = \"\"\"\n",
    "This experiment focuses on comparing different machine learning models \n",
    "for Iris flower classification. We'll track various algorithms, \n",
    "hyperparameters, and performance metrics to find the best approach.\n",
    "\n",
    "Dataset: Iris flower dataset (150 samples, 4 features, 3 classes)\n",
    "Goal: Achieve highest accuracy while maintaining good generalization\n",
    "\"\"\"\n",
    "\n",
    "# Add tags to categorize and organize experiments\n",
    "experiment_tags = {\n",
    "    \"project\": \"ml-tutorial\",\n",
    "    \"dataset\": \"iris\",\n",
    "    \"task\": \"classification\",\n",
    "    \"team\": \"data-science\",\n",
    "    \"priority\": \"learning\"\n",
    "}\n",
    "\n",
    "# Set experiment tags (metadata for organization)\n",
    "for key, value in experiment_tags.items():\n",
    "    mlflow.set_experiment_tag(key, value)\n",
    "    print(f\"ğŸ·ï¸  Tag: {key} = {value}\")\n",
    "\n",
    "print(\"\\nâœ… Experiment setup complete!\")\n",
    "print(f\"ğŸ’¡ All runs in this notebook will be tracked under experiment: '{experiment_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f190eb82",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Data\n",
    "\n",
    "Before we start experimenting with models, let's load and explore our dataset. We'll use the classic Iris dataset for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bc0c2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Loading Iris dataset...\n",
      "ğŸ“ˆ Dataset shape: (150, 4)\n",
      "ğŸ¯ Target classes: ['setosa' 'versicolor' 'virginica']\n",
      "ğŸ“‹ Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "\n",
      "ğŸ“‹ First 5 rows of the dataset:\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "  species  \n",
      "0  setosa  \n",
      "1  setosa  \n",
      "2  setosa  \n",
      "3  setosa  \n",
      "4  setosa  \n",
      "\n",
      "ğŸ“Š Dataset statistics:\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "  species  \n",
      "0  setosa  \n",
      "1  setosa  \n",
      "2  setosa  \n",
      "3  setosa  \n",
      "4  setosa  \n",
      "\n",
      "ğŸ“Š Dataset statistics:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
      "count         150.000000        150.000000         150.000000   \n",
      "mean            5.843333          3.057333           3.758000   \n",
      "std             0.828066          0.435866           1.765298   \n",
      "min             4.300000          2.000000           1.000000   \n",
      "25%             5.100000          2.800000           1.600000   \n",
      "50%             5.800000          3.000000           4.350000   \n",
      "75%             6.400000          3.300000           5.100000   \n",
      "max             7.900000          4.400000           6.900000   \n",
      "\n",
      "       petal width (cm)  \n",
      "count        150.000000  \n",
      "mean           1.199333  \n",
      "std            0.762238  \n",
      "min            0.100000  \n",
      "25%            0.300000  \n",
      "50%            1.300000  \n",
      "75%            1.800000  \n",
      "max            2.500000  \n",
      "\n",
      "ğŸ¯ Class distribution:\n",
      "species\n",
      "setosa        50\n",
      "versicolor    50\n",
      "virginica     50\n",
      "Name: count, dtype: int64\n",
      "\n",
      "âœ‚ï¸  Data split complete:\n",
      "   Training samples: 120\n",
      "   Testing samples: 30\n",
      "   Features: 4\n",
      "\n",
      "âœ… Data preparation complete!\n",
      "   Ready to start model training and MLflow tracking!\n"
     ]
    }
   ],
   "source": [
    "# Load the Iris dataset from scikit-learn\n",
    "# This is a classic dataset with 150 samples, 4 features, and 3 classes\n",
    "print(\"ğŸ“Š Loading Iris dataset...\")\n",
    "iris_data = datasets.load_iris()\n",
    "\n",
    "# Extract features (X) and target labels (y)\n",
    "X = iris_data.data          # Features: sepal length, sepal width, petal length, petal width\n",
    "y = iris_data.target        # Target: iris species (0: setosa, 1: versicolor, 2: virginica)\n",
    "\n",
    "# Get feature names and target names for better understanding\n",
    "feature_names = iris_data.feature_names\n",
    "target_names = iris_data.target_names\n",
    "\n",
    "print(f\"ğŸ“ˆ Dataset shape: {X.shape}\")\n",
    "print(f\"ğŸ¯ Target classes: {target_names}\")\n",
    "print(f\"ğŸ“‹ Features: {feature_names}\")\n",
    "\n",
    "# Create a pandas DataFrame for better data exploration\n",
    "# This makes it easier to visualize and understand our data\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['species'] = pd.Categorical.from_codes(y, target_names)\n",
    "\n",
    "print(\"\\nğŸ“‹ First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nğŸ“Š Dataset statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nğŸ¯ Class distribution:\")\n",
    "print(df['species'].value_counts())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# This is crucial for evaluating model performance on unseen data\n",
    "test_size = 0.2         # Use 20% for testing, 80% for training\n",
    "random_state = 42       # Set random seed for reproducibility\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=test_size, \n",
    "    random_state=random_state,\n",
    "    stratify=y  # Ensure balanced split across all classes\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ‚ï¸  Data split complete:\")\n",
    "print(f\"   Training samples: {X_train.shape[0]}\")\n",
    "print(f\"   Testing samples: {X_test.shape[0]}\")\n",
    "print(f\"   Features: {X_train.shape[1]}\")\n",
    "\n",
    "# Log data information that we'll use in our experiments\n",
    "data_info = {\n",
    "    \"dataset_name\": \"iris\",\n",
    "    \"total_samples\": len(X),\n",
    "    \"n_features\": X.shape[1],\n",
    "    \"n_classes\": len(target_names),\n",
    "    \"train_samples\": len(X_train),\n",
    "    \"test_samples\": len(X_test),\n",
    "    \"test_size\": test_size,\n",
    "    \"random_state\": random_state\n",
    "}\n",
    "\n",
    "print(\"\\nâœ… Data preparation complete!\")\n",
    "print(\"   Ready to start model training and MLflow tracking!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a266e",
   "metadata": {},
   "source": [
    "## 5. Log Parameters and Hyperparameters\n",
    "\n",
    "**Parameters** in MLflow are the configuration values that control your experiment. This includes:\n",
    "- **Hyperparameters**: Model-specific settings (learning rate, number of trees, etc.)\n",
    "- **Data parameters**: Dataset splits, preprocessing settings\n",
    "- **Environment parameters**: Library versions, hardware specs\n",
    "\n",
    "### Why Log Parameters?\n",
    "- **Reproducibility**: Recreate exact same results\n",
    "- **Comparison**: Compare different configurations\n",
    "- **Optimization**: Track what works best\n",
    "- **Documentation**: Maintain experiment history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55a989db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting MLflow run for parameter logging demonstration...\n",
      "ğŸ“ Run ID: 96ad5c7c035f47e9bcc70025e20b1fea\n",
      "\n",
      "ğŸ“‹ Logging individual parameters...\n",
      "ğŸ“ Run ID: 96ad5c7c035f47e9bcc70025e20b1fea\n",
      "\n",
      "ğŸ“‹ Logging individual parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Logging multiple parameters at once...\n",
      "âœ… All parameters logged successfully!\n",
      "\n",
      "ğŸ” View this run in MLflow UI:\n",
      "   Run ID: 96ad5c7c035f47e9bcc70025e20b1fea\n",
      "   Experiment: iris-classification-tutorial\n",
      "\n",
      "ğŸ’¡ Parameter Logging Best Practices:\n",
      "   âœ“ Use descriptive parameter names\n",
      "   âœ“ Log all hyperparameters that affect model behavior\n",
      "   âœ“ Include data processing parameters\n",
      "   âœ“ Track environment information for reproducibility\n",
      "   âœ“ Use consistent naming conventions\n",
      "\n",
      "ğŸ¯ Parameter logging demonstration complete!\n",
      "These parameters are now stored in MLflow and can be:\n",
      "   - Viewed in the MLflow UI\n",
      "   - Queried programmatically\n",
      "   - Used for experiment comparison\n",
      "   - Referenced for model reproduction\n"
     ]
    }
   ],
   "source": [
    "# Let's start our first MLflow run to demonstrate parameter logging\n",
    "# A \"run\" is a single execution of your ML code (one training session)\n",
    "\n",
    "print(\"ğŸš€ Starting MLflow run for parameter logging demonstration...\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"parameter-logging-demo\") as run:\n",
    "    print(f\"ğŸ“ Run ID: {run.info.run_id}\")\n",
    "    \n",
    "    # Method 1: Log individual parameters using mlflow.log_param()\n",
    "    # This is useful for single values or when you want to log parameters one by one\n",
    "    print(\"\\nğŸ“‹ Logging individual parameters...\")\n",
    "    \n",
    "    mlflow.log_param(\"model_type\", \"logistic_regression\")  # String parameter\n",
    "    mlflow.log_param(\"test_size\", 0.2)                     # Float parameter  \n",
    "    mlflow.log_param(\"random_state\", 42)                   # Integer parameter\n",
    "    mlflow.log_param(\"cross_validation\", True)             # Boolean parameter\n",
    "    \n",
    "    # Method 2: Log multiple parameters at once using mlflow.log_params()\n",
    "    # This is more efficient when you have many parameters\n",
    "    print(\"ğŸ“‹ Logging multiple parameters at once...\")\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    model_params = {\n",
    "        \"solver\": \"lbfgs\",           # Algorithm to use in optimization\n",
    "        \"max_iter\": 1000,            # Maximum number of iterations\n",
    "        \"multi_class\": \"auto\",       # How to handle multi-class classification\n",
    "        \"penalty\": \"l2\",             # Regularization penalty\n",
    "        \"C\": 1.0,                    # Inverse of regularization strength\n",
    "        \"fit_intercept\": True,       # Whether to calculate intercept\n",
    "        \"class_weight\": None         # Weights for balancing classes\n",
    "    }\n",
    "    mlflow.log_params(model_params)\n",
    "    \n",
    "    # Data processing parameters\n",
    "    data_params = {\n",
    "        \"dataset_version\": \"1.0\",\n",
    "        \"feature_scaling\": \"none\",\n",
    "        \"feature_selection\": \"all\",\n",
    "        \"data_split_strategy\": \"stratified\"\n",
    "    }\n",
    "    mlflow.log_params(data_params)\n",
    "    \n",
    "    # Environment and system parameters\n",
    "    system_params = {\n",
    "        \"sklearn_version\": \"1.3.0\",\n",
    "        \"python_version\": \"3.9\",\n",
    "        \"platform\": \"macOS\",\n",
    "        \"cpu_count\": os.cpu_count()\n",
    "    }\n",
    "    mlflow.log_params(system_params)\n",
    "    \n",
    "    print(\"âœ… All parameters logged successfully!\")\n",
    "    print(f\"\\nğŸ” View this run in MLflow UI:\")\n",
    "    print(f\"   Run ID: {run.info.run_id}\")\n",
    "    print(f\"   Experiment: {experiment_name}\")\n",
    "    \n",
    "    # Log the data information we prepared earlier\n",
    "    mlflow.log_params(data_info)\n",
    "    \n",
    "    print(\"\\nğŸ’¡ Parameter Logging Best Practices:\")\n",
    "    print(\"   âœ“ Use descriptive parameter names\")\n",
    "    print(\"   âœ“ Log all hyperparameters that affect model behavior\")\n",
    "    print(\"   âœ“ Include data processing parameters\")\n",
    "    print(\"   âœ“ Track environment information for reproducibility\")\n",
    "    print(\"   âœ“ Use consistent naming conventions\")\n",
    "    \n",
    "print(\"\\nğŸ¯ Parameter logging demonstration complete!\")\n",
    "print(\"These parameters are now stored in MLflow and can be:\")\n",
    "print(\"   - Viewed in the MLflow UI\")\n",
    "print(\"   - Queried programmatically\") \n",
    "print(\"   - Used for experiment comparison\")\n",
    "print(\"   - Referenced for model reproduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe126bc",
   "metadata": {},
   "source": [
    "## 6. Train a Simple Model with MLflow Tracking\n",
    "\n",
    "Now let's train our first machine learning model while tracking everything with MLflow. We'll train a Logistic Regression model and log every step of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4422782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Starting model training with complete MLflow tracking...\n",
      "ğŸ†” Run ID: 1b22db2a24a34e73880e57ebc0da8a85\n",
      "ğŸ“… Start Time: 1756448173293\n",
      "ğŸ“ Logging model hyperparameters...\n",
      "ğŸ†” Run ID: 1b22db2a24a34e73880e57ebc0da8a85\n",
      "ğŸ“… Start Time: 1756448173293\n",
      "ğŸ“ Logging model hyperparameters...\n",
      "ğŸ‹ï¸ Training Logistic Regression model...\n",
      "âœ… Model training complete!\n",
      "ğŸ”® Making predictions...\n",
      "ğŸ“Š Training predictions shape: (120,)\n",
      "ğŸ“Š Test predictions shape: (30,)\n",
      "ğŸ“ˆ Calculating performance metrics...\n",
      "\\nğŸ“Š Training Performance:\n",
      "   Accuracy:  0.9750\n",
      "   Precision: 0.9752\n",
      "   Recall:    0.9750\n",
      "   F1-Score:  0.9750\n",
      "\\nğŸ¯ Test Performance:\n",
      "   Accuracy:  0.9667\n",
      "   Precision: 0.9697\n",
      "   Recall:    0.9667\n",
      "   F1-Score:  0.9666\n",
      "\\nâš–ï¸  Overfitting Check:\n",
      "   Accuracy difference: 0.0083\n",
      "   âœ… Model generalizes well!\n",
      "\\nğŸ’¾ Logging metrics to MLflow...\n",
      "ğŸ‹ï¸ Training Logistic Regression model...\n",
      "âœ… Model training complete!\n",
      "ğŸ”® Making predictions...\n",
      "ğŸ“Š Training predictions shape: (120,)\n",
      "ğŸ“Š Test predictions shape: (30,)\n",
      "ğŸ“ˆ Calculating performance metrics...\n",
      "\\nğŸ“Š Training Performance:\n",
      "   Accuracy:  0.9750\n",
      "   Precision: 0.9752\n",
      "   Recall:    0.9750\n",
      "   F1-Score:  0.9750\n",
      "\\nğŸ¯ Test Performance:\n",
      "   Accuracy:  0.9667\n",
      "   Precision: 0.9697\n",
      "   Recall:    0.9667\n",
      "   F1-Score:  0.9666\n",
      "\\nâš–ï¸  Overfitting Check:\n",
      "   Accuracy difference: 0.0083\n",
      "   âœ… Model generalizes well!\n",
      "\\nğŸ’¾ Logging metrics to MLflow...\n",
      "ğŸ·ï¸  Adding tags for organization...\n",
      "\\nâœ… Model training and tracking complete!\n",
      "ğŸ” View results in MLflow UI - Run ID: 1b22db2a24a34e73880e57ebc0da8a85\n",
      "\\nğŸ‰ Your first tracked ML experiment is done!\n",
      "This run now contains:\n",
      "   âœ“ All hyperparameters\n",
      "   âœ“ Training and test metrics\n",
      "   âœ“ Run metadata and tags\n",
      "   âœ“ Timestamps and run information\n",
      "ğŸ·ï¸  Adding tags for organization...\n",
      "\\nâœ… Model training and tracking complete!\n",
      "ğŸ” View results in MLflow UI - Run ID: 1b22db2a24a34e73880e57ebc0da8a85\n",
      "\\nğŸ‰ Your first tracked ML experiment is done!\n",
      "This run now contains:\n",
      "   âœ“ All hyperparameters\n",
      "   âœ“ Training and test metrics\n",
      "   âœ“ Run metadata and tags\n",
      "   âœ“ Timestamps and run information\n"
     ]
    }
   ],
   "source": [
    "# Start a new MLflow run for model training\n",
    "print(\"ğŸ¯ Starting model training with complete MLflow tracking...\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"logistic-regression-baseline\") as run:\n",
    "    \n",
    "    print(f\"ğŸ†” Run ID: {run.info.run_id}\")\n",
    "    print(f\"ğŸ“… Start Time: {run.info.start_time}\")\n",
    "    \n",
    "    # Step 1: Define model hyperparameters\n",
    "    # These parameters control how the model learns\n",
    "    model_params = {\n",
    "        \"solver\": \"lbfgs\",         # Optimization algorithm\n",
    "        \"max_iter\": 1000,          # Maximum iterations for convergence\n",
    "        \"multi_class\": \"auto\",     # Multi-class strategy\n",
    "        \"random_state\": 42,        # For reproducible results\n",
    "        \"C\": 1.0,                  # Regularization strength (inverse)\n",
    "        \"penalty\": \"l2\"            # Regularization type\n",
    "    }\n",
    "    \n",
    "    # Step 2: Log all hyperparameters to MLflow\n",
    "    print(\"ğŸ“ Logging model hyperparameters...\")\n",
    "    mlflow.log_params(model_params)\n",
    "    \n",
    "    # Also log data-related parameters\n",
    "    mlflow.log_params({\n",
    "        \"train_samples\": len(X_train),\n",
    "        \"test_samples\": len(X_test),\n",
    "        \"n_features\": X_train.shape[1],\n",
    "        \"n_classes\": len(np.unique(y_train))\n",
    "    })\n",
    "    \n",
    "    # Step 3: Create and train the model\n",
    "    print(\"ğŸ‹ï¸ Training Logistic Regression model...\")\n",
    "    \n",
    "    # Initialize the model with our hyperparameters\n",
    "    model = LogisticRegression(**model_params)\n",
    "    \n",
    "    # Train the model on training data\n",
    "    # This is where the actual machine learning happens\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"âœ… Model training complete!\")\n",
    "    \n",
    "    # Step 4: Make predictions on both training and test sets\n",
    "    print(\"ğŸ”® Making predictions...\")\n",
    "    \n",
    "    # Predict on training data (to check for overfitting)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_train_prob = model.predict_proba(X_train)  # Get probability scores\n",
    "    \n",
    "    # Predict on test data (for true performance evaluation)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_prob = model.predict_proba(X_test)\n",
    "    \n",
    "    print(f\"ğŸ“Š Training predictions shape: {y_train_pred.shape}\")\n",
    "    print(f\"ğŸ“Š Test predictions shape: {y_test_pred.shape}\")\n",
    "    \n",
    "    # Step 5: Calculate performance metrics\n",
    "    print(\"ğŸ“ˆ Calculating performance metrics...\")\n",
    "    \n",
    "    # Training metrics (to detect overfitting)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    train_precision = precision_score(y_train, y_train_pred, average='weighted')\n",
    "    train_recall = recall_score(y_train, y_train_pred, average='weighted')\n",
    "    train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "    \n",
    "    # Test metrics (true performance indicators)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "    test_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "    \n",
    "    # Print results for immediate feedback\n",
    "    print(f\"\\\\nğŸ“Š Training Performance:\")\n",
    "    print(f\"   Accuracy:  {train_accuracy:.4f}\")\n",
    "    print(f\"   Precision: {train_precision:.4f}\")\n",
    "    print(f\"   Recall:    {train_recall:.4f}\")\n",
    "    print(f\"   F1-Score:  {train_f1:.4f}\")\n",
    "    \n",
    "    print(f\"\\\\nğŸ¯ Test Performance:\")\n",
    "    print(f\"   Accuracy:  {test_accuracy:.4f}\")\n",
    "    print(f\"   Precision: {test_precision:.4f}\")\n",
    "    print(f\"   Recall:    {test_recall:.4f}\")\n",
    "    print(f\"   F1-Score:  {test_f1:.4f}\")\n",
    "    \n",
    "    # Calculate overfitting indicator\n",
    "    accuracy_diff = train_accuracy - test_accuracy\n",
    "    print(f\"\\\\nâš–ï¸  Overfitting Check:\")\n",
    "    print(f\"   Accuracy difference: {accuracy_diff:.4f}\")\n",
    "    if accuracy_diff > 0.05:\n",
    "        print(\"   âš ï¸  Potential overfitting detected!\")\n",
    "    else:\n",
    "        print(\"   âœ… Model generalizes well!\")\n",
    "    \n",
    "    # Step 6: Log all metrics to MLflow\n",
    "    print(\"\\\\nğŸ’¾ Logging metrics to MLflow...\")\n",
    "    \n",
    "    # Log training metrics\n",
    "    mlflow.log_metric(\"train_accuracy\", train_accuracy)\n",
    "    mlflow.log_metric(\"train_precision\", train_precision)\n",
    "    mlflow.log_metric(\"train_recall\", train_recall)\n",
    "    mlflow.log_metric(\"train_f1\", train_f1)\n",
    "    \n",
    "    # Log test metrics  \n",
    "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "    mlflow.log_metric(\"test_precision\", test_precision)\n",
    "    mlflow.log_metric(\"test_recall\", test_recall)\n",
    "    mlflow.log_metric(\"test_f1\", test_f1)\n",
    "    \n",
    "    # Log derived metrics\n",
    "    mlflow.log_metric(\"accuracy_diff\", accuracy_diff)\n",
    "    mlflow.log_metric(\"model_complexity\", len(model.coef_[0]))  # Number of features\n",
    "    \n",
    "    # Step 7: Add run tags for organization\n",
    "    print(\"ğŸ·ï¸  Adding tags for organization...\")\n",
    "    mlflow.set_tag(\"model_type\", \"logistic_regression\")\n",
    "    mlflow.set_tag(\"status\", \"completed\")\n",
    "    mlflow.set_tag(\"dataset\", \"iris\")\n",
    "    mlflow.set_tag(\"purpose\", \"baseline_model\")\n",
    "    \n",
    "    print(f\"\\\\nâœ… Model training and tracking complete!\")\n",
    "    print(f\"ğŸ” View results in MLflow UI - Run ID: {run.info.run_id}\")\n",
    "\n",
    "print(\"\\\\nğŸ‰ Your first tracked ML experiment is done!\")\n",
    "print(\"This run now contains:\")\n",
    "print(\"   âœ“ All hyperparameters\")\n",
    "print(\"   âœ“ Training and test metrics\") \n",
    "print(\"   âœ“ Run metadata and tags\")\n",
    "print(\"   âœ“ Timestamps and run information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354de9c6",
   "metadata": {},
   "source": [
    "## 7. Log Metrics During Training\n",
    "\n",
    "**Metrics** in MLflow track the performance and behavior of your models over time. Unlike parameters (which are input configurations), metrics are output measurements that change during or after training.\n",
    "\n",
    "### Types of Metrics to Track:\n",
    "- **Performance metrics**: Accuracy, precision, recall, F1-score\n",
    "- **Training metrics**: Loss, convergence rate, training time\n",
    "- **Resource metrics**: Memory usage, CPU time, GPU utilization\n",
    "- **Business metrics**: Model interpretability scores, fairness metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "680600bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Advanced Metrics Logging Demonstration...\n",
      "ğŸ†” Run ID: 627754b683394b8c831bf35540128002\n",
      "\\nğŸ“ˆ Logging individual metrics...\n",
      "   âœ“ Accuracy: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ“ Precision (macro): 0.9697\n",
      "   âœ“ Precision (micro): 0.9667\n",
      "   âœ“ Precision (weighted): 0.9697\n",
      "\\nğŸ“Š Logging metrics with steps (simulating iterative training)...\n",
      "   Step 0: Training accuracy = 0.7733\n",
      "   Step 1: Training accuracy = 0.8120\n",
      "   Step 2: Training accuracy = 0.8507\n",
      "   Step 3: Training accuracy = 0.8893\n",
      "   Step 4: Training accuracy = 0.9280\n",
      "   Step 5: Training accuracy = 0.9667\n",
      "\\nâš¡ Logging system performance metrics...\n",
      "   âœ“ Training time: 0.0393 seconds\n",
      "   âœ“ Model parameters: 15\n",
      "   âœ“ Features used: 4\n",
      "   âœ“ Model size: 32 bytes\n",
      "\\nğŸ’¼ Logging business-relevant metrics...\n",
      "   âœ“ Model parameters: 15\n",
      "   âœ“ Features used: 4\n",
      "   âœ“ Model size: 32 bytes\n",
      "\\nğŸ’¼ Logging business-relevant metrics...\n",
      "   âœ“ Average prediction confidence: 0.8681\n",
      "   âœ“ Predicted class setosa ratio: 0.3333\n",
      "   âœ“ Predicted class versicolor ratio: 0.3000\n",
      "   âœ“ Predicted class virginica ratio: 0.3667\n",
      "\\nğŸ” Logging derived metrics...\n",
      "   âœ“ setosa - Precision: 1.0000, Recall: 1.0000\n",
      "   âœ“ Average prediction confidence: 0.8681\n",
      "   âœ“ Predicted class setosa ratio: 0.3333\n",
      "   âœ“ Predicted class versicolor ratio: 0.3000\n",
      "   âœ“ Predicted class virginica ratio: 0.3667\n",
      "\\nğŸ” Logging derived metrics...\n",
      "   âœ“ setosa - Precision: 1.0000, Recall: 1.0000\n",
      "   âœ“ versicolor - Precision: 1.0000, Recall: 0.9000\n",
      "   âœ“ virginica - Precision: 0.9091, Recall: 1.0000\n",
      "   ğŸ“‰ Worst performing class: virginica (0.9091)\n",
      "   ğŸ“ˆ Best performing class: setosa (1.0000)\n",
      "\\nâœ… Advanced metrics logging complete!\n",
      "ğŸ” View all metrics in MLflow UI - Run ID: 627754b683394b8c831bf35540128002\n",
      "\\nğŸ’¡ Metrics Logging Best Practices:\n",
      "   âœ“ Log metrics consistently across all experiments\n",
      "   âœ“ Use descriptive metric names\n",
      "   âœ“ Track both training and validation metrics\n",
      "   âœ“ Log system performance metrics for optimization\n",
      "   âœ“ Include business-relevant metrics\n",
      "   âœ“ Use step parameter for iterative training tracking\n",
      "   âœ“ Log derived metrics for deeper insights\n",
      "   âœ“ versicolor - Precision: 1.0000, Recall: 0.9000\n",
      "   âœ“ virginica - Precision: 0.9091, Recall: 1.0000\n",
      "   ğŸ“‰ Worst performing class: virginica (0.9091)\n",
      "   ğŸ“ˆ Best performing class: setosa (1.0000)\n",
      "\\nâœ… Advanced metrics logging complete!\n",
      "ğŸ” View all metrics in MLflow UI - Run ID: 627754b683394b8c831bf35540128002\n",
      "\\nğŸ’¡ Metrics Logging Best Practices:\n",
      "   âœ“ Log metrics consistently across all experiments\n",
      "   âœ“ Use descriptive metric names\n",
      "   âœ“ Track both training and validation metrics\n",
      "   âœ“ Log system performance metrics for optimization\n",
      "   âœ“ Include business-relevant metrics\n",
      "   âœ“ Use step parameter for iterative training tracking\n",
      "   âœ“ Log derived metrics for deeper insights\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate advanced metric logging techniques\n",
    "print(\"ğŸ“Š Advanced Metrics Logging Demonstration...\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"advanced-metrics-demo\") as run:\n",
    "    \n",
    "    print(f\"ğŸ†” Run ID: {run.info.run_id}\")\n",
    "    \n",
    "    # Train a model quickly for demonstration\n",
    "    model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    \n",
    "    # Time the training process\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)\n",
    "    \n",
    "    # Method 1: Log individual metrics with mlflow.log_metric()\n",
    "    print(\"\\\\nğŸ“ˆ Logging individual metrics...\")\n",
    "    \n",
    "    # Basic performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    print(f\"   âœ“ Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Detailed per-class metrics\n",
    "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "    precision_micro = precision_score(y_test, y_pred, average='micro')\n",
    "    precision_weighted = precision_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    mlflow.log_metric(\"precision_macro\", precision_macro)\n",
    "    mlflow.log_metric(\"precision_micro\", precision_micro) \n",
    "    mlflow.log_metric(\"precision_weighted\", precision_weighted)\n",
    "    \n",
    "    print(f\"   âœ“ Precision (macro): {precision_macro:.4f}\")\n",
    "    print(f\"   âœ“ Precision (micro): {precision_micro:.4f}\")\n",
    "    print(f\"   âœ“ Precision (weighted): {precision_weighted:.4f}\")\n",
    "    \n",
    "    # Method 2: Log metrics with step parameter (useful for iterative training)\n",
    "    print(\"\\\\nğŸ“Š Logging metrics with steps (simulating iterative training)...\")\n",
    "    \n",
    "    # Simulate logging metrics at different training epochs/steps\n",
    "    # This is useful for tracking training progress over time\n",
    "    for step in range(0, 6):\n",
    "        # Simulate improving accuracy over training steps\n",
    "        simulated_accuracy = accuracy * (0.8 + 0.04 * step)  # Gradually improve\n",
    "        mlflow.log_metric(\"training_accuracy\", simulated_accuracy, step=step)\n",
    "        print(f\"   Step {step}: Training accuracy = {simulated_accuracy:.4f}\")\n",
    "    \n",
    "    # Method 3: Log system and performance metrics\n",
    "    print(\"\\\\nâš¡ Logging system performance metrics...\")\n",
    "    \n",
    "    # Training time\n",
    "    mlflow.log_metric(\"training_time_seconds\", training_time)\n",
    "    print(f\"   âœ“ Training time: {training_time:.4f} seconds\")\n",
    "    \n",
    "    # Model complexity metrics\n",
    "    n_features = X_train.shape[1]\n",
    "    n_params = len(model.coef_.flatten()) + len(model.intercept_)\n",
    "    mlflow.log_metric(\"model_parameters\", n_params)\n",
    "    mlflow.log_metric(\"feature_count\", n_features)\n",
    "    \n",
    "    print(f\"   âœ“ Model parameters: {n_params}\")\n",
    "    print(f\"   âœ“ Features used: {n_features}\")\n",
    "    \n",
    "    # Memory usage (approximate)\n",
    "    model_size_bytes = model.__sizeof__()\n",
    "    mlflow.log_metric(\"model_size_bytes\", model_size_bytes)\n",
    "    print(f\"   âœ“ Model size: {model_size_bytes} bytes\")\n",
    "    \n",
    "    # Method 4: Log business and interpretability metrics\n",
    "    print(\"\\\\nğŸ’¼ Logging business-relevant metrics...\")\n",
    "    \n",
    "    # Confidence scores (average prediction probability)\n",
    "    avg_confidence = np.mean(np.max(y_prob, axis=1))\n",
    "    mlflow.log_metric(\"average_prediction_confidence\", avg_confidence)\n",
    "    print(f\"   âœ“ Average prediction confidence: {avg_confidence:.4f}\")\n",
    "    \n",
    "    # Class balance in predictions\n",
    "    pred_class_distribution = np.bincount(y_pred) / len(y_pred)\n",
    "    for i, ratio in enumerate(pred_class_distribution):\n",
    "        mlflow.log_metric(f\"predicted_class_{i}_ratio\", ratio)\n",
    "        print(f\"   âœ“ Predicted class {target_names[i]} ratio: {ratio:.4f}\")\n",
    "    \n",
    "    # Method 5: Log derived and comparative metrics\n",
    "    print(\"\\\\nğŸ” Logging derived metrics...\")\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    per_class_precision = precision_score(y_test, y_pred, average=None)\n",
    "    per_class_recall = recall_score(y_test, y_pred, average=None)\n",
    "    \n",
    "    for i, (prec, rec) in enumerate(zip(per_class_precision, per_class_recall)):\n",
    "        mlflow.log_metric(f\"precision_class_{target_names[i]}\", prec)\n",
    "        mlflow.log_metric(f\"recall_class_{target_names[i]}\", rec)\n",
    "        print(f\"   âœ“ {target_names[i]} - Precision: {prec:.4f}, Recall: {rec:.4f}\")\n",
    "    \n",
    "    # Worst and best performing class\n",
    "    worst_class_idx = np.argmin(per_class_precision)\n",
    "    best_class_idx = np.argmax(per_class_precision)\n",
    "    \n",
    "    mlflow.log_metric(\"worst_class_precision\", per_class_precision[worst_class_idx])\n",
    "    mlflow.log_metric(\"best_class_precision\", per_class_precision[best_class_idx])\n",
    "    \n",
    "    print(f\"   ğŸ“‰ Worst performing class: {target_names[worst_class_idx]} ({per_class_precision[worst_class_idx]:.4f})\")\n",
    "    print(f\"   ğŸ“ˆ Best performing class: {target_names[best_class_idx]} ({per_class_precision[best_class_idx]:.4f})\")\n",
    "    \n",
    "    # Add descriptive tags\n",
    "    mlflow.set_tag(\"metrics_logged\", \"comprehensive\")\n",
    "    mlflow.set_tag(\"analysis_depth\", \"detailed\")\n",
    "    \n",
    "    print(f\"\\\\nâœ… Advanced metrics logging complete!\")\n",
    "    print(f\"ğŸ” View all metrics in MLflow UI - Run ID: {run.info.run_id}\")\n",
    "\n",
    "print(\"\\\\nğŸ’¡ Metrics Logging Best Practices:\")\n",
    "print(\"   âœ“ Log metrics consistently across all experiments\")\n",
    "print(\"   âœ“ Use descriptive metric names\")\n",
    "print(\"   âœ“ Track both training and validation metrics\")\n",
    "print(\"   âœ“ Log system performance metrics for optimization\")\n",
    "print(\"   âœ“ Include business-relevant metrics\")\n",
    "print(\"   âœ“ Use step parameter for iterative training tracking\")\n",
    "print(\"   âœ“ Log derived metrics for deeper insights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b5b3f7",
   "metadata": {},
   "source": [
    "## 8. Log Model Artifacts\n",
    "\n",
    "**Artifacts** in MLflow are files associated with your experiments. This includes:\n",
    "- **Models**: Trained model files (pickle, joblib, etc.)\n",
    "- **Plots**: Visualizations and charts\n",
    "- **Data files**: Processed datasets, feature importance files\n",
    "- **Reports**: Model evaluation reports, documentation\n",
    "- **Code**: Scripts, notebooks, configuration files\n",
    "\n",
    "Artifacts enable full reproducibility and comprehensive experiment documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "330f8fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Comprehensive Artifact Logging Demonstration...\n",
      "ğŸ†” Run ID: 627754b683394b8c831bf35540128002\n",
      "\n",
      "ğŸ“Š Creating and logging visualization artifacts...\n",
      "   âœ“ Saved confusion matrix: confusion_matrix.png\n",
      "   âœ“ Saved confusion matrix: confusion_matrix.png\n",
      "   âœ“ Saved feature importance: feature_importance.png\n",
      "   âœ“ Saved feature importance: feature_importance.png\n",
      "   âœ“ Saved prediction analysis: prediction_analysis.png\n",
      "\n",
      "ğŸ’¾ Creating and logging data artifacts...\n",
      "   âœ“ Saved classification report: classification_report.csv\n",
      "   âœ“ Saved detailed predictions: detailed_predictions.csv\n",
      "   âœ“ Saved model summary: model_summary.json\n",
      "\n",
      "ğŸ“ˆ Creating performance tracking data...\n",
      "   âœ“ Saved training history: training_history.csv\n",
      "\n",
      "âœ… Artifact logging complete!\n",
      "ğŸ“ All files saved locally and logged to MLflow run: 627754b683394b8c831bf35540128002\n",
      "ğŸ”— Artifacts will be available in the MLflow UI under this run\n",
      "ğŸ§¹ Local artifact files cleaned up!\n",
      "   âœ“ Saved prediction analysis: prediction_analysis.png\n",
      "\n",
      "ğŸ’¾ Creating and logging data artifacts...\n",
      "   âœ“ Saved classification report: classification_report.csv\n",
      "   âœ“ Saved detailed predictions: detailed_predictions.csv\n",
      "   âœ“ Saved model summary: model_summary.json\n",
      "\n",
      "ğŸ“ˆ Creating performance tracking data...\n",
      "   âœ“ Saved training history: training_history.csv\n",
      "\n",
      "âœ… Artifact logging complete!\n",
      "ğŸ“ All files saved locally and logged to MLflow run: 627754b683394b8c831bf35540128002\n",
      "ğŸ”— Artifacts will be available in the MLflow UI under this run\n",
      "ğŸ§¹ Local artifact files cleaned up!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ğŸ“¦ Comprehensive Artifact Logging Demonstration\n",
    "print(\"ğŸ“¦ Comprehensive Artifact Logging Demonstration...\")\n",
    "print(f\"ğŸ†” Run ID: {run.info.run_id}\")\n",
    "\n",
    "# Create visualizations and save as artifacts\n",
    "print(\"\\nğŸ“Š Creating and logging visualization artifacts...\")\n",
    "\n",
    "# 1. Confusion Matrix Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=target_names, yticklabels=target_names)\n",
    "plt.title('Confusion Matrix - Test Set Performance')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "confusion_matrix_path = \"confusion_matrix.png\"\n",
    "plt.savefig(confusion_matrix_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   âœ“ Saved confusion matrix: {confusion_matrix_path}\")\n",
    "\n",
    "# 2. Feature Importance Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "# For logistic regression, use the absolute values of coefficients as importance\n",
    "feature_importance = np.abs(model.coef_[0])\n",
    "plt.barh(range(len(feature_names)), feature_importance)\n",
    "plt.yticks(range(len(feature_names)), feature_names)\n",
    "plt.xlabel('Feature Importance (|Coefficient|)')\n",
    "plt.title('Feature Importance - Logistic Regression')\n",
    "plt.tight_layout()\n",
    "feature_importance_path = \"feature_importance.png\"\n",
    "plt.savefig(feature_importance_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   âœ“ Saved feature importance: {feature_importance_path}\")\n",
    "\n",
    "# 3. Prediction Analysis Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Get prediction probabilities\n",
    "y_prob = model.predict_proba(X_test)\n",
    "max_probs = np.max(y_prob, axis=1)\n",
    "\n",
    "# Create subplots for detailed analysis\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Prediction confidence distribution\n",
    "ax1.hist(max_probs, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax1.set_xlabel('Maximum Prediction Probability')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Prediction Confidence Distribution')\n",
    "ax1.axvline(np.mean(max_probs), color='red', linestyle='--', \n",
    "           label=f'Mean: {np.mean(max_probs):.3f}')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Class-wise prediction accuracy\n",
    "from sklearn.metrics import classification_report\n",
    "class_report = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "classes = list(target_names)\n",
    "precisions = [class_report[cls]['precision'] for cls in classes]\n",
    "recalls = [class_report[cls]['recall'] for cls in classes]\n",
    "\n",
    "x = np.arange(len(classes))\n",
    "width = 0.35\n",
    "ax2.bar(x - width/2, precisions, width, label='Precision', alpha=0.8)\n",
    "ax2.bar(x + width/2, recalls, width, label='Recall', alpha=0.8)\n",
    "ax2.set_xlabel('Classes')\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_title('Per-Class Performance Metrics')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(classes)\n",
    "ax2.legend()\n",
    "ax2.set_ylim(0, 1.1)\n",
    "\n",
    "# Plot 3: Feature correlation with predictions\n",
    "ax3.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='viridis', alpha=0.6, s=50)\n",
    "ax3.set_xlabel(feature_names[0])\n",
    "ax3.set_ylabel(feature_names[1])\n",
    "ax3.set_title('True Labels - Feature Space')\n",
    "\n",
    "# Plot 4: Prediction errors analysis\n",
    "incorrect_mask = y_test != y_pred\n",
    "if np.any(incorrect_mask):\n",
    "    ax4.scatter(X_test[~incorrect_mask, 0], X_test[~incorrect_mask, 1], \n",
    "               c=y_test[~incorrect_mask], cmap='viridis', alpha=0.6, s=50, label='Correct')\n",
    "    ax4.scatter(X_test[incorrect_mask, 0], X_test[incorrect_mask, 1], \n",
    "               c='red', marker='x', s=100, label='Incorrect')\n",
    "    ax4.set_xlabel(feature_names[0])\n",
    "    ax4.set_ylabel(feature_names[1])\n",
    "    ax4.set_title('Prediction Errors Analysis')\n",
    "    ax4.legend()\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'No prediction errors!', ha='center', va='center', \n",
    "            transform=ax4.transAxes, fontsize=16, color='green')\n",
    "    ax4.set_title('Prediction Errors Analysis')\n",
    "\n",
    "plt.tight_layout()\n",
    "prediction_analysis_path = \"prediction_analysis.png\"\n",
    "plt.savefig(prediction_analysis_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   âœ“ Saved prediction analysis: {prediction_analysis_path}\")\n",
    "\n",
    "# Log all visualization artifacts to MLflow\n",
    "mlflow.log_artifact(confusion_matrix_path)\n",
    "mlflow.log_artifact(feature_importance_path)\n",
    "mlflow.log_artifact(prediction_analysis_path)\n",
    "\n",
    "print(\"\\nğŸ’¾ Creating and logging data artifacts...\")\n",
    "\n",
    "# 4. Classification Report as CSV\n",
    "class_report_df = pd.DataFrame(classification_report(y_test, y_pred, \n",
    "                                                   target_names=target_names, \n",
    "                                                   output_dict=True)).T\n",
    "class_report_path = \"classification_report.csv\"\n",
    "class_report_df.to_csv(class_report_path)\n",
    "mlflow.log_artifact(class_report_path)\n",
    "print(f\"   âœ“ Saved classification report: {class_report_path}\")\n",
    "\n",
    "# 5. Detailed Predictions with Probabilities\n",
    "results_df = pd.DataFrame({\n",
    "    'true_label': y_test,\n",
    "    'predicted_label': y_pred,\n",
    "    'true_class_name': [target_names[i] for i in y_test],\n",
    "    'predicted_class_name': [target_names[i] for i in y_pred],\n",
    "    'is_correct': y_test == y_pred,\n",
    "    'max_probability': np.max(y_prob, axis=1),\n",
    "    'confidence': np.max(y_prob, axis=1)\n",
    "})\n",
    "\n",
    "# Add individual class probabilities\n",
    "for i, class_name in enumerate(target_names):\n",
    "    results_df[f'prob_{class_name}'] = y_prob[:, i]\n",
    "\n",
    "predictions_path = \"detailed_predictions.csv\"\n",
    "results_df.to_csv(predictions_path, index=False)\n",
    "mlflow.log_artifact(predictions_path)\n",
    "print(f\"   âœ“ Saved detailed predictions: {predictions_path}\")\n",
    "\n",
    "# Create model summary report\n",
    "model_summary = {\n",
    "    \"model_type\": \"LogisticRegression\",\n",
    "    \"training_samples\": len(X_train),\n",
    "    \"test_samples\": len(X_test),\n",
    "    \"features\": feature_names,  # Remove .tolist() since it's already a list\n",
    "    \"classes\": target_names.tolist(),\n",
    "    \"hyperparameters\": model_params,\n",
    "    \"performance\": {\n",
    "        \"accuracy\": float(accuracy_score(y_test, y_pred)),\n",
    "        \"precision\": float(precision_score(y_test, y_pred, average='weighted')),\n",
    "        \"recall\": float(recall_score(y_test, y_pred, average='weighted')),\n",
    "        \"f1\": float(f1_score(y_test, y_pred, average='weighted'))\n",
    "    },\n",
    "    \"feature_coefficients\": model.coef_.tolist(),\n",
    "    \"intercept\": model.intercept_.tolist()\n",
    "}\n",
    "\n",
    "# Save as JSON\n",
    "import json\n",
    "model_summary_path = \"model_summary.json\"\n",
    "with open(model_summary_path, 'w') as f:\n",
    "    json.dump(model_summary, f, indent=2)\n",
    "\n",
    "mlflow.log_artifact(model_summary_path)\n",
    "print(f\"   âœ“ Saved model summary: {model_summary_path}\")\n",
    "\n",
    "# 6. Model Performance Over Time (simulated)\n",
    "print(\"\\nğŸ“ˆ Creating performance tracking data...\")\n",
    "performance_history = []\n",
    "for step in range(1, 11):\n",
    "    # Simulate training progress (this would come from actual training loop)\n",
    "    simulated_accuracy = 0.4 + (step/10) * 0.5 + np.random.normal(0, 0.02)\n",
    "    performance_history.append({\n",
    "        'step': step,\n",
    "        'accuracy': min(max(simulated_accuracy, 0), 1),  # Clamp between 0 and 1\n",
    "        'loss': max(0.1, 2.0 - step * 0.15 + np.random.normal(0, 0.05))\n",
    "    })\n",
    "\n",
    "performance_df = pd.DataFrame(performance_history)\n",
    "performance_path = \"training_history.csv\"\n",
    "performance_df.to_csv(performance_path, index=False)\n",
    "mlflow.log_artifact(performance_path)\n",
    "print(f\"   âœ“ Saved training history: {performance_path}\")\n",
    "\n",
    "print(f\"\\nâœ… Artifact logging complete!\")\n",
    "print(f\"ğŸ“ All files saved locally and logged to MLflow run: {run.info.run_id}\")\n",
    "print(f\"ğŸ”— Artifacts will be available in the MLflow UI under this run\")\n",
    "\n",
    "# Clean up local files (optional)\n",
    "import os\n",
    "for file_path in [confusion_matrix_path, feature_importance_path, prediction_analysis_path,\n",
    "                 class_report_path, predictions_path, model_summary_path, performance_path]:\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        \n",
    "print(\"ğŸ§¹ Local artifact files cleaned up!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc8d272",
   "metadata": {},
   "source": [
    "## 9. Register Model in MLflow Model Registry\n",
    "\n",
    "The **MLflow Model Registry** is a centralized model store that provides:\n",
    "- **Model Versioning**: Track different versions of the same model\n",
    "- **Stage Management**: Move models through Staging â†’ Production\n",
    "- **Model Lineage**: Connect models to their training runs\n",
    "- **Collaboration**: Share models across teams\n",
    "- **Governance**: Control model deployments and approvals\n",
    "\n",
    "### Model Lifecycle Stages:\n",
    "- **None**: Newly registered model\n",
    "- **Staging**: Model being tested/validated\n",
    "- **Production**: Model deployed for real use\n",
    "- **Archived**: Retired model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e75672a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ended previous MLflow run\n",
      "ğŸ”„ Ready to start model registry demonstration...\n"
     ]
    }
   ],
   "source": [
    "# End any active MLflow runs before starting model registry demo\n",
    "try:\n",
    "    mlflow.end_run()\n",
    "    print(\"âœ… Ended previous MLflow run\")\n",
    "except Exception as e:\n",
    "    print(f\"â„¹ï¸  No active run to end: {e}\")\n",
    "\n",
    "print(\"ğŸ”„ Ready to start model registry demonstration...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f28796c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸª MLflow Model Registry Demonstration...\n",
      "ğŸ†” Run ID: 1e6590f9b61749c782be833208a47b83\n",
      "\\nğŸ‹ï¸ Training production-ready model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ğŸ“Š Model Performance:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/29 16:16:23 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Accuracy: 0.9667\n",
      "      F1-Score: 0.9666\n",
      "\\nğŸ“ Registering model in MLflow Model Registry...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'iris-classifier-production' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'iris-classifier-production'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Model registered as: iris-classifier-production\n",
      "   ğŸ“ Model URI: models:/m-f08c9eba181142e8b14799d644081f52\n",
      "\\nğŸ”„ Working with Model Registry...\n",
      "\\nğŸ“‹ Registered Model: iris-classifier-production\n",
      "   Description: No description\n",
      "   Creation Time: 1756448150627\n",
      "\\nğŸ“š Model Versions (2 total):\n",
      "   Version 2:\n",
      "      Stage: None\n",
      "      Status: READY\n",
      "      Run ID: 1e6590f9b61749c782be833208a47b83\n",
      "      Creation Time: 1756448184335\n",
      "   Version 1:\n",
      "      Stage: None\n",
      "      Status: READY\n",
      "      Run ID: 9c7f1df7423c4d63bc808120e437653e\n",
      "      Creation Time: 1756448150640\n",
      "\\nğŸ­ Managing Model Stages...\n",
      "\\nğŸ¯ Working with Version 2...\n",
      "   ğŸ“¤ Transitioning to Staging...\n",
      "âš ï¸  Model registry operation failed: ('cannot represent an object', <Metric: dataset_digest=None, dataset_name=None, key='accuracy', model_id='m-f08c9eba181142e8b14799d644081f52', run_id='1e6590f9b61749c782be833208a47b83', step=0, timestamp=1756448183564, value=0.9666666666666667>)\n",
      "   This might happen if using file-based tracking without a server\n",
      "   Model registry works best with MLflow tracking server\n",
      "\\nâœ… Model Registry demonstration complete!\n",
      "\\nğŸ’¡ Model Registry Best Practices:\n",
      "   âœ“ Use descriptive model names and versions\n",
      "   âœ“ Add detailed descriptions and tags\n",
      "   âœ“ Follow proper stage transitions (None â†’ Staging â†’ Production)\n",
      "   âœ“ Archive old versions when promoting new ones\n",
      "   âœ“ Include performance metrics in model descriptions\n",
      "   âœ“ Use tags for categorization and searchability\n",
      "   âœ“ Implement proper validation before production promotion\n",
      "   âœ“ Consider using MLflow tracking server for team collaboration\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate MLflow Model Registry functionality\n",
    "print(\"ğŸª MLflow Model Registry Demonstration...\")\n",
    "\n",
    "# Define the registered model name\n",
    "registered_model_name = \"iris-classifier-production\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"model-for-registry\") as run:\n",
    "    \n",
    "    print(f\"ğŸ†” Run ID: {run.info.run_id}\")\n",
    "    \n",
    "    # Train a high-quality model for registration\n",
    "    print(\"\\\\nğŸ‹ï¸ Training production-ready model...\")\n",
    "    \n",
    "    # Use optimized hyperparameters\n",
    "    production_params = {\n",
    "        \"solver\": \"lbfgs\",\n",
    "        \"max_iter\": 2000,        # More iterations for better convergence\n",
    "        \"multi_class\": \"auto\",\n",
    "        \"random_state\": 42,\n",
    "        \"C\": 0.8,                # Slightly more regularization\n",
    "        \"penalty\": \"l2\"\n",
    "    }\n",
    "    \n",
    "    # Train the model\n",
    "    production_model = LogisticRegression(**production_params)\n",
    "    production_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    y_pred = production_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Log everything to MLflow\n",
    "    mlflow.log_params(production_params)\n",
    "    mlflow.log_metrics({\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1\n",
    "    })\n",
    "    \n",
    "    print(f\"   ğŸ“Š Model Performance:\")\n",
    "    print(f\"      Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"      F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Create model signature for proper schema documentation\n",
    "    signature = infer_signature(X_train, production_model.predict(X_train))\n",
    "    \n",
    "    # === STEP 1: LOG MODEL WITH REGISTRY NAME ===\n",
    "    print(\"\\\\nğŸ“ Registering model in MLflow Model Registry...\")\n",
    "    \n",
    "    # Log model and register it simultaneously\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=production_model,\n",
    "        artifact_path=\"model\",                           # Path within run artifacts\n",
    "        signature=signature,                             # Input/output schema\n",
    "        input_example=X_train[:3],                      # Sample input for testing\n",
    "        registered_model_name=registered_model_name,    # Register with this name\n",
    "        pip_requirements=[\"scikit-learn>=1.0\", \"pandas\", \"numpy\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"   âœ… Model registered as: {registered_model_name}\")\n",
    "    print(f\"   ğŸ“ Model URI: {model_info.model_uri}\")\n",
    "    \n",
    "    # Add tags for this run\n",
    "    mlflow.set_tag(\"model_purpose\", \"production_candidate\")\n",
    "    mlflow.set_tag(\"quality_check\", \"passed\")\n",
    "    mlflow.set_tag(\"model_type\", \"logistic_regression\")\n",
    "\n",
    "# === STEP 2: WORK WITH REGISTERED MODEL VERSIONS ===\n",
    "print(\"\\\\nğŸ”„ Working with Model Registry...\")\n",
    "\n",
    "# Get model registry client\n",
    "client = MlflowClient()\n",
    "\n",
    "try:\n",
    "    # Get information about the registered model\n",
    "    registered_model = client.get_registered_model(registered_model_name)\n",
    "    print(f\"\\\\nğŸ“‹ Registered Model: {registered_model.name}\")\n",
    "    print(f\"   Description: {registered_model.description or 'No description'}\")\n",
    "    print(f\"   Creation Time: {registered_model.creation_timestamp}\")\n",
    "    \n",
    "    # List all versions of this model\n",
    "    model_versions = client.search_model_versions(f\"name='{registered_model_name}'\")\n",
    "    print(f\"\\\\nğŸ“š Model Versions ({len(model_versions)} total):\")\n",
    "    \n",
    "    for version in model_versions:\n",
    "        print(f\"   Version {version.version}:\")\n",
    "        print(f\"      Stage: {version.current_stage}\")\n",
    "        print(f\"      Status: {version.status}\")\n",
    "        print(f\"      Run ID: {version.run_id}\")\n",
    "        print(f\"      Creation Time: {version.creation_timestamp}\")\n",
    "    \n",
    "    # === STEP 3: MANAGE MODEL STAGES ===\n",
    "    print(\"\\\\nğŸ­ Managing Model Stages...\")\n",
    "    \n",
    "    # Get the latest version\n",
    "    latest_version = model_versions[0]  # Most recent version\n",
    "    current_version_number = latest_version.version\n",
    "    \n",
    "    print(f\"\\\\nğŸ¯ Working with Version {current_version_number}...\")\n",
    "    \n",
    "    # Transition model to Staging\n",
    "    print(\"   ğŸ“¤ Transitioning to Staging...\")\n",
    "    client.transition_model_version_stage(\n",
    "        name=registered_model_name,\n",
    "        version=current_version_number,\n",
    "        stage=\"Staging\",\n",
    "        archive_existing_versions=False  # Keep other versions in their current stages\n",
    "    )\n",
    "    \n",
    "    # Add annotation explaining the transition\n",
    "    client.update_model_version(\n",
    "        name=registered_model_name,\n",
    "        version=current_version_number,\n",
    "        description=f\"Iris classifier trained on {len(X_train)} samples. Accuracy: {accuracy:.4f}\"\n",
    "    )\n",
    "    \n",
    "    print(f\"   âœ… Model version {current_version_number} moved to Staging\")\n",
    "    \n",
    "    # === STEP 4: ADD MODEL AND VERSION TAGS ===\n",
    "    print(\"\\\\nğŸ·ï¸  Adding descriptive tags...\")\n",
    "    \n",
    "    # Add tags to the registered model (applies to all versions)\n",
    "    client.set_registered_model_tag(\n",
    "        name=registered_model_name,\n",
    "        key=\"task\",\n",
    "        value=\"classification\"\n",
    "    )\n",
    "    \n",
    "    client.set_registered_model_tag(\n",
    "        name=registered_model_name,\n",
    "        key=\"dataset\",\n",
    "        value=\"iris\"\n",
    "    )\n",
    "    \n",
    "    # Add tags to specific model version\n",
    "    client.set_model_version_tag(\n",
    "        name=registered_model_name,\n",
    "        version=current_version_number,\n",
    "        key=\"validation_accuracy\",\n",
    "        value=str(round(accuracy, 4))\n",
    "    )\n",
    "    \n",
    "    client.set_model_version_tag(\n",
    "        name=registered_model_name,\n",
    "        version=current_version_number,\n",
    "        key=\"training_framework\",\n",
    "        value=\"scikit-learn\"\n",
    "    )\n",
    "    \n",
    "    print(\"   âœ… Tags added successfully\")\n",
    "    \n",
    "    # === STEP 5: SIMULATE PROMOTION TO PRODUCTION ===\n",
    "    print(\"\\\\nğŸš€ Simulating production deployment...\")\n",
    "    \n",
    "    # In a real scenario, you would run additional validation here\n",
    "    # For demo, we'll promote directly to production\n",
    "    \n",
    "    print(\"   âœ”ï¸  Model validation passed\")\n",
    "    print(\"   âœ”ï¸  Performance benchmarks met\")\n",
    "    print(\"   âœ”ï¸  Security review completed\")\n",
    "    \n",
    "    # Promote to production\n",
    "    client.transition_model_version_stage(\n",
    "        name=registered_model_name,\n",
    "        version=current_version_number,\n",
    "        stage=\"Production\",\n",
    "        archive_existing_versions=True  # Archive previous production versions\n",
    "    )\n",
    "    \n",
    "    print(f\"   ğŸ‰ Model version {current_version_number} promoted to Production!\")\n",
    "    \n",
    "    # === STEP 6: VIEW MODEL REGISTRY STATUS ===\n",
    "    print(\"\\\\nğŸ“Š Current Model Registry Status:\")\n",
    "    \n",
    "    # Refresh model versions list\n",
    "    updated_versions = client.search_model_versions(f\"name='{registered_model_name}'\")\n",
    "    \n",
    "    for version in updated_versions:\n",
    "        stage_emoji = {\n",
    "            \"None\": \"âšª\",\n",
    "            \"Staging\": \"ğŸŸ¡\", \n",
    "            \"Production\": \"ğŸŸ¢\",\n",
    "            \"Archived\": \"ğŸ”´\"\n",
    "        }\n",
    "        \n",
    "        emoji = stage_emoji.get(version.current_stage, \"â“\")\n",
    "        print(f\"   {emoji} Version {version.version}: {version.current_stage}\")\n",
    "        \n",
    "        # Show tags for this version\n",
    "        version_tags = client.get_model_version(registered_model_name, version.version).tags\n",
    "        if version_tags:\n",
    "            for tag_key, tag_value in version_tags.items():\n",
    "                print(f\"        ğŸ·ï¸  {tag_key}: {tag_value}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Model registry operation failed: {e}\")\n",
    "    print(\"   This might happen if using file-based tracking without a server\")\n",
    "    print(\"   Model registry works best with MLflow tracking server\")\n",
    "\n",
    "print(\"\\\\nâœ… Model Registry demonstration complete!\")\n",
    "print(\"\\\\nğŸ’¡ Model Registry Best Practices:\")\n",
    "print(\"   âœ“ Use descriptive model names and versions\")\n",
    "print(\"   âœ“ Add detailed descriptions and tags\")\n",
    "print(\"   âœ“ Follow proper stage transitions (None â†’ Staging â†’ Production)\")\n",
    "print(\"   âœ“ Archive old versions when promoting new ones\")\n",
    "print(\"   âœ“ Include performance metrics in model descriptions\")\n",
    "print(\"   âœ“ Use tags for categorization and searchability\")\n",
    "print(\"   âœ“ Implement proper validation before production promotion\")\n",
    "print(\"   âœ“ Consider using MLflow tracking server for team collaboration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a4af1d",
   "metadata": {},
   "source": [
    "## 10. Load and Use Registered Model\n",
    "\n",
    "Once models are registered, you can load them for inference in various ways:\n",
    "- **By version**: Load a specific model version\n",
    "- **By stage**: Load the current production model\n",
    "- **By run ID**: Load model from a specific training run\n",
    "\n",
    "This enables consistent model deployment and easy rollbacks if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "337520f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Loading and Using Registered Models...\n",
      "\\nğŸ­ Method 1: Loading model by stage...\n",
      "âš ï¸  Could not load production model: No versions of model with name 'iris-classifier-production' and stage 'Production' found\n",
      "   This is normal if no model is in Production stage yet\n",
      "\\nğŸ”¢ Method 2: Loading model by specific version...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/29 16:16:25 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded model version 2 from: models:/iris-classifier-production/2\n",
      "   ğŸ“ˆ Version 2 Accuracy: 0.9667\n",
      "\\nğŸ†” Method 3: Loading model by run ID...\n",
      "   ğŸ†” Demo run ID: 970b1545effe48609e99d3f91e3ce650\n",
      "âœ… Loaded model from run: runs:/970b1545effe48609e99d3f91e3ce650/demo_model\n",
      "   ğŸ“ˆ Run Model Accuracy: 0.9667\n",
      "\\nğŸ“Š Model Loading and Inference Analysis...\n",
      "\\nğŸ“‹ Prediction Comparison (first 10 rows):\n",
      "   actual actual_name  version_pred  version_correct  run_pred  run_correct\n",
      "0       0      setosa             0             True         0         True\n",
      "1       2   virginica             2             True         2         True\n",
      "2       1  versicolor             1             True         1         True\n",
      "3       1  versicolor             1             True         1         True\n",
      "4       0      setosa             0             True         0         True\n",
      "5       1  versicolor             1             True         1         True\n",
      "6       0      setosa             0             True         0         True\n",
      "7       0      setosa             0             True         0         True\n",
      "8       2   virginica             2             True         2         True\n",
      "9       1  versicolor             1             True         1         True\n",
      "\\nğŸš€ Practical Inference Examples...\n",
      "\\nğŸŒ¸ Example 1: Single flower prediction\n",
      "   Input features: [4.4 3.  1.3 0.2]\n",
      "   Predicted class: setosa (index: 0)\n",
      "   Actual class: setosa (index: 0)\n",
      "   Correct: âœ…\n",
      "\\nğŸŒ» Example 2: Batch prediction with confidence\n",
      "   ğŸ†” Demo run ID: 970b1545effe48609e99d3f91e3ce650\n",
      "âœ… Loaded model from run: runs:/970b1545effe48609e99d3f91e3ce650/demo_model\n",
      "   ğŸ“ˆ Run Model Accuracy: 0.9667\n",
      "\\nğŸ“Š Model Loading and Inference Analysis...\n",
      "\\nğŸ“‹ Prediction Comparison (first 10 rows):\n",
      "   actual actual_name  version_pred  version_correct  run_pred  run_correct\n",
      "0       0      setosa             0             True         0         True\n",
      "1       2   virginica             2             True         2         True\n",
      "2       1  versicolor             1             True         1         True\n",
      "3       1  versicolor             1             True         1         True\n",
      "4       0      setosa             0             True         0         True\n",
      "5       1  versicolor             1             True         1         True\n",
      "6       0      setosa             0             True         0         True\n",
      "7       0      setosa             0             True         0         True\n",
      "8       2   virginica             2             True         2         True\n",
      "9       1  versicolor             1             True         1         True\n",
      "\\nğŸš€ Practical Inference Examples...\n",
      "\\nğŸŒ¸ Example 1: Single flower prediction\n",
      "   Input features: [4.4 3.  1.3 0.2]\n",
      "   Predicted class: setosa (index: 0)\n",
      "   Actual class: setosa (index: 0)\n",
      "   Correct: âœ…\n",
      "\\nğŸŒ» Example 2: Batch prediction with confidence\n",
      "   Batch Predictions with Confidence:\n",
      "   Sample 1: setosa (confidence: 0.985) vs setosa âœ…\n",
      "   Sample 2: virginica (confidence: 0.608) vs virginica âœ…\n",
      "   Sample 3: versicolor (confidence: 0.809) vs versicolor âœ…\n",
      "   Sample 4: versicolor (confidence: 0.840) vs versicolor âœ…\n",
      "   Sample 5: setosa (confidence: 0.988) vs setosa âœ…\n",
      "\\nğŸ“‹ Model Metadata and Information...\n",
      "\\nğŸ·ï¸  Model Registry Information:\n",
      "   Name: iris-classifier-production\n",
      "   Version: 2\n",
      "   Stage: None\n",
      "   Status: READY\n",
      "   Description: None\n",
      "   Creation Time: 1756448184335\n",
      "   Source Run: 1e6590f9b61749c782be833208a47b83\n",
      "\\nâœ… Model loading and inference demonstration complete!\n",
      "\\nğŸ’¡ Model Loading Best Practices:\n",
      "   âœ“ Use stage-based loading for production systems\n",
      "   âœ“ Load by version for specific model comparisons\n",
      "   âœ“ Load by run ID for debugging and development\n",
      "   âœ“ Always validate loaded models before use\n",
      "   âœ“ Handle loading exceptions gracefully\n",
      "   âœ“ Log inference requests and results for monitoring\n",
      "   âœ“ Cache loaded models for better performance\n",
      "   âœ“ Implement model health checks in production\n",
      "\\nğŸ¯ Model Serving Options:\n",
      "   ğŸŒ REST API: mlflow models serve\n",
      "   â˜ï¸  Cloud: Deploy to AWS SageMaker, Azure ML, etc.\n",
      "   ğŸ³ Docker: mlflow models build-docker\n",
      "   âš¡ Real-time: Apache Spark, Ray Serve\n",
      "   ğŸ“± Edge: Convert to TensorFlow Lite, ONNX\n",
      "   Batch Predictions with Confidence:\n",
      "   Sample 1: setosa (confidence: 0.985) vs setosa âœ…\n",
      "   Sample 2: virginica (confidence: 0.608) vs virginica âœ…\n",
      "   Sample 3: versicolor (confidence: 0.809) vs versicolor âœ…\n",
      "   Sample 4: versicolor (confidence: 0.840) vs versicolor âœ…\n",
      "   Sample 5: setosa (confidence: 0.988) vs setosa âœ…\n",
      "\\nğŸ“‹ Model Metadata and Information...\n",
      "\\nğŸ·ï¸  Model Registry Information:\n",
      "   Name: iris-classifier-production\n",
      "   Version: 2\n",
      "   Stage: None\n",
      "   Status: READY\n",
      "   Description: None\n",
      "   Creation Time: 1756448184335\n",
      "   Source Run: 1e6590f9b61749c782be833208a47b83\n",
      "\\nâœ… Model loading and inference demonstration complete!\n",
      "\\nğŸ’¡ Model Loading Best Practices:\n",
      "   âœ“ Use stage-based loading for production systems\n",
      "   âœ“ Load by version for specific model comparisons\n",
      "   âœ“ Load by run ID for debugging and development\n",
      "   âœ“ Always validate loaded models before use\n",
      "   âœ“ Handle loading exceptions gracefully\n",
      "   âœ“ Log inference requests and results for monitoring\n",
      "   âœ“ Cache loaded models for better performance\n",
      "   âœ“ Implement model health checks in production\n",
      "\\nğŸ¯ Model Serving Options:\n",
      "   ğŸŒ REST API: mlflow models serve\n",
      "   â˜ï¸  Cloud: Deploy to AWS SageMaker, Azure ML, etc.\n",
      "   ğŸ³ Docker: mlflow models build-docker\n",
      "   âš¡ Real-time: Apache Spark, Ray Serve\n",
      "   ğŸ“± Edge: Convert to TensorFlow Lite, ONNX\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate loading and using registered models\n",
    "print(\"ğŸ“¥ Loading and Using Registered Models...\")\n",
    "\n",
    "# === METHOD 1: LOAD BY STAGE ===\n",
    "print(\"\\\\nğŸ­ Method 1: Loading model by stage...\")\n",
    "\n",
    "try:\n",
    "    # Load the current production model\n",
    "    # This is the most common approach in production systems\n",
    "    production_model_uri = f\"models:/{registered_model_name}/Production\"\n",
    "    production_model = mlflow.pyfunc.load_model(production_model_uri)\n",
    "    \n",
    "    print(f\"âœ… Loaded production model from: {production_model_uri}\")\n",
    "    \n",
    "    # Make predictions with the loaded model\n",
    "    print(\"\\\\nğŸ”® Making predictions with production model...\")\n",
    "    \n",
    "    # Use test data for predictions\n",
    "    predictions = production_model.predict(X_test)\n",
    "    \n",
    "    print(f\"   ğŸ“Š Predictions shape: {predictions.shape}\")\n",
    "    print(f\"   ğŸ¯ Sample predictions: {predictions[:5]}\")\n",
    "    \n",
    "    # Calculate and display performance\n",
    "    prod_accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"   ğŸ“ˆ Production Model Accuracy: {prod_accuracy:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Could not load production model: {e}\")\n",
    "    print(\"   This is normal if no model is in Production stage yet\")\n",
    "\n",
    "# === METHOD 2: LOAD BY VERSION ===\n",
    "print(\"\\\\nğŸ”¢ Method 2: Loading model by specific version...\")\n",
    "\n",
    "try:\n",
    "    # Get the latest version number\n",
    "    client = MlflowClient()\n",
    "    latest_versions = client.get_latest_versions(registered_model_name, stages=[\"None\", \"Staging\", \"Production\"])\n",
    "    \n",
    "    if latest_versions:\n",
    "        latest_version = latest_versions[0].version\n",
    "        \n",
    "        # Load specific version\n",
    "        version_model_uri = f\"models:/{registered_model_name}/{latest_version}\"\n",
    "        version_model = mlflow.pyfunc.load_model(version_model_uri)\n",
    "        \n",
    "        print(f\"âœ… Loaded model version {latest_version} from: {version_model_uri}\")\n",
    "        \n",
    "        # Make predictions\n",
    "        version_predictions = version_model.predict(X_test)\n",
    "        version_accuracy = accuracy_score(y_test, version_predictions)\n",
    "        print(f\"   ğŸ“ˆ Version {latest_version} Accuracy: {version_accuracy:.4f}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"   âš ï¸  No model versions found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Could not load model by version: {e}\")\n",
    "\n",
    "# === METHOD 3: LOAD BY RUN ID ===\n",
    "print(\"\\\\nğŸ†” Method 3: Loading model by run ID...\")\n",
    "\n",
    "# For demonstration, let's train a new model and load it by run ID\n",
    "with mlflow.start_run(run_name=\"model-for-run-id-demo\") as demo_run:\n",
    "    \n",
    "    # Train a simple model\n",
    "    demo_model = LogisticRegression(random_state=42)\n",
    "    demo_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Log the model\n",
    "    demo_signature = infer_signature(X_train, demo_model.predict(X_train))\n",
    "    demo_model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=demo_model,\n",
    "        artifact_path=\"demo_model\",\n",
    "        signature=demo_signature\n",
    "    )\n",
    "    \n",
    "    demo_run_id = demo_run.info.run_id\n",
    "    print(f\"   ğŸ†” Demo run ID: {demo_run_id}\")\n",
    "\n",
    "# Load model by run ID\n",
    "run_model_uri = f\"runs:/{demo_run_id}/demo_model\"\n",
    "run_model = mlflow.pyfunc.load_model(run_model_uri)\n",
    "\n",
    "print(f\"âœ… Loaded model from run: {run_model_uri}\")\n",
    "\n",
    "# Make predictions\n",
    "run_predictions = run_model.predict(X_test)\n",
    "run_accuracy = accuracy_score(y_test, run_predictions)\n",
    "print(f\"   ğŸ“ˆ Run Model Accuracy: {run_accuracy:.4f}\")\n",
    "\n",
    "# === MODEL COMPARISON AND ANALYSIS ===\n",
    "print(\"\\\\nğŸ“Š Model Loading and Inference Analysis...\")\n",
    "\n",
    "# Create a comprehensive prediction analysis\n",
    "prediction_results = pd.DataFrame({\n",
    "    'actual': y_test,\n",
    "    'actual_name': [target_names[i] for i in y_test]\n",
    "})\n",
    "\n",
    "# Add predictions from different loading methods\n",
    "if 'predictions' in locals():\n",
    "    prediction_results['production_pred'] = predictions\n",
    "    prediction_results['production_correct'] = y_test == predictions\n",
    "\n",
    "if 'version_predictions' in locals():\n",
    "    prediction_results['version_pred'] = version_predictions\n",
    "    prediction_results['version_correct'] = y_test == version_predictions\n",
    "\n",
    "prediction_results['run_pred'] = run_predictions\n",
    "prediction_results['run_correct'] = y_test == run_predictions\n",
    "\n",
    "print(\"\\\\nğŸ“‹ Prediction Comparison (first 10 rows):\")\n",
    "print(prediction_results.head(10))\n",
    "\n",
    "# Calculate consistency between models\n",
    "if 'predictions' in locals() and 'version_predictions' in locals():\n",
    "    consistency = np.mean(predictions == version_predictions)\n",
    "    print(f\"\\\\nğŸ”„ Model Consistency:\")\n",
    "    print(f\"   Production vs Version: {consistency:.4f} agreement\")\n",
    "\n",
    "# === PRACTICAL INFERENCE EXAMPLES ===\n",
    "print(\"\\\\nğŸš€ Practical Inference Examples...\")\n",
    "\n",
    "# Example 1: Single prediction\n",
    "print(\"\\\\nğŸŒ¸ Example 1: Single flower prediction\")\n",
    "single_flower = X_test[0:1]  # First test sample\n",
    "single_prediction = run_model.predict(single_flower)[0]\n",
    "single_actual = y_test[0]\n",
    "\n",
    "print(f\"   Input features: {single_flower[0]}\")\n",
    "print(f\"   Predicted class: {target_names[int(single_prediction)]} (index: {single_prediction})\")\n",
    "print(f\"   Actual class: {target_names[single_actual]} (index: {single_actual})\")\n",
    "print(f\"   Correct: {'âœ…' if single_prediction == single_actual else 'âŒ'}\")\n",
    "\n",
    "# Example 2: Batch prediction with confidence\n",
    "print(\"\\\\nğŸŒ» Example 2: Batch prediction with confidence\")\n",
    "\n",
    "# Load the sklearn model directly for probability scores\n",
    "try:\n",
    "    sklearn_model = mlflow.sklearn.load_model(run_model_uri)\n",
    "    batch_samples = X_test[:5]\n",
    "    batch_predictions = sklearn_model.predict(batch_samples)\n",
    "    batch_probabilities = sklearn_model.predict_proba(batch_samples)\n",
    "    \n",
    "    print(\"   Batch Predictions with Confidence:\")\n",
    "    for i, (pred, prob, actual) in enumerate(zip(batch_predictions, batch_probabilities, y_test[:5])):\n",
    "        confidence = np.max(prob)\n",
    "        predicted_class = target_names[int(pred)]\n",
    "        actual_class = target_names[actual]\n",
    "        status = 'âœ…' if pred == actual else 'âŒ'\n",
    "        \n",
    "        print(f\"   Sample {i+1}: {predicted_class} (confidence: {confidence:.3f}) vs {actual_class} {status}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸  Could not load sklearn model directly: {e}\")\n",
    "\n",
    "# === MODEL METADATA AND INFORMATION ===\n",
    "print(\"\\\\nğŸ“‹ Model Metadata and Information...\")\n",
    "\n",
    "try:\n",
    "    # Get model information\n",
    "    model_version_info = client.get_model_version(registered_model_name, latest_version)\n",
    "    \n",
    "    print(f\"\\\\nğŸ·ï¸  Model Registry Information:\")\n",
    "    print(f\"   Name: {model_version_info.name}\")\n",
    "    print(f\"   Version: {model_version_info.version}\")\n",
    "    print(f\"   Stage: {model_version_info.current_stage}\")\n",
    "    print(f\"   Status: {model_version_info.status}\")\n",
    "    print(f\"   Description: {model_version_info.description}\")\n",
    "    print(f\"   Creation Time: {model_version_info.creation_timestamp}\")\n",
    "    print(f\"   Source Run: {model_version_info.run_id}\")\n",
    "    \n",
    "    # Show tags\n",
    "    if model_version_info.tags:\n",
    "        print(f\"   Tags:\")\n",
    "        for tag_key, tag_value in model_version_info.tags.items():\n",
    "            print(f\"      {tag_key}: {tag_value}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"   âš ï¸  Could not retrieve model metadata: {e}\")\n",
    "\n",
    "print(\"\\\\nâœ… Model loading and inference demonstration complete!\")\n",
    "\n",
    "print(\"\\\\nğŸ’¡ Model Loading Best Practices:\")\n",
    "print(\"   âœ“ Use stage-based loading for production systems\")\n",
    "print(\"   âœ“ Load by version for specific model comparisons\")\n",
    "print(\"   âœ“ Load by run ID for debugging and development\")\n",
    "print(\"   âœ“ Always validate loaded models before use\")\n",
    "print(\"   âœ“ Handle loading exceptions gracefully\")\n",
    "print(\"   âœ“ Log inference requests and results for monitoring\")\n",
    "print(\"   âœ“ Cache loaded models for better performance\")\n",
    "print(\"   âœ“ Implement model health checks in production\")\n",
    "\n",
    "print(\"\\\\nğŸ¯ Model Serving Options:\")\n",
    "print(\"   ğŸŒ REST API: mlflow models serve\")\n",
    "print(\"   â˜ï¸  Cloud: Deploy to AWS SageMaker, Azure ML, etc.\")\n",
    "print(\"   ğŸ³ Docker: mlflow models build-docker\")\n",
    "print(\"   âš¡ Real-time: Apache Spark, Ray Serve\")\n",
    "print(\"   ğŸ“± Edge: Convert to TensorFlow Lite, ONNX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf93282",
   "metadata": {},
   "source": [
    "## 11. Compare Multiple Experiment Runs\n",
    "\n",
    "One of MLflow's most powerful features is the ability to compare multiple experiments systematically. This enables data-driven decision making and hypothesis testing in ML development.\n",
    "\n",
    "### What to Compare:\n",
    "- **Different algorithms**: Logistic Regression vs Random Forest vs SVM\n",
    "- **Hyperparameter variations**: Different learning rates, regularization values\n",
    "- **Feature engineering**: Different preprocessing techniques\n",
    "- **Data variations**: Different train/test splits, feature selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73471620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ Comprehensive Experiment Comparison...\n",
      "\\nğŸ§ª Experiment 1: Comparing Different Algorithms...\n",
      "\\n   ğŸš€ Training Logistic Regression...\n",
      "      âœ… Logistic Regression - Accuracy: 0.9667, Time: 0.0990s\n",
      "\\n   ğŸš€ Training Random Forest...\n",
      "      âœ… Logistic Regression - Accuracy: 0.9667, Time: 0.0990s\n",
      "\\n   ğŸš€ Training Random Forest...\n",
      "      âœ… Random Forest - Accuracy: 0.9000, Time: 0.3229s\n",
      "\\nğŸ›ï¸  Experiment 2: Logistic Regression Hyperparameter Tuning...\n",
      "\\n   ğŸ”§ Testing C = 0.1...\n",
      "      ğŸ“Š C=0.1: Accuracy = 0.9667\n",
      "\\n   ğŸ”§ Testing C = 0.5...\n",
      "      âœ… Random Forest - Accuracy: 0.9000, Time: 0.3229s\n",
      "\\nğŸ›ï¸  Experiment 2: Logistic Regression Hyperparameter Tuning...\n",
      "\\n   ğŸ”§ Testing C = 0.1...\n",
      "      ğŸ“Š C=0.1: Accuracy = 0.9667\n",
      "\\n   ğŸ”§ Testing C = 0.5...\n",
      "      ğŸ“Š C=0.5: Accuracy = 0.9667\n",
      "\\n   ğŸ”§ Testing C = 1.0...\n",
      "      ğŸ“Š C=1.0: Accuracy = 0.9667\n",
      "\\n   ğŸ”§ Testing C = 2.0...\n",
      "      ğŸ“Š C=0.5: Accuracy = 0.9667\n",
      "\\n   ğŸ”§ Testing C = 1.0...\n",
      "      ğŸ“Š C=1.0: Accuracy = 0.9667\n",
      "\\n   ğŸ”§ Testing C = 2.0...\n",
      "      ğŸ“Š C=2.0: Accuracy = 0.9667\n",
      "\\n   ğŸ”§ Testing C = 5.0...\n",
      "      ğŸ“Š C=5.0: Accuracy = 1.0000\n",
      "\\nâš™ï¸  Experiment 3: Feature Engineering Comparison...\n",
      "\\n   ğŸ”¬ Testing No Scaling...\n",
      "      ğŸ“Š C=2.0: Accuracy = 0.9667\n",
      "\\n   ğŸ”§ Testing C = 5.0...\n",
      "      ğŸ“Š C=5.0: Accuracy = 1.0000\n",
      "\\nâš™ï¸  Experiment 3: Feature Engineering Comparison...\n",
      "\\n   ğŸ”¬ Testing No Scaling...\n",
      "      ğŸ“ˆ No Scaling: Accuracy = 0.9667\n",
      "\\n   ğŸ”¬ Testing Standard Scaling...\n",
      "      ğŸ“ˆ Standard Scaling: Accuracy = 0.9333\n",
      "\\n   ğŸ”¬ Testing MinMax Scaling...\n",
      "      ğŸ“ˆ MinMax Scaling: Accuracy = 0.9000\n",
      "\\nğŸ“Š Comprehensive Results Analysis...\n",
      "\\nğŸ† Algorithm Comparison Results:\n",
      "             algorithm                            run_id  accuracy  precision  \\\n",
      "0  Logistic Regression  e45821eeede84432b8c4a958d396e1df    0.9667     0.9697   \n",
      "1        Random Forest  80cb2877b19e4c16bd419e3a402034c2    0.9000     0.9024   \n",
      "\n",
      "   recall  f1_score  training_time  \n",
      "0  0.9667    0.9666         0.0990  \n",
      "1  0.9000    0.8997         0.3229  \n",
      "\\nğŸ›ï¸  Hyperparameter Tuning Results:\n",
      "     C                            run_id  accuracy  precision  recall  \\\n",
      "0  0.1  038dab9226f94bec9c47fd2cdf64bb21    0.9667     0.9697  0.9667   \n",
      "1  0.5  de21e7c389494cf29d36b92923fa95f5    0.9667     0.9697  0.9667   \n",
      "2  1.0  d3d4fe3cb16d490aa88c0ed52b18ea86    0.9667     0.9697  0.9667   \n",
      "3  2.0  efc8623045184b369bbb34adbe62f1f7    0.9667     0.9697  0.9667   \n",
      "4  5.0  23e0dfb86a6448d388d558d04bb31987    1.0000     1.0000  1.0000   \n",
      "\n",
      "   f1_score  \n",
      "0    0.9666  \n",
      "1    0.9666  \n",
      "2    0.9666  \n",
      "3    0.9666  \n",
      "4    1.0000  \n",
      "\\nâš™ï¸  Feature Engineering Results:\n",
      "            scaling                            run_id  accuracy  precision  \\\n",
      "0        No Scaling  cb5b691d5f2a41738bcc42f548a5d881    0.9667     0.9697   \n",
      "1  Standard Scaling  0e19dbe1cf2a45bd953cc467e4e34605    0.9333     0.9333   \n",
      "2    MinMax Scaling  bee7eeaf35dd43c28bc697d90b745a4d    0.9000     0.9024   \n",
      "\n",
      "   recall  f1_score  \n",
      "0  0.9667    0.9666  \n",
      "1  0.9333    0.9333  \n",
      "2  0.9000    0.8997  \n",
      "\\nğŸ¥‡ Best Performers Summary:\n",
      "   ğŸ¯ Best Algorithm: Logistic Regression (Accuracy: 0.9667)\n",
      "   ğŸ›ï¸  Best C Value: 5.0 (Accuracy: 1.0000)\n",
      "   âš™ï¸  Best Scaling: No Scaling (Accuracy: 0.9667)\n",
      "\\nğŸ” Programmatic Experiment Querying...\n",
      "      ğŸ“ˆ No Scaling: Accuracy = 0.9667\n",
      "\\n   ğŸ”¬ Testing Standard Scaling...\n",
      "      ğŸ“ˆ Standard Scaling: Accuracy = 0.9333\n",
      "\\n   ğŸ”¬ Testing MinMax Scaling...\n",
      "      ğŸ“ˆ MinMax Scaling: Accuracy = 0.9000\n",
      "\\nğŸ“Š Comprehensive Results Analysis...\n",
      "\\nğŸ† Algorithm Comparison Results:\n",
      "             algorithm                            run_id  accuracy  precision  \\\n",
      "0  Logistic Regression  e45821eeede84432b8c4a958d396e1df    0.9667     0.9697   \n",
      "1        Random Forest  80cb2877b19e4c16bd419e3a402034c2    0.9000     0.9024   \n",
      "\n",
      "   recall  f1_score  training_time  \n",
      "0  0.9667    0.9666         0.0990  \n",
      "1  0.9000    0.8997         0.3229  \n",
      "\\nğŸ›ï¸  Hyperparameter Tuning Results:\n",
      "     C                            run_id  accuracy  precision  recall  \\\n",
      "0  0.1  038dab9226f94bec9c47fd2cdf64bb21    0.9667     0.9697  0.9667   \n",
      "1  0.5  de21e7c389494cf29d36b92923fa95f5    0.9667     0.9697  0.9667   \n",
      "2  1.0  d3d4fe3cb16d490aa88c0ed52b18ea86    0.9667     0.9697  0.9667   \n",
      "3  2.0  efc8623045184b369bbb34adbe62f1f7    0.9667     0.9697  0.9667   \n",
      "4  5.0  23e0dfb86a6448d388d558d04bb31987    1.0000     1.0000  1.0000   \n",
      "\n",
      "   f1_score  \n",
      "0    0.9666  \n",
      "1    0.9666  \n",
      "2    0.9666  \n",
      "3    0.9666  \n",
      "4    1.0000  \n",
      "\\nâš™ï¸  Feature Engineering Results:\n",
      "            scaling                            run_id  accuracy  precision  \\\n",
      "0        No Scaling  cb5b691d5f2a41738bcc42f548a5d881    0.9667     0.9697   \n",
      "1  Standard Scaling  0e19dbe1cf2a45bd953cc467e4e34605    0.9333     0.9333   \n",
      "2    MinMax Scaling  bee7eeaf35dd43c28bc697d90b745a4d    0.9000     0.9024   \n",
      "\n",
      "   recall  f1_score  \n",
      "0  0.9667    0.9666  \n",
      "1  0.9333    0.9333  \n",
      "2  0.9000    0.8997  \n",
      "\\nğŸ¥‡ Best Performers Summary:\n",
      "   ğŸ¯ Best Algorithm: Logistic Regression (Accuracy: 0.9667)\n",
      "   ğŸ›ï¸  Best C Value: 5.0 (Accuracy: 1.0000)\n",
      "   âš™ï¸  Best Scaling: No Scaling (Accuracy: 0.9667)\n",
      "\\nğŸ” Programmatic Experiment Querying...\n",
      "\\nğŸ“‹ Querying experiment: iris-classification-tutorial (ID: 636605946707093340)\n",
      "\\nğŸ“‹ Querying experiment: iris-classification-tutorial (ID: 636605946707093340)\n",
      "   ğŸ“ˆ Total runs in experiment: 25\n",
      "   ğŸ“ˆ Total runs in experiment: 25\n",
      "   ğŸ§ª Algorithm comparison runs: 2\n",
      "\\nğŸ† Overall Best Run:\n",
      "   Run ID: 23e0dfb86a6448d388d558d04bb31987\n",
      "   Accuracy: 1.0000\n",
      "   Algorithm: logistic_regression\n",
      "   Tags: hyperparameter_tuning\n",
      "\\nğŸ“Š Visualization and Analysis Recommendations:\n",
      "\\nğŸ’¡ To visualize these results, you can:\n",
      "   ğŸ“ˆ Use MLflow UI to compare runs side-by-side\n",
      "   ğŸ“Š Create custom plots comparing metrics across runs\n",
      "   ğŸ¯ Use parallel coordinates plots for hyperparameter analysis\n",
      "   ğŸ“‹ Export run data to pandas for custom analysis\n",
      "   ğŸ” Filter runs by tags and parameters for focused comparisons\n",
      "\\nğŸ¯ Advanced Analysis Ideas:\n",
      "   ğŸ“Š Statistical significance testing between models\n",
      "   ğŸ”„ Cross-validation across multiple runs\n",
      "   ğŸ“ˆ Learning curves for training progression\n",
      "   ğŸ›ï¸  Hyperparameter optimization with tools like Optuna\n",
      "   ğŸ“‹ A/B testing frameworks for model comparison\n",
      "\\nâœ… Comprehensive experiment comparison complete!\n",
      "\\nğŸš€ Next Steps:\n",
      "   1. Open MLflow UI to visually compare runs\n",
      "   2. Export best model to production\n",
      "   3. Set up automated hyperparameter tuning\n",
      "   4. Implement model monitoring and drift detection\n",
      "   5. Scale experiments with MLflow Projects\n",
      "   ğŸ§ª Algorithm comparison runs: 2\n",
      "\\nğŸ† Overall Best Run:\n",
      "   Run ID: 23e0dfb86a6448d388d558d04bb31987\n",
      "   Accuracy: 1.0000\n",
      "   Algorithm: logistic_regression\n",
      "   Tags: hyperparameter_tuning\n",
      "\\nğŸ“Š Visualization and Analysis Recommendations:\n",
      "\\nğŸ’¡ To visualize these results, you can:\n",
      "   ğŸ“ˆ Use MLflow UI to compare runs side-by-side\n",
      "   ğŸ“Š Create custom plots comparing metrics across runs\n",
      "   ğŸ¯ Use parallel coordinates plots for hyperparameter analysis\n",
      "   ğŸ“‹ Export run data to pandas for custom analysis\n",
      "   ğŸ” Filter runs by tags and parameters for focused comparisons\n",
      "\\nğŸ¯ Advanced Analysis Ideas:\n",
      "   ğŸ“Š Statistical significance testing between models\n",
      "   ğŸ”„ Cross-validation across multiple runs\n",
      "   ğŸ“ˆ Learning curves for training progression\n",
      "   ğŸ›ï¸  Hyperparameter optimization with tools like Optuna\n",
      "   ğŸ“‹ A/B testing frameworks for model comparison\n",
      "\\nâœ… Comprehensive experiment comparison complete!\n",
      "\\nğŸš€ Next Steps:\n",
      "   1. Open MLflow UI to visually compare runs\n",
      "   2. Export best model to production\n",
      "   3. Set up automated hyperparameter tuning\n",
      "   4. Implement model monitoring and drift detection\n",
      "   5. Scale experiments with MLflow Projects\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate comprehensive experiment comparison\n",
    "print(\"ğŸ”¬ Comprehensive Experiment Comparison...\")\n",
    "\n",
    "# === EXPERIMENT 1: DIFFERENT ALGORITHMS ===\n",
    "print(\"\\\\nğŸ§ª Experiment 1: Comparing Different Algorithms...\")\n",
    "\n",
    "algorithms = [\n",
    "    {\n",
    "        \"name\": \"Logistic Regression\",\n",
    "        \"model\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "        \"params\": {\"algorithm\": \"logistic_regression\", \"solver\": \"lbfgs\", \"max_iter\": 1000}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Random Forest\",\n",
    "        \"model\": RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "        \"params\": {\"algorithm\": \"random_forest\", \"n_estimators\": 100, \"max_depth\": None}\n",
    "    }\n",
    "]\n",
    "\n",
    "algorithm_results = []\n",
    "\n",
    "for algo_config in algorithms:\n",
    "    with mlflow.start_run(run_name=f\"algorithm-comparison-{algo_config['name'].lower().replace(' ', '-')}\"):\n",
    "        \n",
    "        print(f\"\\\\n   ğŸš€ Training {algo_config['name']}...\")\n",
    "        \n",
    "        # Train the model\n",
    "        model = algo_config['model']\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred, average='weighted'),\n",
    "            \"recall\": recall_score(y_test, y_pred, average='weighted'),\n",
    "            \"f1_score\": f1_score(y_test, y_pred, average='weighted'),\n",
    "            \"training_time\": training_time\n",
    "        }\n",
    "        \n",
    "        # Log parameters and metrics\n",
    "        mlflow.log_params(algo_config['params'])\n",
    "        mlflow.log_metrics(metrics)\n",
    "        \n",
    "        # Add tags\n",
    "        mlflow.set_tag(\"experiment_type\", \"algorithm_comparison\")\n",
    "        mlflow.set_tag(\"algorithm\", algo_config['params']['algorithm'])\n",
    "        \n",
    "        # Store results for comparison\n",
    "        result = {\n",
    "            \"algorithm\": algo_config['name'],\n",
    "            \"run_id\": mlflow.active_run().info.run_id,\n",
    "            **metrics\n",
    "        }\n",
    "        algorithm_results.append(result)\n",
    "        \n",
    "        print(f\"      âœ… {algo_config['name']} - Accuracy: {metrics['accuracy']:.4f}, Time: {training_time:.4f}s\")\n",
    "\n",
    "# === EXPERIMENT 2: HYPERPARAMETER TUNING ===\n",
    "print(\"\\\\nğŸ›ï¸  Experiment 2: Logistic Regression Hyperparameter Tuning...\")\n",
    "\n",
    "# Test different C values (regularization strength)\n",
    "c_values = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "hyperparameter_results = []\n",
    "\n",
    "for c_value in c_values:\n",
    "    with mlflow.start_run(run_name=f\"lr-hyperparameter-C-{c_value}\"):\n",
    "        \n",
    "        print(f\"\\\\n   ğŸ”§ Testing C = {c_value}...\")\n",
    "        \n",
    "        # Train model with specific C value\n",
    "        model = LogisticRegression(C=c_value, random_state=42, max_iter=1000)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred, average='weighted'),\n",
    "            \"recall\": recall_score(y_test, y_pred, average='weighted'),\n",
    "            \"f1_score\": f1_score(y_test, y_pred, average='weighted')\n",
    "        }\n",
    "        \n",
    "        # Log parameters and metrics\n",
    "        mlflow.log_params({\n",
    "            \"algorithm\": \"logistic_regression\",\n",
    "            \"C\": c_value,\n",
    "            \"solver\": \"lbfgs\",\n",
    "            \"regularization\": \"l2\"\n",
    "        })\n",
    "        mlflow.log_metrics(metrics)\n",
    "        \n",
    "        # Tags for organization\n",
    "        mlflow.set_tag(\"experiment_type\", \"hyperparameter_tuning\")\n",
    "        mlflow.set_tag(\"parameter_tuned\", \"C\")\n",
    "        \n",
    "        result = {\n",
    "            \"C\": c_value,\n",
    "            \"run_id\": mlflow.active_run().info.run_id,\n",
    "            **metrics\n",
    "        }\n",
    "        hyperparameter_results.append(result)\n",
    "        \n",
    "        print(f\"      ğŸ“Š C={c_value}: Accuracy = {metrics['accuracy']:.4f}\")\n",
    "\n",
    "# === EXPERIMENT 3: FEATURE ENGINEERING ===\n",
    "print(\"\\\\nâš™ï¸  Experiment 3: Feature Engineering Comparison...\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "feature_configs = [\n",
    "    {\"name\": \"No Scaling\", \"scaler\": None},\n",
    "    {\"name\": \"Standard Scaling\", \"scaler\": StandardScaler()},\n",
    "    {\"name\": \"MinMax Scaling\", \"scaler\": MinMaxScaler()}\n",
    "]\n",
    "\n",
    "feature_results = []\n",
    "\n",
    "for config in feature_configs:\n",
    "    with mlflow.start_run(run_name=f\"feature-engineering-{config['name'].lower().replace(' ', '-')}\"):\n",
    "        \n",
    "        print(f\"\\\\n   ğŸ”¬ Testing {config['name']}...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        if config['scaler'] is not None:\n",
    "            X_train_scaled = config['scaler'].fit_transform(X_train)\n",
    "            X_test_scaled = config['scaler'].transform(X_test)\n",
    "        else:\n",
    "            X_train_scaled = X_train\n",
    "            X_test_scaled = X_test\n",
    "        \n",
    "        # Train model\n",
    "        model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred, average='weighted'),\n",
    "            \"recall\": recall_score(y_test, y_pred, average='weighted'),\n",
    "            \"f1_score\": f1_score(y_test, y_pred, average='weighted')\n",
    "        }\n",
    "        \n",
    "        # Log parameters and metrics\n",
    "        mlflow.log_params({\n",
    "            \"algorithm\": \"logistic_regression\",\n",
    "            \"feature_scaling\": config['name'].lower().replace(' ', '_'),\n",
    "            \"scaler_type\": type(config['scaler']).__name__ if config['scaler'] else \"none\"\n",
    "        })\n",
    "        mlflow.log_metrics(metrics)\n",
    "        \n",
    "        # Tags\n",
    "        mlflow.set_tag(\"experiment_type\", \"feature_engineering\")\n",
    "        mlflow.set_tag(\"scaling_method\", config['name'])\n",
    "        \n",
    "        result = {\n",
    "            \"scaling\": config['name'],\n",
    "            \"run_id\": mlflow.active_run().info.run_id,\n",
    "            **metrics\n",
    "        }\n",
    "        feature_results.append(result)\n",
    "        \n",
    "        print(f\"      ğŸ“ˆ {config['name']}: Accuracy = {metrics['accuracy']:.4f}\")\n",
    "\n",
    "# === ANALYZE AND COMPARE RESULTS ===\n",
    "print(\"\\\\nğŸ“Š Comprehensive Results Analysis...\")\n",
    "\n",
    "# Create comparison DataFrames\n",
    "print(\"\\\\nğŸ† Algorithm Comparison Results:\")\n",
    "algo_df = pd.DataFrame(algorithm_results)\n",
    "print(algo_df.round(4))\n",
    "\n",
    "print(\"\\\\nğŸ›ï¸  Hyperparameter Tuning Results:\")\n",
    "hyperparam_df = pd.DataFrame(hyperparameter_results)\n",
    "print(hyperparam_df.round(4))\n",
    "\n",
    "print(\"\\\\nâš™ï¸  Feature Engineering Results:\")\n",
    "feature_df = pd.DataFrame(feature_results)\n",
    "print(feature_df.round(4))\n",
    "\n",
    "# Find best performers\n",
    "print(\"\\\\nğŸ¥‡ Best Performers Summary:\")\n",
    "\n",
    "best_algorithm = algo_df.loc[algo_df['accuracy'].idxmax()]\n",
    "print(f\"   ğŸ¯ Best Algorithm: {best_algorithm['algorithm']} (Accuracy: {best_algorithm['accuracy']:.4f})\")\n",
    "\n",
    "best_hyperparameter = hyperparam_df.loc[hyperparam_df['accuracy'].idxmax()]\n",
    "print(f\"   ğŸ›ï¸  Best C Value: {best_hyperparameter['C']} (Accuracy: {best_hyperparameter['accuracy']:.4f})\")\n",
    "\n",
    "best_feature = feature_df.loc[feature_df['accuracy'].idxmax()]\n",
    "print(f\"   âš™ï¸  Best Scaling: {best_feature['scaling']} (Accuracy: {best_feature['accuracy']:.4f})\")\n",
    "\n",
    "# === PROGRAMMATIC EXPERIMENT QUERYING ===\n",
    "print(\"\\\\nğŸ” Programmatic Experiment Querying...\")\n",
    "\n",
    "# Get experiment ID\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "experiment_id = experiment.experiment_id\n",
    "\n",
    "# Search runs programmatically\n",
    "print(f\"\\\\nğŸ“‹ Querying experiment: {experiment_name} (ID: {experiment_id})\")\n",
    "\n",
    "# Query all runs in this experiment\n",
    "all_runs = mlflow.search_runs(experiment_ids=[experiment_id])\n",
    "print(f\"   ğŸ“ˆ Total runs in experiment: {len(all_runs)}\")\n",
    "\n",
    "# Filter runs by tag\n",
    "algorithm_comparison_runs = mlflow.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    filter_string=\"tags.experiment_type = 'algorithm_comparison'\"\n",
    ")\n",
    "print(f\"   ğŸ§ª Algorithm comparison runs: {len(algorithm_comparison_runs)}\")\n",
    "\n",
    "# Get best run by metric\n",
    "if len(all_runs) > 0:\n",
    "    best_run = all_runs.loc[all_runs['metrics.accuracy'].idxmax()]\n",
    "    print(f\"\\\\nğŸ† Overall Best Run:\")\n",
    "    print(f\"   Run ID: {best_run['run_id']}\")\n",
    "    print(f\"   Accuracy: {best_run['metrics.accuracy']:.4f}\")\n",
    "    print(f\"   Algorithm: {best_run.get('params.algorithm', 'Unknown')}\")\n",
    "    print(f\"   Tags: {best_run.get('tags.experiment_type', 'Unknown')}\")\n",
    "\n",
    "# === VISUALIZATION RECOMMENDATIONS ===\n",
    "print(\"\\\\nğŸ“Š Visualization and Analysis Recommendations:\")\n",
    "print(\"\\\\nğŸ’¡ To visualize these results, you can:\")\n",
    "print(\"   ğŸ“ˆ Use MLflow UI to compare runs side-by-side\")\n",
    "print(\"   ğŸ“Š Create custom plots comparing metrics across runs\")\n",
    "print(\"   ğŸ¯ Use parallel coordinates plots for hyperparameter analysis\")\n",
    "print(\"   ğŸ“‹ Export run data to pandas for custom analysis\")\n",
    "print(\"   ğŸ” Filter runs by tags and parameters for focused comparisons\")\n",
    "\n",
    "print(\"\\\\nğŸ¯ Advanced Analysis Ideas:\")\n",
    "print(\"   ğŸ“Š Statistical significance testing between models\")\n",
    "print(\"   ğŸ”„ Cross-validation across multiple runs\")\n",
    "print(\"   ğŸ“ˆ Learning curves for training progression\")\n",
    "print(\"   ğŸ›ï¸  Hyperparameter optimization with tools like Optuna\")\n",
    "print(\"   ğŸ“‹ A/B testing frameworks for model comparison\")\n",
    "\n",
    "print(\"\\\\nâœ… Comprehensive experiment comparison complete!\")\n",
    "print(\"\\\\nğŸš€ Next Steps:\")\n",
    "print(\"   1. Open MLflow UI to visually compare runs\")\n",
    "print(\"   2. Export best model to production\")\n",
    "print(\"   3. Set up automated hyperparameter tuning\")\n",
    "print(\"   4. Implement model monitoring and drift detection\")\n",
    "print(\"   5. Scale experiments with MLflow Projects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb395a2e",
   "metadata": {},
   "source": [
    "## ğŸ‰ Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You've completed a comprehensive MLflow tutorial covering all major aspects of experiment tracking and model management.\n",
    "\n",
    "### ğŸ“š What You've Learned:\n",
    "\n",
    "1. **âœ… MLflow Setup**: Configured tracking URI and experiment organization\n",
    "2. **âœ… Parameter Logging**: Tracked hyperparameters and configuration settings  \n",
    "3. **âœ… Metric Tracking**: Logged performance metrics and training progress\n",
    "4. **âœ… Artifact Management**: Saved models, plots, and data files\n",
    "5. **âœ… Model Registry**: Registered, versioned, and staged models\n",
    "6. **âœ… Model Loading**: Loaded models for inference and production use\n",
    "7. **âœ… Experiment Comparison**: Systematically compared multiple approaches\n",
    "\n",
    "### ğŸš€ Production Best Practices:\n",
    "\n",
    "- **ğŸ”§ Set up MLflow Tracking Server** for team collaboration\n",
    "- **ğŸ“Š Use consistent experiment naming** and tagging strategies  \n",
    "- **ğŸ¯ Implement automated model validation** before production deployment\n",
    "- **ğŸ“ˆ Monitor model performance** in production with drift detection\n",
    "- **ğŸ”„ Set up CI/CD pipelines** with MLflow for automated training and deployment\n",
    "- **ğŸ“‹ Document experiments** with detailed descriptions and metadata\n",
    "- **ğŸ›¡ï¸ Implement model governance** with approval workflows\n",
    "\n",
    "### ğŸ¯ Next Learning Steps:\n",
    "\n",
    "1. **MLflow Projects**: Package your ML code for reproducible runs\n",
    "2. **MLflow Plugins**: Extend MLflow with custom tracking backends\n",
    "3. **Model Serving**: Deploy models with `mlflow models serve`\n",
    "4. **Docker Integration**: Containerize models with `mlflow models build-docker`\n",
    "5. **Cloud Deployment**: Deploy to AWS SageMaker, Azure ML, or GCP\n",
    "6. **Advanced Tracking**: Custom metrics, nested runs, and system metrics\n",
    "7. **Integration**: Connect with Apache Airflow, Kubernetes, and Spark\n",
    "\n",
    "### ğŸ”— Useful Resources:\n",
    "\n",
    "- **ğŸ“– MLflow Documentation**: https://mlflow.org/docs/latest/\n",
    "- **ğŸ“ MLflow Tutorials**: https://mlflow.org/docs/latest/tutorials-and-examples/\n",
    "- **ğŸ’» GitHub Repository**: https://github.com/mlflow/mlflow\n",
    "- **ğŸª MLflow Model Registry**: https://mlflow.org/docs/latest/model-registry.html\n",
    "- **ğŸš€ Deployment Guide**: https://mlflow.org/docs/latest/models.html#deployment\n",
    "\n",
    "### ğŸ’¡ Remember:\n",
    "\n",
    "> *\"The best MLflow setup is the one your team actually uses consistently.\"*\n",
    "\n",
    "Start simple, iterate, and gradually add complexity as your ML operations mature. Focus on creating reproducible experiments and maintainable ML workflows.\n",
    "\n",
    "**Happy experimenting! ğŸ§ªâœ¨**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
