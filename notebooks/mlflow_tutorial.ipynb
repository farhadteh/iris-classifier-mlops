{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9cf6cd9",
   "metadata": {},
   "source": [
    "# Complete MLflow Tutorial: Learn Machine Learning Experiment Tracking\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this notebook, you'll understand:\n",
    "- How to set up and use MLflow for experiment tracking\n",
    "- How to log parameters, metrics, and artifacts\n",
    "- How to manage model versions with MLflow Model Registry\n",
    "- How to compare experiments and make data-driven decisions\n",
    "- Best practices for MLflow in production workflows\n",
    "\n",
    "## üìñ What is MLflow?\n",
    "MLflow is an open-source platform for managing the machine learning lifecycle, including:\n",
    "- **Tracking**: Record and query experiments (code, data, config, results)\n",
    "- **Projects**: Package data science code in a reusable, reproducible form\n",
    "- **Models**: Manage and deploy models from various ML libraries\n",
    "- **Registry**: Centralized model store for collaborative model management\n",
    "\n",
    "Let's dive into each component step by step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e454db8",
   "metadata": {},
   "source": [
    "## 1. Import MLflow and Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries. Each import serves a specific purpose in our ML workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e96648e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Farhad/Documents/ML_project/MLFLOW/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n",
      "MLflow version: 3.1.4\n"
     ]
    }
   ],
   "source": [
    "# MLflow - Main library for experiment tracking and model management\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature  # For automatically inferring model signatures\n",
    "from mlflow.tracking import MlflowClient    # Client for programmatic access to MLflow\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd                         # For data manipulation and analysis\n",
    "import numpy as np                          # For numerical operations\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn import datasets               # For loading sample datasets\n",
    "from sklearn.model_selection import train_test_split  # For splitting data\n",
    "from sklearn.linear_model import LogisticRegression   # Our ML model\n",
    "from sklearn.ensemble import RandomForestClassifier   # Alternative model for comparison\n",
    "from sklearn.metrics import (               # For calculating model performance metrics\n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt             # For creating plots\n",
    "import seaborn as sns                       # For advanced statistical visualizations\n",
    "\n",
    "# System and utility libraries\n",
    "import os                                   # For file system operations\n",
    "import warnings                             # For handling warnings\n",
    "warnings.filterwarnings('ignore')          # Suppress warnings for cleaner output\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"MLflow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58034a9",
   "metadata": {},
   "source": [
    "## 2. Set Up MLflow Tracking Server Connection\n",
    "\n",
    "MLflow can store tracking data in different locations:\n",
    "- **Local filesystem**: `file:./mlruns` (default)\n",
    "- **Database**: `sqlite:///mlflow.db` or `postgresql://...`\n",
    "- **Remote server**: `http://localhost:5000` or `http://your-server:5000`\n",
    "\n",
    "We'll configure our setup to connect to a local MLflow tracking server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "427be4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó MLflow tracking URI: file:./mlruns\n",
      "‚úÖ MLflow tracking setup complete!\n",
      "üìä Found 1 existing experiments\n",
      "   - iris-classification-tutorial (ID: 636605946707093340)\n"
     ]
    }
   ],
   "source": [
    "# Set the MLflow tracking URI\n",
    "# This tells MLflow where to store experiment data and artifacts\n",
    "# Options:\n",
    "# 1. Local file system: \"file:./mlruns\" \n",
    "# 2. Local server: \"http://127.0.0.1:8080\"\n",
    "# 3. Remote server: \"http://your-mlflow-server.com:5000\"\n",
    "\n",
    "# For this tutorial, we'll use local file-based tracking for simplicity\n",
    "# You can change this to \"http://127.0.0.1:8080\" if you have the server running\n",
    "tracking_uri = \"file:./mlruns\"\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "\n",
    "# Verify the tracking URI is set correctly\n",
    "current_uri = mlflow.get_tracking_uri()\n",
    "print(f\"üîó MLflow tracking URI: {current_uri}\")\n",
    "\n",
    "# Create an MLflow client for programmatic access\n",
    "# This allows us to interact with MLflow programmatically\n",
    "client = MlflowClient()\n",
    "\n",
    "print(\"‚úÖ MLflow tracking setup complete!\")\n",
    "\n",
    "# Let's check if we can connect and list any existing experiments\n",
    "try:\n",
    "    # Use search_experiments() instead of list_experiments() for newer MLflow versions\n",
    "    experiments = client.search_experiments()\n",
    "    print(f\"üìä Found {len(experiments)} existing experiments\")\n",
    "    for exp in experiments:\n",
    "        print(f\"   - {exp.name} (ID: {exp.experiment_id})\")\n",
    "except AttributeError:\n",
    "    # Fallback for older MLflow versions\n",
    "    try:\n",
    "        experiments = client.list_experiments()\n",
    "        print(f\"üìä Found {len(experiments)} existing experiments\")\n",
    "        for exp in experiments:\n",
    "            print(f\"   - {exp.name} (ID: {exp.experiment_id})\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Note: Could not list experiments: {e}\")\n",
    "        print(\"   This is normal if no experiments exist yet or if using file-based tracking.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Note: Could not connect to MLflow: {e}\")\n",
    "    print(\"   This is normal if no experiments exist yet or if using file-based tracking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4f69f1",
   "metadata": {},
   "source": [
    "## 3. Create and Start an MLflow Experiment\n",
    "\n",
    "**Experiments** in MLflow are containers for organizing related runs. Think of them as project folders where you group related model training attempts.\n",
    "\n",
    "### Best Practices for Experiment Naming:\n",
    "- Use descriptive names: `iris-classification-comparison`\n",
    "- Include version or date: `model-v2-2024`\n",
    "- Separate by model type: `logistic-regression-experiments`\n",
    "- Group by business objective: `customer-churn-prediction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ccf4761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Experiment: 'iris-classification-tutorial'\n",
      "üìç Experiment ID: 636605946707093340\n",
      "üìÇ Artifact Location: file:///Users/Farhad/Documents/ML_project/MLFLOW/mlruns/636605946707093340\n",
      "üè∑Ô∏è  Tag: project = ml-tutorial\n",
      "üè∑Ô∏è  Tag: dataset = iris\n",
      "üè∑Ô∏è  Tag: task = classification\n",
      "üè∑Ô∏è  Tag: team = data-science\n",
      "üè∑Ô∏è  Tag: priority = learning\n",
      "\n",
      "‚úÖ Experiment setup complete!\n",
      "üí° All runs in this notebook will be tracked under experiment: 'iris-classification-tutorial'\n"
     ]
    }
   ],
   "source": [
    "# Define experiment name with descriptive naming\n",
    "experiment_name = \"iris-classification-tutorial\"\n",
    "\n",
    "# Create or get the experiment\n",
    "# mlflow.set_experiment() either:\n",
    "# 1. Creates a new experiment if it doesn't exist\n",
    "# 2. Sets the active experiment if it already exists\n",
    "experiment = mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"üß™ Experiment: '{experiment_name}'\")\n",
    "print(f\"üìç Experiment ID: {experiment.experiment_id}\")\n",
    "print(f\"üìÇ Artifact Location: {experiment.artifact_location}\")\n",
    "\n",
    "# You can also create experiments with additional metadata\n",
    "# This is useful for providing context and description\n",
    "experiment_description = \"\"\"\n",
    "This experiment focuses on comparing different machine learning models \n",
    "for Iris flower classification. We'll track various algorithms, \n",
    "hyperparameters, and performance metrics to find the best approach.\n",
    "\n",
    "Dataset: Iris flower dataset (150 samples, 4 features, 3 classes)\n",
    "Goal: Achieve highest accuracy while maintaining good generalization\n",
    "\"\"\"\n",
    "\n",
    "# Add tags to categorize and organize experiments\n",
    "experiment_tags = {\n",
    "    \"project\": \"ml-tutorial\",\n",
    "    \"dataset\": \"iris\",\n",
    "    \"task\": \"classification\",\n",
    "    \"team\": \"data-science\",\n",
    "    \"priority\": \"learning\"\n",
    "}\n",
    "\n",
    "# Set experiment tags (metadata for organization)\n",
    "for key, value in experiment_tags.items():\n",
    "    mlflow.set_experiment_tag(key, value)\n",
    "    print(f\"üè∑Ô∏è  Tag: {key} = {value}\")\n",
    "\n",
    "print(\"\\n‚úÖ Experiment setup complete!\")\n",
    "print(f\"üí° All runs in this notebook will be tracked under experiment: '{experiment_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f190eb82",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Data\n",
    "\n",
    "Before we start experimenting with models, let's load and explore our dataset. We'll use the classic Iris dataset for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bc0c2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading Iris dataset...\n",
      "üìà Dataset shape: (150, 4)\n",
      "üéØ Target classes: ['setosa' 'versicolor' 'virginica']\n",
      "üìã Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "\n",
      "üìã First 5 rows of the dataset:\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "  species  \n",
      "0  setosa  \n",
      "1  setosa  \n",
      "2  setosa  \n",
      "3  setosa  \n",
      "4  setosa  \n",
      "\n",
      "üìä Dataset statistics:\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "  species  \n",
      "0  setosa  \n",
      "1  setosa  \n",
      "2  setosa  \n",
      "3  setosa  \n",
      "4  setosa  \n",
      "\n",
      "üìä Dataset statistics:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
      "count         150.000000        150.000000         150.000000   \n",
      "mean            5.843333          3.057333           3.758000   \n",
      "std             0.828066          0.435866           1.765298   \n",
      "min             4.300000          2.000000           1.000000   \n",
      "25%             5.100000          2.800000           1.600000   \n",
      "50%             5.800000          3.000000           4.350000   \n",
      "75%             6.400000          3.300000           5.100000   \n",
      "max             7.900000          4.400000           6.900000   \n",
      "\n",
      "       petal width (cm)  \n",
      "count        150.000000  \n",
      "mean           1.199333  \n",
      "std            0.762238  \n",
      "min            0.100000  \n",
      "25%            0.300000  \n",
      "50%            1.300000  \n",
      "75%            1.800000  \n",
      "max            2.500000  \n",
      "\n",
      "üéØ Class distribution:\n",
      "species\n",
      "setosa        50\n",
      "versicolor    50\n",
      "virginica     50\n",
      "Name: count, dtype: int64\n",
      "\n",
      "‚úÇÔ∏è  Data split complete:\n",
      "   Training samples: 120\n",
      "   Testing samples: 30\n",
      "   Features: 4\n",
      "\n",
      "‚úÖ Data preparation complete!\n",
      "   Ready to start model training and MLflow tracking!\n"
     ]
    }
   ],
   "source": [
    "# Load the Iris dataset from scikit-learn\n",
    "# This is a classic dataset with 150 samples, 4 features, and 3 classes\n",
    "print(\"üìä Loading Iris dataset...\")\n",
    "iris_data = datasets.load_iris()\n",
    "\n",
    "# Extract features (X) and target labels (y)\n",
    "X = iris_data.data          # Features: sepal length, sepal width, petal length, petal width\n",
    "y = iris_data.target        # Target: iris species (0: setosa, 1: versicolor, 2: virginica)\n",
    "\n",
    "# Get feature names and target names for better understanding\n",
    "feature_names = iris_data.feature_names\n",
    "target_names = iris_data.target_names\n",
    "\n",
    "print(f\"üìà Dataset shape: {X.shape}\")\n",
    "print(f\"üéØ Target classes: {target_names}\")\n",
    "print(f\"üìã Features: {feature_names}\")\n",
    "\n",
    "# Create a pandas DataFrame for better data exploration\n",
    "# This makes it easier to visualize and understand our data\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['species'] = pd.Categorical.from_codes(y, target_names)\n",
    "\n",
    "print(\"\\nüìã First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nüìä Dataset statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nüéØ Class distribution:\")\n",
    "print(df['species'].value_counts())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# This is crucial for evaluating model performance on unseen data\n",
    "test_size = 0.2         # Use 20% for testing, 80% for training\n",
    "random_state = 42       # Set random seed for reproducibility\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=test_size, \n",
    "    random_state=random_state,\n",
    "    stratify=y  # Ensure balanced split across all classes\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÇÔ∏è  Data split complete:\")\n",
    "print(f\"   Training samples: {X_train.shape[0]}\")\n",
    "print(f\"   Testing samples: {X_test.shape[0]}\")\n",
    "print(f\"   Features: {X_train.shape[1]}\")\n",
    "\n",
    "# Log data information that we'll use in our experiments\n",
    "data_info = {\n",
    "    \"dataset_name\": \"iris\",\n",
    "    \"total_samples\": len(X),\n",
    "    \"n_features\": X.shape[1],\n",
    "    \"n_classes\": len(target_names),\n",
    "    \"train_samples\": len(X_train),\n",
    "    \"test_samples\": len(X_test),\n",
    "    \"test_size\": test_size,\n",
    "    \"random_state\": random_state\n",
    "}\n",
    "\n",
    "print(\"\\n‚úÖ Data preparation complete!\")\n",
    "print(\"   Ready to start model training and MLflow tracking!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a266e",
   "metadata": {},
   "source": [
    "## 5. Log Parameters and Hyperparameters\n",
    "\n",
    "**Parameters** in MLflow are the configuration values that control your experiment. This includes:\n",
    "- **Hyperparameters**: Model-specific settings (learning rate, number of trees, etc.)\n",
    "- **Data parameters**: Dataset splits, preprocessing settings\n",
    "- **Environment parameters**: Library versions, hardware specs\n",
    "\n",
    "### Why Log Parameters?\n",
    "- **Reproducibility**: Recreate exact same results\n",
    "- **Comparison**: Compare different configurations\n",
    "- **Optimization**: Track what works best\n",
    "- **Documentation**: Maintain experiment history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55a989db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting MLflow run for parameter logging demonstration...\n",
      "üìù Run ID: 96ad5c7c035f47e9bcc70025e20b1fea\n",
      "\n",
      "üìã Logging individual parameters...\n",
      "üìù Run ID: 96ad5c7c035f47e9bcc70025e20b1fea\n",
      "\n",
      "üìã Logging individual parameters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Logging multiple parameters at once...\n",
      "‚úÖ All parameters logged successfully!\n",
      "\n",
      "üîç View this run in MLflow UI:\n",
      "   Run ID: 96ad5c7c035f47e9bcc70025e20b1fea\n",
      "   Experiment: iris-classification-tutorial\n",
      "\n",
      "üí° Parameter Logging Best Practices:\n",
      "   ‚úì Use descriptive parameter names\n",
      "   ‚úì Log all hyperparameters that affect model behavior\n",
      "   ‚úì Include data processing parameters\n",
      "   ‚úì Track environment information for reproducibility\n",
      "   ‚úì Use consistent naming conventions\n",
      "\n",
      "üéØ Parameter logging demonstration complete!\n",
      "These parameters are now stored in MLflow and can be:\n",
      "   - Viewed in the MLflow UI\n",
      "   - Queried programmatically\n",
      "   - Used for experiment comparison\n",
      "   - Referenced for model reproduction\n"
     ]
    }
   ],
   "source": [
    "# Let's start our first MLflow run to demonstrate parameter logging\n",
    "# A \"run\" is a single execution of your ML code (one training session)\n",
    "\n",
    "print(\"üöÄ Starting MLflow run for parameter logging demonstration...\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"parameter-logging-demo\") as run:\n",
    "    print(f\"üìù Run ID: {run.info.run_id}\")\n",
    "    \n",
    "    # Method 1: Log individual parameters using mlflow.log_param()\n",
    "    # This is useful for single values or when you want to log parameters one by one\n",
    "    print(\"\\nüìã Logging individual parameters...\")\n",
    "    \n",
    "    mlflow.log_param(\"model_type\", \"logistic_regression\")  # String parameter\n",
    "    mlflow.log_param(\"test_size\", 0.2)                     # Float parameter  \n",
    "    mlflow.log_param(\"random_state\", 42)                   # Integer parameter\n",
    "    mlflow.log_param(\"cross_validation\", True)             # Boolean parameter\n",
    "    \n",
    "    # Method 2: Log multiple parameters at once using mlflow.log_params()\n",
    "    # This is more efficient when you have many parameters\n",
    "    print(\"üìã Logging multiple parameters at once...\")\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    model_params = {\n",
    "        \"solver\": \"lbfgs\",           # Algorithm to use in optimization\n",
    "        \"max_iter\": 1000,            # Maximum number of iterations\n",
    "        \"multi_class\": \"auto\",       # How to handle multi-class classification\n",
    "        \"penalty\": \"l2\",             # Regularization penalty\n",
    "        \"C\": 1.0,                    # Inverse of regularization strength\n",
    "        \"fit_intercept\": True,       # Whether to calculate intercept\n",
    "        \"class_weight\": None         # Weights for balancing classes\n",
    "    }\n",
    "    mlflow.log_params(model_params)\n",
    "    \n",
    "    # Data processing parameters\n",
    "    data_params = {\n",
    "        \"dataset_version\": \"1.0\",\n",
    "        \"feature_scaling\": \"none\",\n",
    "        \"feature_selection\": \"all\",\n",
    "        \"data_split_strategy\": \"stratified\"\n",
    "    }\n",
    "    mlflow.log_params(data_params)\n",
    "    \n",
    "    # Environment and system parameters\n",
    "    system_params = {\n",
    "        \"sklearn_version\": \"1.3.0\",\n",
    "        \"python_version\": \"3.9\",\n",
    "        \"platform\": \"macOS\",\n",
    "        \"cpu_count\": os.cpu_count()\n",
    "    }\n",
    "    mlflow.log_params(system_params)\n",
    "    \n",
    "    print(\"‚úÖ All parameters logged successfully!\")\n",
    "    print(f\"\\nüîç View this run in MLflow UI:\")\n",
    "    print(f\"   Run ID: {run.info.run_id}\")\n",
    "    print(f\"   Experiment: {experiment_name}\")\n",
    "    \n",
    "    # Log the data information we prepared earlier\n",
    "    mlflow.log_params(data_info)\n",
    "    \n",
    "    print(\"\\nüí° Parameter Logging Best Practices:\")\n",
    "    print(\"   ‚úì Use descriptive parameter names\")\n",
    "    print(\"   ‚úì Log all hyperparameters that affect model behavior\")\n",
    "    print(\"   ‚úì Include data processing parameters\")\n",
    "    print(\"   ‚úì Track environment information for reproducibility\")\n",
    "    print(\"   ‚úì Use consistent naming conventions\")\n",
    "    \n",
    "print(\"\\nüéØ Parameter logging demonstration complete!\")\n",
    "print(\"These parameters are now stored in MLflow and can be:\")\n",
    "print(\"   - Viewed in the MLflow UI\")\n",
    "print(\"   - Queried programmatically\") \n",
    "print(\"   - Used for experiment comparison\")\n",
    "print(\"   - Referenced for model reproduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe126bc",
   "metadata": {},
   "source": [
    "## 6. Train a Simple Model with MLflow Tracking\n",
    "\n",
    "Now let's train our first machine learning model while tracking everything with MLflow. We'll train a Logistic Regression model and log every step of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4422782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Starting model training with complete MLflow tracking...\n",
      "üÜî Run ID: 1b22db2a24a34e73880e57ebc0da8a85\n",
      "üìÖ Start Time: 1756448173293\n",
      "üìù Logging model hyperparameters...\n",
      "üÜî Run ID: 1b22db2a24a34e73880e57ebc0da8a85\n",
      "üìÖ Start Time: 1756448173293\n",
      "üìù Logging model hyperparameters...\n",
      "üèãÔ∏è Training Logistic Regression model...\n",
      "‚úÖ Model training complete!\n",
      "üîÆ Making predictions...\n",
      "üìä Training predictions shape: (120,)\n",
      "üìä Test predictions shape: (30,)\n",
      "üìà Calculating performance metrics...\n",
      "\\nüìä Training Performance:\n",
      "   Accuracy:  0.9750\n",
      "   Precision: 0.9752\n",
      "   Recall:    0.9750\n",
      "   F1-Score:  0.9750\n",
      "\\nüéØ Test Performance:\n",
      "   Accuracy:  0.9667\n",
      "   Precision: 0.9697\n",
      "   Recall:    0.9667\n",
      "   F1-Score:  0.9666\n",
      "\\n‚öñÔ∏è  Overfitting Check:\n",
      "   Accuracy difference: 0.0083\n",
      "   ‚úÖ Model generalizes well!\n",
      "\\nüíæ Logging metrics to MLflow...\n",
      "üèãÔ∏è Training Logistic Regression model...\n",
      "‚úÖ Model training complete!\n",
      "üîÆ Making predictions...\n",
      "üìä Training predictions shape: (120,)\n",
      "üìä Test predictions shape: (30,)\n",
      "üìà Calculating performance metrics...\n",
      "\\nüìä Training Performance:\n",
      "   Accuracy:  0.9750\n",
      "   Precision: 0.9752\n",
      "   Recall:    0.9750\n",
      "   F1-Score:  0.9750\n",
      "\\nüéØ Test Performance:\n",
      "   Accuracy:  0.9667\n",
      "   Precision: 0.9697\n",
      "   Recall:    0.9667\n",
      "   F1-Score:  0.9666\n",
      "\\n‚öñÔ∏è  Overfitting Check:\n",
      "   Accuracy difference: 0.0083\n",
      "   ‚úÖ Model generalizes well!\n",
      "\\nüíæ Logging metrics to MLflow...\n",
      "üè∑Ô∏è  Adding tags for organization...\n",
      "\\n‚úÖ Model training and tracking complete!\n",
      "üîç View results in MLflow UI - Run ID: 1b22db2a24a34e73880e57ebc0da8a85\n",
      "\\nüéâ Your first tracked ML experiment is done!\n",
      "This run now contains:\n",
      "   ‚úì All hyperparameters\n",
      "   ‚úì Training and test metrics\n",
      "   ‚úì Run metadata and tags\n",
      "   ‚úì Timestamps and run information\n",
      "üè∑Ô∏è  Adding tags for organization...\n",
      "\\n‚úÖ Model training and tracking complete!\n",
      "üîç View results in MLflow UI - Run ID: 1b22db2a24a34e73880e57ebc0da8a85\n",
      "\\nüéâ Your first tracked ML experiment is done!\n",
      "This run now contains:\n",
      "   ‚úì All hyperparameters\n",
      "   ‚úì Training and test metrics\n",
      "   ‚úì Run metadata and tags\n",
      "   ‚úì Timestamps and run information\n"
     ]
    }
   ],
   "source": [
    "# Start a new MLflow run for model training\n",
    "print(\"üéØ Starting model training with complete MLflow tracking...\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"logistic-regression-baseline\") as run:\n",
    "    \n",
    "    print(f\"üÜî Run ID: {run.info.run_id}\")\n",
    "    print(f\"üìÖ Start Time: {run.info.start_time}\")\n",
    "    \n",
    "    # Step 1: Define model hyperparameters\n",
    "    # These parameters control how the model learns\n",
    "    model_params = {\n",
    "        \"solver\": \"lbfgs\",         # Optimization algorithm\n",
    "        \"max_iter\": 1000,          # Maximum iterations for convergence\n",
    "        \"multi_class\": \"auto\",     # Multi-class strategy\n",
    "        \"random_state\": 42,        # For reproducible results\n",
    "        \"C\": 1.0,                  # Regularization strength (inverse)\n",
    "        \"penalty\": \"l2\"            # Regularization type\n",
    "    }\n",
    "    \n",
    "    # Step 2: Log all hyperparameters to MLflow\n",
    "    print(\"üìù Logging model hyperparameters...\")\n",
    "    mlflow.log_params(model_params)\n",
    "    \n",
    "    # Also log data-related parameters\n",
    "    mlflow.log_params({\n",
    "        \"train_samples\": len(X_train),\n",
    "        \"test_samples\": len(X_test),\n",
    "        \"n_features\": X_train.shape[1],\n",
    "        \"n_classes\": len(np.unique(y_train))\n",
    "    })\n",
    "    \n",
    "    # Step 3: Create and train the model\n",
    "    print(\"üèãÔ∏è Training Logistic Regression model...\")\n",
    "    \n",
    "    # Initialize the model with our hyperparameters\n",
    "    model = LogisticRegression(**model_params)\n",
    "    \n",
    "    # Train the model on training data\n",
    "    # This is where the actual machine learning happens\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"‚úÖ Model training complete!\")\n",
    "    \n",
    "    # Step 4: Make predictions on both training and test sets\n",
    "    print(\"üîÆ Making predictions...\")\n",
    "    \n",
    "    # Predict on training data (to check for overfitting)\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_train_prob = model.predict_proba(X_train)  # Get probability scores\n",
    "    \n",
    "    # Predict on test data (for true performance evaluation)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_prob = model.predict_proba(X_test)\n",
    "    \n",
    "    print(f\"üìä Training predictions shape: {y_train_pred.shape}\")\n",
    "    print(f\"üìä Test predictions shape: {y_test_pred.shape}\")\n",
    "    \n",
    "    # Step 5: Calculate performance metrics\n",
    "    print(\"üìà Calculating performance metrics...\")\n",
    "    \n",
    "    # Training metrics (to detect overfitting)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    train_precision = precision_score(y_train, y_train_pred, average='weighted')\n",
    "    train_recall = recall_score(y_train, y_train_pred, average='weighted')\n",
    "    train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "    \n",
    "    # Test metrics (true performance indicators)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "    test_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "    test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "    \n",
    "    # Print results for immediate feedback\n",
    "    print(f\"\\\\nüìä Training Performance:\")\n",
    "    print(f\"   Accuracy:  {train_accuracy:.4f}\")\n",
    "    print(f\"   Precision: {train_precision:.4f}\")\n",
    "    print(f\"   Recall:    {train_recall:.4f}\")\n",
    "    print(f\"   F1-Score:  {train_f1:.4f}\")\n",
    "    \n",
    "    print(f\"\\\\nüéØ Test Performance:\")\n",
    "    print(f\"   Accuracy:  {test_accuracy:.4f}\")\n",
    "    print(f\"   Precision: {test_precision:.4f}\")\n",
    "    print(f\"   Recall:    {test_recall:.4f}\")\n",
    "    print(f\"   F1-Score:  {test_f1:.4f}\")\n",
    "    \n",
    "    # Calculate overfitting indicator\n",
    "    accuracy_diff = train_accuracy - test_accuracy\n",
    "    print(f\"\\\\n‚öñÔ∏è  Overfitting Check:\")\n",
    "    print(f\"   Accuracy difference: {accuracy_diff:.4f}\")\n",
    "    if accuracy_diff > 0.05:\n",
    "        print(\"   ‚ö†Ô∏è  Potential overfitting detected!\")\n",
    "    else:\n",
    "        print(\"   ‚úÖ Model generalizes well!\")\n",
    "    \n",
    "    # Step 6: Log all metrics to MLflow\n",
    "    print(\"\\\\nüíæ Logging metrics to MLflow...\")\n",
    "    \n",
    "    # Log training metrics\n",
    "    mlflow.log_metric(\"train_accuracy\", train_accuracy)\n",
    "    mlflow.log_metric(\"train_precision\", train_precision)\n",
    "    mlflow.log_metric(\"train_recall\", train_recall)\n",
    "    mlflow.log_metric(\"train_f1\", train_f1)\n",
    "    \n",
    "    # Log test metrics  \n",
    "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "    mlflow.log_metric(\"test_precision\", test_precision)\n",
    "    mlflow.log_metric(\"test_recall\", test_recall)\n",
    "    mlflow.log_metric(\"test_f1\", test_f1)\n",
    "    \n",
    "    # Log derived metrics\n",
    "    mlflow.log_metric(\"accuracy_diff\", accuracy_diff)\n",
    "    mlflow.log_metric(\"model_complexity\", len(model.coef_[0]))  # Number of features\n",
    "    \n",
    "    # Step 7: Add run tags for organization\n",
    "    print(\"üè∑Ô∏è  Adding tags for organization...\")\n",
    "    mlflow.set_tag(\"model_type\", \"logistic_regression\")\n",
    "    mlflow.set_tag(\"status\", \"completed\")\n",
    "    mlflow.set_tag(\"dataset\", \"iris\")\n",
    "    mlflow.set_tag(\"purpose\", \"baseline_model\")\n",
    "    \n",
    "    print(f\"\\\\n‚úÖ Model training and tracking complete!\")\n",
    "    print(f\"üîç View results in MLflow UI - Run ID: {run.info.run_id}\")\n",
    "\n",
    "print(\"\\\\nüéâ Your first tracked ML experiment is done!\")\n",
    "print(\"This run now contains:\")\n",
    "print(\"   ‚úì All hyperparameters\")\n",
    "print(\"   ‚úì Training and test metrics\") \n",
    "print(\"   ‚úì Run metadata and tags\")\n",
    "print(\"   ‚úì Timestamps and run information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354de9c6",
   "metadata": {},
   "source": [
    "## 7. Log Metrics During Training\n",
    "\n",
    "**Metrics** in MLflow track the performance and behavior of your models over time. Unlike parameters (which are input configurations), metrics are output measurements that change during or after training.\n",
    "\n",
    "### Types of Metrics to Track:\n",
    "- **Performance metrics**: Accuracy, precision, recall, F1-score\n",
    "- **Training metrics**: Loss, convergence rate, training time\n",
    "- **Resource metrics**: Memory usage, CPU time, GPU utilization\n",
    "- **Business metrics**: Model interpretability scores, fairness metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "680600bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Advanced Metrics Logging Demonstration...\n",
      "üÜî Run ID: 627754b683394b8c831bf35540128002\n",
      "\\nüìà Logging individual metrics...\n",
      "   ‚úì Accuracy: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úì Precision (macro): 0.9697\n",
      "   ‚úì Precision (micro): 0.9667\n",
      "   ‚úì Precision (weighted): 0.9697\n",
      "\\nüìä Logging metrics with steps (simulating iterative training)...\n",
      "   Step 0: Training accuracy = 0.7733\n",
      "   Step 1: Training accuracy = 0.8120\n",
      "   Step 2: Training accuracy = 0.8507\n",
      "   Step 3: Training accuracy = 0.8893\n",
      "   Step 4: Training accuracy = 0.9280\n",
      "   Step 5: Training accuracy = 0.9667\n",
      "\\n‚ö° Logging system performance metrics...\n",
      "   ‚úì Training time: 0.0393 seconds\n",
      "   ‚úì Model parameters: 15\n",
      "   ‚úì Features used: 4\n",
      "   ‚úì Model size: 32 bytes\n",
      "\\nüíº Logging business-relevant metrics...\n",
      "   ‚úì Model parameters: 15\n",
      "   ‚úì Features used: 4\n",
      "   ‚úì Model size: 32 bytes\n",
      "\\nüíº Logging business-relevant metrics...\n",
      "   ‚úì Average prediction confidence: 0.8681\n",
      "   ‚úì Predicted class setosa ratio: 0.3333\n",
      "   ‚úì Predicted class versicolor ratio: 0.3000\n",
      "   ‚úì Predicted class virginica ratio: 0.3667\n",
      "\\nüîç Logging derived metrics...\n",
      "   ‚úì setosa - Precision: 1.0000, Recall: 1.0000\n",
      "   ‚úì Average prediction confidence: 0.8681\n",
      "   ‚úì Predicted class setosa ratio: 0.3333\n",
      "   ‚úì Predicted class versicolor ratio: 0.3000\n",
      "   ‚úì Predicted class virginica ratio: 0.3667\n",
      "\\nüîç Logging derived metrics...\n",
      "   ‚úì setosa - Precision: 1.0000, Recall: 1.0000\n",
      "   ‚úì versicolor - Precision: 1.0000, Recall: 0.9000\n",
      "   ‚úì virginica - Precision: 0.9091, Recall: 1.0000\n",
      "   üìâ Worst performing class: virginica (0.9091)\n",
      "   üìà Best performing class: setosa (1.0000)\n",
      "\\n‚úÖ Advanced metrics logging complete!\n",
      "üîç View all metrics in MLflow UI - Run ID: 627754b683394b8c831bf35540128002\n",
      "\\nüí° Metrics Logging Best Practices:\n",
      "   ‚úì Log metrics consistently across all experiments\n",
      "   ‚úì Use descriptive metric names\n",
      "   ‚úì Track both training and validation metrics\n",
      "   ‚úì Log system performance metrics for optimization\n",
      "   ‚úì Include business-relevant metrics\n",
      "   ‚úì Use step parameter for iterative training tracking\n",
      "   ‚úì Log derived metrics for deeper insights\n",
      "   ‚úì versicolor - Precision: 1.0000, Recall: 0.9000\n",
      "   ‚úì virginica - Precision: 0.9091, Recall: 1.0000\n",
      "   üìâ Worst performing class: virginica (0.9091)\n",
      "   üìà Best performing class: setosa (1.0000)\n",
      "\\n‚úÖ Advanced metrics logging complete!\n",
      "üîç View all metrics in MLflow UI - Run ID: 627754b683394b8c831bf35540128002\n",
      "\\nüí° Metrics Logging Best Practices:\n",
      "   ‚úì Log metrics consistently across all experiments\n",
      "   ‚úì Use descriptive metric names\n",
      "   ‚úì Track both training and validation metrics\n",
      "   ‚úì Log system performance metrics for optimization\n",
      "   ‚úì Include business-relevant metrics\n",
      "   ‚úì Use step parameter for iterative training tracking\n",
      "   ‚úì Log derived metrics for deeper insights\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate advanced metric logging techniques\n",
    "print(\"üìä Advanced Metrics Logging Demonstration...\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"advanced-metrics-demo\") as run:\n",
    "    \n",
    "    print(f\"üÜî Run ID: {run.info.run_id}\")\n",
    "    \n",
    "    # Train a model quickly for demonstration\n",
    "    model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    \n",
    "    # Time the training process\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)\n",
    "    \n",
    "    # Method 1: Log individual metrics with mlflow.log_metric()\n",
    "    print(\"\\\\nüìà Logging individual metrics...\")\n",
    "    \n",
    "    # Basic performance metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    print(f\"   ‚úì Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Detailed per-class metrics\n",
    "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "    precision_micro = precision_score(y_test, y_pred, average='micro')\n",
    "    precision_weighted = precision_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    mlflow.log_metric(\"precision_macro\", precision_macro)\n",
    "    mlflow.log_metric(\"precision_micro\", precision_micro) \n",
    "    mlflow.log_metric(\"precision_weighted\", precision_weighted)\n",
    "    \n",
    "    print(f\"   ‚úì Precision (macro): {precision_macro:.4f}\")\n",
    "    print(f\"   ‚úì Precision (micro): {precision_micro:.4f}\")\n",
    "    print(f\"   ‚úì Precision (weighted): {precision_weighted:.4f}\")\n",
    "    \n",
    "    # Method 2: Log metrics with step parameter (useful for iterative training)\n",
    "    print(\"\\\\nüìä Logging metrics with steps (simulating iterative training)...\")\n",
    "    \n",
    "    # Simulate logging metrics at different training epochs/steps\n",
    "    # This is useful for tracking training progress over time\n",
    "    for step in range(0, 6):\n",
    "        # Simulate improving accuracy over training steps\n",
    "        simulated_accuracy = accuracy * (0.8 + 0.04 * step)  # Gradually improve\n",
    "        mlflow.log_metric(\"training_accuracy\", simulated_accuracy, step=step)\n",
    "        print(f\"   Step {step}: Training accuracy = {simulated_accuracy:.4f}\")\n",
    "    \n",
    "    # Method 3: Log system and performance metrics\n",
    "    print(\"\\\\n‚ö° Logging system performance metrics...\")\n",
    "    \n",
    "    # Training time\n",
    "    mlflow.log_metric(\"training_time_seconds\", training_time)\n",
    "    print(f\"   ‚úì Training time: {training_time:.4f} seconds\")\n",
    "    \n",
    "    # Model complexity metrics\n",
    "    n_features = X_train.shape[1]\n",
    "    n_params = len(model.coef_.flatten()) + len(model.intercept_)\n",
    "    mlflow.log_metric(\"model_parameters\", n_params)\n",
    "    mlflow.log_metric(\"feature_count\", n_features)\n",
    "    \n",
    "    print(f\"   ‚úì Model parameters: {n_params}\")\n",
    "    print(f\"   ‚úì Features used: {n_features}\")\n",
    "    \n",
    "    # Memory usage (approximate)\n",
    "    model_size_bytes = model.__sizeof__()\n",
    "    mlflow.log_metric(\"model_size_bytes\", model_size_bytes)\n",
    "    print(f\"   ‚úì Model size: {model_size_bytes} bytes\")\n",
    "    \n",
    "    # Method 4: Log business and interpretability metrics\n",
    "    print(\"\\\\nüíº Logging business-relevant metrics...\")\n",
    "    \n",
    "    # Confidence scores (average prediction probability)\n",
    "    avg_confidence = np.mean(np.max(y_prob, axis=1))\n",
    "    mlflow.log_metric(\"average_prediction_confidence\", avg_confidence)\n",
    "    print(f\"   ‚úì Average prediction confidence: {avg_confidence:.4f}\")\n",
    "    \n",
    "    # Class balance in predictions\n",
    "    pred_class_distribution = np.bincount(y_pred) / len(y_pred)\n",
    "    for i, ratio in enumerate(pred_class_distribution):\n",
    "        mlflow.log_metric(f\"predicted_class_{i}_ratio\", ratio)\n",
    "        print(f\"   ‚úì Predicted class {target_names[i]} ratio: {ratio:.4f}\")\n",
    "    \n",
    "    # Method 5: Log derived and comparative metrics\n",
    "    print(\"\\\\nüîç Logging derived metrics...\")\n",
    "    \n",
    "    # Calculate per-class metrics\n",
    "    per_class_precision = precision_score(y_test, y_pred, average=None)\n",
    "    per_class_recall = recall_score(y_test, y_pred, average=None)\n",
    "    \n",
    "    for i, (prec, rec) in enumerate(zip(per_class_precision, per_class_recall)):\n",
    "        mlflow.log_metric(f\"precision_class_{target_names[i]}\", prec)\n",
    "        mlflow.log_metric(f\"recall_class_{target_names[i]}\", rec)\n",
    "        print(f\"   ‚úì {target_names[i]} - Precision: {prec:.4f}, Recall: {rec:.4f}\")\n",
    "    \n",
    "    # Worst and best performing class\n",
    "    worst_class_idx = np.argmin(per_class_precision)\n",
    "    best_class_idx = np.argmax(per_class_precision)\n",
    "    \n",
    "    mlflow.log_metric(\"worst_class_precision\", per_class_precision[worst_class_idx])\n",
    "    mlflow.log_metric(\"best_class_precision\", per_class_precision[best_class_idx])\n",
    "    \n",
    "    print(f\"   üìâ Worst performing class: {target_names[worst_class_idx]} ({per_class_precision[worst_class_idx]:.4f})\")\n",
    "    print(f\"   üìà Best performing class: {target_names[best_class_idx]} ({per_class_precision[best_class_idx]:.4f})\")\n",
    "    \n",
    "    # Add descriptive tags\n",
    "    mlflow.set_tag(\"metrics_logged\", \"comprehensive\")\n",
    "    mlflow.set_tag(\"analysis_depth\", \"detailed\")\n",
    "    \n",
    "    print(f\"\\\\n‚úÖ Advanced metrics logging complete!\")\n",
    "    print(f\"üîç View all metrics in MLflow UI - Run ID: {run.info.run_id}\")\n",
    "\n",
    "print(\"\\\\nüí° Metrics Logging Best Practices:\")\n",
    "print(\"   ‚úì Log metrics consistently across all experiments\")\n",
    "print(\"   ‚úì Use descriptive metric names\")\n",
    "print(\"   ‚úì Track both training and validation metrics\")\n",
    "print(\"   ‚úì Log system performance metrics for optimization\")\n",
    "print(\"   ‚úì Include business-relevant metrics\")\n",
    "print(\"   ‚úì Use step parameter for iterative training tracking\")\n",
    "print(\"   ‚úì Log derived metrics for deeper insights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b5b3f7",
   "metadata": {},
   "source": [
    "## 8. Log Model Artifacts\n",
    "\n",
    "**Artifacts** in MLflow are files associated with your experiments. This includes:\n",
    "- **Models**: Trained model files (pickle, joblib, etc.)\n",
    "- **Plots**: Visualizations and charts\n",
    "- **Data files**: Processed datasets, feature importance files\n",
    "- **Reports**: Model evaluation reports, documentation\n",
    "- **Code**: Scripts, notebooks, configuration files\n",
    "\n",
    "Artifacts enable full reproducibility and comprehensive experiment documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "330f8fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Comprehensive Artifact Logging Demonstration...\n",
      "üÜî Run ID: 627754b683394b8c831bf35540128002\n",
      "\n",
      "üìä Creating and logging visualization artifacts...\n",
      "   ‚úì Saved confusion matrix: confusion_matrix.png\n",
      "   ‚úì Saved confusion matrix: confusion_matrix.png\n",
      "   ‚úì Saved feature importance: feature_importance.png\n",
      "   ‚úì Saved feature importance: feature_importance.png\n",
      "   ‚úì Saved prediction analysis: prediction_analysis.png\n",
      "\n",
      "üíæ Creating and logging data artifacts...\n",
      "   ‚úì Saved classification report: classification_report.csv\n",
      "   ‚úì Saved detailed predictions: detailed_predictions.csv\n",
      "   ‚úì Saved model summary: model_summary.json\n",
      "\n",
      "üìà Creating performance tracking data...\n",
      "   ‚úì Saved training history: training_history.csv\n",
      "\n",
      "‚úÖ Artifact logging complete!\n",
      "üìÅ All files saved locally and logged to MLflow run: 627754b683394b8c831bf35540128002\n",
      "üîó Artifacts will be available in the MLflow UI under this run\n",
      "üßπ Local artifact files cleaned up!\n",
      "   ‚úì Saved prediction analysis: prediction_analysis.png\n",
      "\n",
      "üíæ Creating and logging data artifacts...\n",
      "   ‚úì Saved classification report: classification_report.csv\n",
      "   ‚úì Saved detailed predictions: detailed_predictions.csv\n",
      "   ‚úì Saved model summary: model_summary.json\n",
      "\n",
      "üìà Creating performance tracking data...\n",
      "   ‚úì Saved training history: training_history.csv\n",
      "\n",
      "‚úÖ Artifact logging complete!\n",
      "üìÅ All files saved locally and logged to MLflow run: 627754b683394b8c831bf35540128002\n",
      "üîó Artifacts will be available in the MLflow UI under this run\n",
      "üßπ Local artifact files cleaned up!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üì¶ Comprehensive Artifact Logging Demonstration\n",
    "print(\"üì¶ Comprehensive Artifact Logging Demonstration...\")\n",
    "print(f\"üÜî Run ID: {run.info.run_id}\")\n",
    "\n",
    "# Create visualizations and save as artifacts\n",
    "print(\"\\nüìä Creating and logging visualization artifacts...\")\n",
    "\n",
    "# 1. Confusion Matrix Heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=target_names, yticklabels=target_names)\n",
    "plt.title('Confusion Matrix - Test Set Performance')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "confusion_matrix_path = \"confusion_matrix.png\"\n",
    "plt.savefig(confusion_matrix_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   ‚úì Saved confusion matrix: {confusion_matrix_path}\")\n",
    "\n",
    "# 2. Feature Importance Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "# For logistic regression, use the absolute values of coefficients as importance\n",
    "feature_importance = np.abs(model.coef_[0])\n",
    "plt.barh(range(len(feature_names)), feature_importance)\n",
    "plt.yticks(range(len(feature_names)), feature_names)\n",
    "plt.xlabel('Feature Importance (|Coefficient|)')\n",
    "plt.title('Feature Importance - Logistic Regression')\n",
    "plt.tight_layout()\n",
    "feature_importance_path = \"feature_importance.png\"\n",
    "plt.savefig(feature_importance_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   ‚úì Saved feature importance: {feature_importance_path}\")\n",
    "\n",
    "# 3. Prediction Analysis Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Get prediction probabilities\n",
    "y_prob = model.predict_proba(X_test)\n",
    "max_probs = np.max(y_prob, axis=1)\n",
    "\n",
    "# Create subplots for detailed analysis\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Prediction confidence distribution\n",
    "ax1.hist(max_probs, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax1.set_xlabel('Maximum Prediction Probability')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Prediction Confidence Distribution')\n",
    "ax1.axvline(np.mean(max_probs), color='red', linestyle='--', \n",
    "           label=f'Mean: {np.mean(max_probs):.3f}')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Class-wise prediction accuracy\n",
    "from sklearn.metrics import classification_report\n",
    "class_report = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)\n",
    "classes = list(target_names)\n",
    "precisions = [class_report[cls]['precision'] for cls in classes]\n",
    "recalls = [class_report[cls]['recall'] for cls in classes]\n",
    "\n",
    "x = np.arange(len(classes))\n",
    "width = 0.35\n",
    "ax2.bar(x - width/2, precisions, width, label='Precision', alpha=0.8)\n",
    "ax2.bar(x + width/2, recalls, width, label='Recall', alpha=0.8)\n",
    "ax2.set_xlabel('Classes')\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_title('Per-Class Performance Metrics')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(classes)\n",
    "ax2.legend()\n",
    "ax2.set_ylim(0, 1.1)\n",
    "\n",
    "# Plot 3: Feature correlation with predictions\n",
    "ax3.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='viridis', alpha=0.6, s=50)\n",
    "ax3.set_xlabel(feature_names[0])\n",
    "ax3.set_ylabel(feature_names[1])\n",
    "ax3.set_title('True Labels - Feature Space')\n",
    "\n",
    "# Plot 4: Prediction errors analysis\n",
    "incorrect_mask = y_test != y_pred\n",
    "if np.any(incorrect_mask):\n",
    "    ax4.scatter(X_test[~incorrect_mask, 0], X_test[~incorrect_mask, 1], \n",
    "               c=y_test[~incorrect_mask], cmap='viridis', alpha=0.6, s=50, label='Correct')\n",
    "    ax4.scatter(X_test[incorrect_mask, 0], X_test[incorrect_mask, 1], \n",
    "               c='red', marker='x', s=100, label='Incorrect')\n",
    "    ax4.set_xlabel(feature_names[0])\n",
    "    ax4.set_ylabel(feature_names[1])\n",
    "    ax4.set_title('Prediction Errors Analysis')\n",
    "    ax4.legend()\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'No prediction errors!', ha='center', va='center', \n",
    "            transform=ax4.transAxes, fontsize=16, color='green')\n",
    "    ax4.set_title('Prediction Errors Analysis')\n",
    "\n",
    "plt.tight_layout()\n",
    "prediction_analysis_path = \"prediction_analysis.png\"\n",
    "plt.savefig(prediction_analysis_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"   ‚úì Saved prediction analysis: {prediction_analysis_path}\")\n",
    "\n",
    "# Log all visualization artifacts to MLflow\n",
    "mlflow.log_artifact(confusion_matrix_path)\n",
    "mlflow.log_artifact(feature_importance_path)\n",
    "mlflow.log_artifact(prediction_analysis_path)\n",
    "\n",
    "print(\"\\nüíæ Creating and logging data artifacts...\")\n",
    "\n",
    "# 4. Classification Report as CSV\n",
    "class_report_df = pd.DataFrame(classification_report(y_test, y_pred, \n",
    "                                                   target_names=target_names, \n",
    "                                                   output_dict=True)).T\n",
    "class_report_path = \"classification_report.csv\"\n",
    "class_report_df.to_csv(class_report_path)\n",
    "mlflow.log_artifact(class_report_path)\n",
    "print(f\"   ‚úì Saved classification report: {class_report_path}\")\n",
    "\n",
    "# 5. Detailed Predictions with Probabilities\n",
    "results_df = pd.DataFrame({\n",
    "    'true_label': y_test,\n",
    "    'predicted_label': y_pred,\n",
    "    'true_class_name': [target_names[i] for i in y_test],\n",
    "    'predicted_class_name': [target_names[i] for i in y_pred],\n",
    "    'is_correct': y_test == y_pred,\n",
    "    'max_probability': np.max(y_prob, axis=1),\n",
    "    'confidence': np.max(y_prob, axis=1)\n",
    "})\n",
    "\n",
    "# Add individual class probabilities\n",
    "for i, class_name in enumerate(target_names):\n",
    "    results_df[f'prob_{class_name}'] = y_prob[:, i]\n",
    "\n",
    "predictions_path = \"detailed_predictions.csv\"\n",
    "results_df.to_csv(predictions_path, index=False)\n",
    "mlflow.log_artifact(predictions_path)\n",
    "print(f\"   ‚úì Saved detailed predictions: {predictions_path}\")\n",
    "\n",
    "# Create model summary report\n",
    "model_summary = {\n",
    "    \"model_type\": \"LogisticRegression\",\n",
    "    \"training_samples\": len(X_train),\n",
    "    \"test_samples\": len(X_test),\n",
    "    \"features\": feature_names,  # Remove .tolist() since it's already a list\n",
    "    \"classes\": target_names.tolist(),\n",
    "    \"hyperparameters\": model_params,\n",
    "    \"performance\": {\n",
    "        \"accuracy\": float(accuracy_score(y_test, y_pred)),\n",
    "        \"precision\": float(precision_score(y_test, y_pred, average='weighted')),\n",
    "        \"recall\": float(recall_score(y_test, y_pred, average='weighted')),\n",
    "        \"f1\": float(f1_score(y_test, y_pred, average='weighted'))\n",
    "    },\n",
    "    \"feature_coefficients\": model.coef_.tolist(),\n",
    "    \"intercept\": model.intercept_.tolist()\n",
    "}\n",
    "\n",
    "# Save as JSON\n",
    "import json\n",
    "model_summary_path = \"model_summary.json\"\n",
    "with open(model_summary_path, 'w') as f:\n",
    "    json.dump(model_summary, f, indent=2)\n",
    "\n",
    "mlflow.log_artifact(model_summary_path)\n",
    "print(f\"   ‚úì Saved model summary: {model_summary_path}\")\n",
    "\n",
    "# 6. Model Performance Over Time (simulated)\n",
    "print(\"\\nüìà Creating performance tracking data...\")\n",
    "performance_history = []\n",
    "for step in range(1, 11):\n",
    "    # Simulate training progress (this would come from actual training loop)\n",
    "    simulated_accuracy = 0.4 + (step/10) * 0.5 + np.random.normal(0, 0.02)\n",
    "    performance_history.append({\n",
    "        'step': step,\n",
    "        'accuracy': min(max(simulated_accuracy, 0), 1),  # Clamp between 0 and 1\n",
    "        'loss': max(0.1, 2.0 - step * 0.15 + np.random.normal(0, 0.05))\n",
    "    })\n",
    "\n",
    "performance_df = pd.DataFrame(performance_history)\n",
    "performance_path = \"training_history.csv\"\n",
    "performance_df.to_csv(performance_path, index=False)\n",
    "mlflow.log_artifact(performance_path)\n",
    "print(f\"   ‚úì Saved training history: {performance_path}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Artifact logging complete!\")\n",
    "print(f\"üìÅ All files saved locally and logged to MLflow run: {run.info.run_id}\")\n",
    "print(f\"üîó Artifacts will be available in the MLflow UI under this run\")\n",
    "\n",
    "# Clean up local files (optional)\n",
    "import os\n",
    "for file_path in [confusion_matrix_path, feature_importance_path, prediction_analysis_path,\n",
    "                 class_report_path, predictions_path, model_summary_path, performance_path]:\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        \n",
    "print(\"üßπ Local artifact files cleaned up!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc8d272",
   "metadata": {},
   "source": [
    "## 9. Register Model in MLflow Model Registry\n",
    "\n",
    "The **MLflow Model Registry** is a centralized model store that provides:\n",
    "- **Model Versioning**: Track different versions of the same model\n",
    "- **Stage Management**: Move models through Staging ‚Üí Production\n",
    "- **Model Lineage**: Connect models to their training runs\n",
    "- **Collaboration**: Share models across teams\n",
    "- **Governance**: Control model deployments and approvals\n",
    "\n",
    "### Model Lifecycle Stages:\n",
    "- **None**: Newly registered model\n",
    "- **Staging**: Model being tested/validated\n",
    "- **Production**: Model deployed for real use\n",
    "- **Archived**: Retired model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e75672a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ended previous MLflow run\n",
      "üîÑ Ready to start model registry demonstration...\n"
     ]
    }
   ],
   "source": [
    "# End any active MLflow runs before starting model registry demo\n",
    "try:\n",
    "    mlflow.end_run()\n",
    "    print(\"‚úÖ Ended previous MLflow run\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ÑπÔ∏è  No active run to end: {e}\")\n",
    "\n",
    "print(\"üîÑ Ready to start model registry demonstration...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f28796c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè™ MLflow Model Registry Demonstration...\n",
      "üÜî Run ID: 1e6590f9b61749c782be833208a47b83\n",
      "\\nüèãÔ∏è Training production-ready model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìä Model Performance:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/29 16:16:23 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Accuracy: 0.9667\n",
      "      F1-Score: 0.9666\n",
      "\\nüìù Registering model in MLflow Model Registry...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'iris-classifier-production' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'iris-classifier-production'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Model registered as: iris-classifier-production\n",
      "   üìç Model URI: models:/m-f08c9eba181142e8b14799d644081f52\n",
      "\\nüîÑ Working with Model Registry...\n",
      "\\nüìã Registered Model: iris-classifier-production\n",
      "   Description: No description\n",
      "   Creation Time: 1756448150627\n",
      "\\nüìö Model Versions (2 total):\n",
      "   Version 2:\n",
      "      Stage: None\n",
      "      Status: READY\n",
      "      Run ID: 1e6590f9b61749c782be833208a47b83\n",
      "      Creation Time: 1756448184335\n",
      "   Version 1:\n",
      "      Stage: None\n",
      "      Status: READY\n",
      "      Run ID: 9c7f1df7423c4d63bc808120e437653e\n",
      "      Creation Time: 1756448150640\n",
      "\\nüé≠ Managing Model Stages...\n",
      "\\nüéØ Working with Version 2...\n",
      "   üì§ Transitioning to Staging...\n",
      "‚ö†Ô∏è  Model registry operation failed: ('cannot represent an object', <Metric: dataset_digest=None, dataset_name=None, key='accuracy', model_id='m-f08c9eba181142e8b14799d644081f52', run_id='1e6590f9b61749c782be833208a47b83', step=0, timestamp=1756448183564, value=0.9666666666666667>)\n",
      "   This might happen if using file-based tracking without a server\n",
      "   Model registry works best with MLflow tracking server\n",
      "\\n‚úÖ Model Registry demonstration complete!\n",
      "\\nüí° Model Registry Best Practices:\n",
      "   ‚úì Use descriptive model names and versions\n",
      "   ‚úì Add detailed descriptions and tags\n",
      "   ‚úì Follow proper stage transitions (None ‚Üí Staging ‚Üí Production)\n",
      "   ‚úì Archive old versions when promoting new ones\n",
      "   ‚úì Include performance metrics in model descriptions\n",
      "   ‚úì Use tags for categorization and searchability\n",
      "   ‚úì Implement proper validation before production promotion\n",
      "   ‚úì Consider using MLflow tracking server for team collaboration\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate MLflow Model Registry functionality\n",
    "print(\"üè™ MLflow Model Registry Demonstration...\")\n",
    "\n",
    "# Define the registered model name\n",
    "registered_model_name = \"iris-classifier-production\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"model-for-registry\") as run:\n",
    "    \n",
    "    print(f\"üÜî Run ID: {run.info.run_id}\")\n",
    "    \n",
    "    # Train a high-quality model for registration\n",
    "    print(\"\\\\nüèãÔ∏è Training production-ready model...\")\n",
    "    \n",
    "    # Use optimized hyperparameters\n",
    "    production_params = {\n",
    "        \"solver\": \"lbfgs\",\n",
    "        \"max_iter\": 2000,        # More iterations for better convergence\n",
    "        \"multi_class\": \"auto\",\n",
    "        \"random_state\": 42,\n",
    "        \"C\": 0.8,                # Slightly more regularization\n",
    "        \"penalty\": \"l2\"\n",
    "    }\n",
    "    \n",
    "    # Train the model\n",
    "    production_model = LogisticRegression(**production_params)\n",
    "    production_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate performance\n",
    "    y_pred = production_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Log everything to MLflow\n",
    "    mlflow.log_params(production_params)\n",
    "    mlflow.log_metrics({\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1\n",
    "    })\n",
    "    \n",
    "    print(f\"   üìä Model Performance:\")\n",
    "    print(f\"      Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"      F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Create model signature for proper schema documentation\n",
    "    signature = infer_signature(X_train, production_model.predict(X_train))\n",
    "    \n",
    "    # === STEP 1: LOG MODEL WITH REGISTRY NAME ===\n",
    "    print(\"\\\\nüìù Registering model in MLflow Model Registry...\")\n",
    "    \n",
    "    # Log model and register it simultaneously\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=production_model,\n",
    "        artifact_path=\"model\",                           # Path within run artifacts\n",
    "        signature=signature,                             # Input/output schema\n",
    "        input_example=X_train[:3],                      # Sample input for testing\n",
    "        registered_model_name=registered_model_name,    # Register with this name\n",
    "        pip_requirements=[\"scikit-learn>=1.0\", \"pandas\", \"numpy\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚úÖ Model registered as: {registered_model_name}\")\n",
    "    print(f\"   üìç Model URI: {model_info.model_uri}\")\n",
    "    \n",
    "    # Add tags for this run\n",
    "    mlflow.set_tag(\"model_purpose\", \"production_candidate\")\n",
    "    mlflow.set_tag(\"quality_check\", \"passed\")\n",
    "    mlflow.set_tag(\"model_type\", \"logistic_regression\")\n",
    "\n",
    "# === STEP 2: WORK WITH REGISTERED MODEL VERSIONS ===\n",
    "print(\"\\\\nüîÑ Working with Model Registry...\")\n",
    "\n",
    "# Get model registry client\n",
    "client = MlflowClient()\n",
    "\n",
    "try:\n",
    "    # Get information about the registered model\n",
    "    registered_model = client.get_registered_model(registered_model_name)\n",
    "    print(f\"\\\\nüìã Registered Model: {registered_model.name}\")\n",
    "    print(f\"   Description: {registered_model.description or 'No description'}\")\n",
    "    print(f\"   Creation Time: {registered_model.creation_timestamp}\")\n",
    "    \n",
    "    # List all versions of this model\n",
    "    model_versions = client.search_model_versions(f\"name='{registered_model_name}'\")\n",
    "    print(f\"\\\\nüìö Model Versions ({len(model_versions)} total):\")\n",
    "    \n",
    "    for version in model_versions:\n",
    "        print(f\"   Version {version.version}:\")\n",
    "        print(f\"      Stage: {version.current_stage}\")\n",
    "        print(f\"      Status: {version.status}\")\n",
    "        print(f\"      Run ID: {version.run_id}\")\n",
    "        print(f\"      Creation Time: {version.creation_timestamp}\")\n",
    "    \n",
    "    # === STEP 3: MANAGE MODEL STAGES ===\n",
    "    print(\"\\\\nüé≠ Managing Model Stages...\")\n",
    "    \n",
    "    # Get the latest version\n",
    "    latest_version = model_versions[0]  # Most recent version\n",
    "    current_version_number = latest_version.version\n",
    "    \n",
    "    print(f\"\\\\nüéØ Working with Version {current_version_number}...\")\n",
    "    \n",
    "    # Transition model to Staging\n",
    "    print(\"   üì§ Transitioning to Staging...\")\n",
    "    client.transition_model_version_stage(\n",
    "        name=registered_model_name,\n",
    "        version=current_version_number,\n",
    "        stage=\"Staging\",\n",
    "        archive_existing_versions=False  # Keep other versions in their current stages\n",
    "    )\n",
    "    \n",
    "    # Add annotation explaining the transition\n",
    "    client.update_model_version(\n",
    "        name=registered_model_name,\n",
    "        version=current_version_number,\n",
    "        description=f\"Iris classifier trained on {len(X_train)} samples. Accuracy: {accuracy:.4f}\"\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚úÖ Model version {current_version_number} moved to Staging\")\n",
    "    \n",
    "    # === STEP 4: ADD MODEL AND VERSION TAGS ===\n",
    "    print(\"\\\\nüè∑Ô∏è  Adding descriptive tags...\")\n",
    "    \n",
    "    # Add tags to the registered model (applies to all versions)\n",
    "    client.set_registered_model_tag(\n",
    "        name=registered_model_name,\n",
    "        key=\"task\",\n",
    "        value=\"classification\"\n",
    "    )\n",
    "    \n",
    "    client.set_registered_model_tag(\n",
    "        name=registered_model_name,\n",
    "        key=\"dataset\",\n",
    "        value=\"iris\"\n",
    "    )\n",
    "    \n",
    "    # Add tags to specific model version\n",
    "    client.set_model_version_tag(\n",
    "        name=registered_model_name,\n",
    "        version=current_version_number,\n",
    "        key=\"validation_accuracy\",\n",
    "        value=str(round(accuracy, 4))\n",
    "    )\n",
    "    \n",
    "    client.set_model_version_tag(\n",
    "        name=registered_model_name,\n",
    "        version=current_version_number,\n",
    "        key=\"training_framework\",\n",
    "        value=\"scikit-learn\"\n",
    "    )\n",
    "    \n",
    "    print(\"   ‚úÖ Tags added successfully\")\n",
    "    \n",
    "    # === STEP 5: SIMULATE PROMOTION TO PRODUCTION ===\n",
    "    print(\"\\\\nüöÄ Simulating production deployment...\")\n",
    "    \n",
    "    # In a real scenario, you would run additional validation here\n",
    "    # For demo, we'll promote directly to production\n",
    "    \n",
    "    print(\"   ‚úîÔ∏è  Model validation passed\")\n",
    "    print(\"   ‚úîÔ∏è  Performance benchmarks met\")\n",
    "    print(\"   ‚úîÔ∏è  Security review completed\")\n",
    "    \n",
    "    # Promote to production\n",
    "    client.transition_model_version_stage(\n",
    "        name=registered_model_name,\n",
    "        version=current_version_number,\n",
    "        stage=\"Production\",\n",
    "        archive_existing_versions=True  # Archive previous production versions\n",
    "    )\n",
    "    \n",
    "    print(f\"   üéâ Model version {current_version_number} promoted to Production!\")\n",
    "    \n",
    "    # === STEP 6: VIEW MODEL REGISTRY STATUS ===\n",
    "    print(\"\\\\nüìä Current Model Registry Status:\")\n",
    "    \n",
    "    # Refresh model versions list\n",
    "    updated_versions = client.search_model_versions(f\"name='{registered_model_name}'\")\n",
    "    \n",
    "    for version in updated_versions:\n",
    "        stage_emoji = {\n",
    "            \"None\": \"‚ö™\",\n",
    "            \"Staging\": \"üü°\", \n",
    "            \"Production\": \"üü¢\",\n",
    "            \"Archived\": \"üî¥\"\n",
    "        }\n",
    "        \n",
    "        emoji = stage_emoji.get(version.current_stage, \"‚ùì\")\n",
    "        print(f\"   {emoji} Version {version.version}: {version.current_stage}\")\n",
    "        \n",
    "        # Show tags for this version\n",
    "        version_tags = client.get_model_version(registered_model_name, version.version).tags\n",
    "        if version_tags:\n",
    "            for tag_key, tag_value in version_tags.items():\n",
    "                print(f\"        üè∑Ô∏è  {tag_key}: {tag_value}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Model registry operation failed: {e}\")\n",
    "    print(\"   This might happen if using file-based tracking without a server\")\n",
    "    print(\"   Model registry works best with MLflow tracking server\")\n",
    "\n",
    "print(\"\\\\n‚úÖ Model Registry demonstration complete!\")\n",
    "print(\"\\\\nüí° Model Registry Best Practices:\")\n",
    "print(\"   ‚úì Use descriptive model names and versions\")\n",
    "print(\"   ‚úì Add detailed descriptions and tags\")\n",
    "print(\"   ‚úì Follow proper stage transitions (None ‚Üí Staging ‚Üí Production)\")\n",
    "print(\"   ‚úì Archive old versions when promoting new ones\")\n",
    "print(\"   ‚úì Include performance metrics in model descriptions\")\n",
    "print(\"   ‚úì Use tags for categorization and searchability\")\n",
    "print(\"   ‚úì Implement proper validation before production promotion\")\n",
    "print(\"   ‚úì Consider using MLflow tracking server for team collaboration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a4af1d",
   "metadata": {},
   "source": [
    "## 10. Load and Use Registered Model\n",
    "\n",
    "Once models are registered, you can load them for inference in various ways:\n",
    "- **By version**: Load a specific model version\n",
    "- **By stage**: Load the current production model\n",
    "- **By run ID**: Load model from a specific training run\n",
    "\n",
    "This enables consistent model deployment and easy rollbacks if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "337520f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading and Using Registered Models...\n",
      "\\nüé≠ Method 1: Loading model by stage...\n",
      "‚ö†Ô∏è  Could not load production model: No versions of model with name 'iris-classifier-production' and stage 'Production' found\n",
      "   This is normal if no model is in Production stage yet\n",
      "\\nüî¢ Method 2: Loading model by specific version...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/29 16:16:25 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded model version 2 from: models:/iris-classifier-production/2\n",
      "   üìà Version 2 Accuracy: 0.9667\n",
      "\\nüÜî Method 3: Loading model by run ID...\n",
      "   üÜî Demo run ID: 970b1545effe48609e99d3f91e3ce650\n",
      "‚úÖ Loaded model from run: runs:/970b1545effe48609e99d3f91e3ce650/demo_model\n",
      "   üìà Run Model Accuracy: 0.9667\n",
      "\\nüìä Model Loading and Inference Analysis...\n",
      "\\nüìã Prediction Comparison (first 10 rows):\n",
      "   actual actual_name  version_pred  version_correct  run_pred  run_correct\n",
      "0       0      setosa             0             True         0         True\n",
      "1       2   virginica             2             True         2         True\n",
      "2       1  versicolor             1             True         1         True\n",
      "3       1  versicolor             1             True         1         True\n",
      "4       0      setosa             0             True         0         True\n",
      "5       1  versicolor             1             True         1         True\n",
      "6       0      setosa             0             True         0         True\n",
      "7       0      setosa             0             True         0         True\n",
      "8       2   virginica             2             True         2         True\n",
      "9       1  versicolor             1             True         1         True\n",
      "\\nüöÄ Practical Inference Examples...\n",
      "\\nüå∏ Example 1: Single flower prediction\n",
      "   Input features: [4.4 3.  1.3 0.2]\n",
      "   Predicted class: setosa (index: 0)\n",
      "   Actual class: setosa (index: 0)\n",
      "   Correct: ‚úÖ\n",
      "\\nüåª Example 2: Batch prediction with confidence\n",
      "   üÜî Demo run ID: 970b1545effe48609e99d3f91e3ce650\n",
      "‚úÖ Loaded model from run: runs:/970b1545effe48609e99d3f91e3ce650/demo_model\n",
      "   üìà Run Model Accuracy: 0.9667\n",
      "\\nüìä Model Loading and Inference Analysis...\n",
      "\\nüìã Prediction Comparison (first 10 rows):\n",
      "   actual actual_name  version_pred  version_correct  run_pred  run_correct\n",
      "0       0      setosa             0             True         0         True\n",
      "1       2   virginica             2             True         2         True\n",
      "2       1  versicolor             1             True         1         True\n",
      "3       1  versicolor             1             True         1         True\n",
      "4       0      setosa             0             True         0         True\n",
      "5       1  versicolor             1             True         1         True\n",
      "6       0      setosa             0             True         0         True\n",
      "7       0      setosa             0             True         0         True\n",
      "8       2   virginica             2             True         2         True\n",
      "9       1  versicolor             1             True         1         True\n",
      "\\nüöÄ Practical Inference Examples...\n",
      "\\nüå∏ Example 1: Single flower prediction\n",
      "   Input features: [4.4 3.  1.3 0.2]\n",
      "   Predicted class: setosa (index: 0)\n",
      "   Actual class: setosa (index: 0)\n",
      "   Correct: ‚úÖ\n",
      "\\nüåª Example 2: Batch prediction with confidence\n",
      "   Batch Predictions with Confidence:\n",
      "   Sample 1: setosa (confidence: 0.985) vs setosa ‚úÖ\n",
      "   Sample 2: virginica (confidence: 0.608) vs virginica ‚úÖ\n",
      "   Sample 3: versicolor (confidence: 0.809) vs versicolor ‚úÖ\n",
      "   Sample 4: versicolor (confidence: 0.840) vs versicolor ‚úÖ\n",
      "   Sample 5: setosa (confidence: 0.988) vs setosa ‚úÖ\n",
      "\\nüìã Model Metadata and Information...\n",
      "\\nüè∑Ô∏è  Model Registry Information:\n",
      "   Name: iris-classifier-production\n",
      "   Version: 2\n",
      "   Stage: None\n",
      "   Status: READY\n",
      "   Description: None\n",
      "   Creation Time: 1756448184335\n",
      "   Source Run: 1e6590f9b61749c782be833208a47b83\n",
      "\\n‚úÖ Model loading and inference demonstration complete!\n",
      "\\nüí° Model Loading Best Practices:\n",
      "   ‚úì Use stage-based loading for production systems\n",
      "   ‚úì Load by version for specific model comparisons\n",
      "   ‚úì Load by run ID for debugging and development\n",
      "   ‚úì Always validate loaded models before use\n",
      "   ‚úì Handle loading exceptions gracefully\n",
      "   ‚úì Log inference requests and results for monitoring\n",
      "   ‚úì Cache loaded models for better performance\n",
      "   ‚úì Implement model health checks in production\n",
      "\\nüéØ Model Serving Options:\n",
      "   üåê REST API: mlflow models serve\n",
      "   ‚òÅÔ∏è  Cloud: Deploy to AWS SageMaker, Azure ML, etc.\n",
      "   üê≥ Docker: mlflow models build-docker\n",
      "   ‚ö° Real-time: Apache Spark, Ray Serve\n",
      "   üì± Edge: Convert to TensorFlow Lite, ONNX\n",
      "   Batch Predictions with Confidence:\n",
      "   Sample 1: setosa (confidence: 0.985) vs setosa ‚úÖ\n",
      "   Sample 2: virginica (confidence: 0.608) vs virginica ‚úÖ\n",
      "   Sample 3: versicolor (confidence: 0.809) vs versicolor ‚úÖ\n",
      "   Sample 4: versicolor (confidence: 0.840) vs versicolor ‚úÖ\n",
      "   Sample 5: setosa (confidence: 0.988) vs setosa ‚úÖ\n",
      "\\nüìã Model Metadata and Information...\n",
      "\\nüè∑Ô∏è  Model Registry Information:\n",
      "   Name: iris-classifier-production\n",
      "   Version: 2\n",
      "   Stage: None\n",
      "   Status: READY\n",
      "   Description: None\n",
      "   Creation Time: 1756448184335\n",
      "   Source Run: 1e6590f9b61749c782be833208a47b83\n",
      "\\n‚úÖ Model loading and inference demonstration complete!\n",
      "\\nüí° Model Loading Best Practices:\n",
      "   ‚úì Use stage-based loading for production systems\n",
      "   ‚úì Load by version for specific model comparisons\n",
      "   ‚úì Load by run ID for debugging and development\n",
      "   ‚úì Always validate loaded models before use\n",
      "   ‚úì Handle loading exceptions gracefully\n",
      "   ‚úì Log inference requests and results for monitoring\n",
      "   ‚úì Cache loaded models for better performance\n",
      "   ‚úì Implement model health checks in production\n",
      "\\nüéØ Model Serving Options:\n",
      "   üåê REST API: mlflow models serve\n",
      "   ‚òÅÔ∏è  Cloud: Deploy to AWS SageMaker, Azure ML, etc.\n",
      "   üê≥ Docker: mlflow models build-docker\n",
      "   ‚ö° Real-time: Apache Spark, Ray Serve\n",
      "   üì± Edge: Convert to TensorFlow Lite, ONNX\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate loading and using registered models\n",
    "print(\"üì• Loading and Using Registered Models...\")\n",
    "\n",
    "# === METHOD 1: LOAD BY STAGE ===\n",
    "print(\"\\\\nüé≠ Method 1: Loading model by stage...\")\n",
    "\n",
    "try:\n",
    "    # Load the current production model\n",
    "    # This is the most common approach in production systems\n",
    "    production_model_uri = f\"models:/{registered_model_name}/Production\"\n",
    "    production_model = mlflow.pyfunc.load_model(production_model_uri)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded production model from: {production_model_uri}\")\n",
    "    \n",
    "    # Make predictions with the loaded model\n",
    "    print(\"\\\\nüîÆ Making predictions with production model...\")\n",
    "    \n",
    "    # Use test data for predictions\n",
    "    predictions = production_model.predict(X_test)\n",
    "    \n",
    "    print(f\"   üìä Predictions shape: {predictions.shape}\")\n",
    "    print(f\"   üéØ Sample predictions: {predictions[:5]}\")\n",
    "    \n",
    "    # Calculate and display performance\n",
    "    prod_accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"   üìà Production Model Accuracy: {prod_accuracy:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not load production model: {e}\")\n",
    "    print(\"   This is normal if no model is in Production stage yet\")\n",
    "\n",
    "# === METHOD 2: LOAD BY VERSION ===\n",
    "print(\"\\\\nüî¢ Method 2: Loading model by specific version...\")\n",
    "\n",
    "try:\n",
    "    # Get the latest version number\n",
    "    client = MlflowClient()\n",
    "    latest_versions = client.get_latest_versions(registered_model_name, stages=[\"None\", \"Staging\", \"Production\"])\n",
    "    \n",
    "    if latest_versions:\n",
    "        latest_version = latest_versions[0].version\n",
    "        \n",
    "        # Load specific version\n",
    "        version_model_uri = f\"models:/{registered_model_name}/{latest_version}\"\n",
    "        version_model = mlflow.pyfunc.load_model(version_model_uri)\n",
    "        \n",
    "        print(f\"‚úÖ Loaded model version {latest_version} from: {version_model_uri}\")\n",
    "        \n",
    "        # Make predictions\n",
    "        version_predictions = version_model.predict(X_test)\n",
    "        version_accuracy = accuracy_score(y_test, version_predictions)\n",
    "        print(f\"   üìà Version {latest_version} Accuracy: {version_accuracy:.4f}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  No model versions found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not load model by version: {e}\")\n",
    "\n",
    "# === METHOD 3: LOAD BY RUN ID ===\n",
    "print(\"\\\\nüÜî Method 3: Loading model by run ID...\")\n",
    "\n",
    "# For demonstration, let's train a new model and load it by run ID\n",
    "with mlflow.start_run(run_name=\"model-for-run-id-demo\") as demo_run:\n",
    "    \n",
    "    # Train a simple model\n",
    "    demo_model = LogisticRegression(random_state=42)\n",
    "    demo_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Log the model\n",
    "    demo_signature = infer_signature(X_train, demo_model.predict(X_train))\n",
    "    demo_model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=demo_model,\n",
    "        artifact_path=\"demo_model\",\n",
    "        signature=demo_signature\n",
    "    )\n",
    "    \n",
    "    demo_run_id = demo_run.info.run_id\n",
    "    print(f\"   üÜî Demo run ID: {demo_run_id}\")\n",
    "\n",
    "# Load model by run ID\n",
    "run_model_uri = f\"runs:/{demo_run_id}/demo_model\"\n",
    "run_model = mlflow.pyfunc.load_model(run_model_uri)\n",
    "\n",
    "print(f\"‚úÖ Loaded model from run: {run_model_uri}\")\n",
    "\n",
    "# Make predictions\n",
    "run_predictions = run_model.predict(X_test)\n",
    "run_accuracy = accuracy_score(y_test, run_predictions)\n",
    "print(f\"   üìà Run Model Accuracy: {run_accuracy:.4f}\")\n",
    "\n",
    "# === MODEL COMPARISON AND ANALYSIS ===\n",
    "print(\"\\\\nüìä Model Loading and Inference Analysis...\")\n",
    "\n",
    "# Create a comprehensive prediction analysis\n",
    "prediction_results = pd.DataFrame({\n",
    "    'actual': y_test,\n",
    "    'actual_name': [target_names[i] for i in y_test]\n",
    "})\n",
    "\n",
    "# Add predictions from different loading methods\n",
    "if 'predictions' in locals():\n",
    "    prediction_results['production_pred'] = predictions\n",
    "    prediction_results['production_correct'] = y_test == predictions\n",
    "\n",
    "if 'version_predictions' in locals():\n",
    "    prediction_results['version_pred'] = version_predictions\n",
    "    prediction_results['version_correct'] = y_test == version_predictions\n",
    "\n",
    "prediction_results['run_pred'] = run_predictions\n",
    "prediction_results['run_correct'] = y_test == run_predictions\n",
    "\n",
    "print(\"\\\\nüìã Prediction Comparison (first 10 rows):\")\n",
    "print(prediction_results.head(10))\n",
    "\n",
    "# Calculate consistency between models\n",
    "if 'predictions' in locals() and 'version_predictions' in locals():\n",
    "    consistency = np.mean(predictions == version_predictions)\n",
    "    print(f\"\\\\nüîÑ Model Consistency:\")\n",
    "    print(f\"   Production vs Version: {consistency:.4f} agreement\")\n",
    "\n",
    "# === PRACTICAL INFERENCE EXAMPLES ===\n",
    "print(\"\\\\nüöÄ Practical Inference Examples...\")\n",
    "\n",
    "# Example 1: Single prediction\n",
    "print(\"\\\\nüå∏ Example 1: Single flower prediction\")\n",
    "single_flower = X_test[0:1]  # First test sample\n",
    "single_prediction = run_model.predict(single_flower)[0]\n",
    "single_actual = y_test[0]\n",
    "\n",
    "print(f\"   Input features: {single_flower[0]}\")\n",
    "print(f\"   Predicted class: {target_names[int(single_prediction)]} (index: {single_prediction})\")\n",
    "print(f\"   Actual class: {target_names[single_actual]} (index: {single_actual})\")\n",
    "print(f\"   Correct: {'‚úÖ' if single_prediction == single_actual else '‚ùå'}\")\n",
    "\n",
    "# Example 2: Batch prediction with confidence\n",
    "print(\"\\\\nüåª Example 2: Batch prediction with confidence\")\n",
    "\n",
    "# Load the sklearn model directly for probability scores\n",
    "try:\n",
    "    sklearn_model = mlflow.sklearn.load_model(run_model_uri)\n",
    "    batch_samples = X_test[:5]\n",
    "    batch_predictions = sklearn_model.predict(batch_samples)\n",
    "    batch_probabilities = sklearn_model.predict_proba(batch_samples)\n",
    "    \n",
    "    print(\"   Batch Predictions with Confidence:\")\n",
    "    for i, (pred, prob, actual) in enumerate(zip(batch_predictions, batch_probabilities, y_test[:5])):\n",
    "        confidence = np.max(prob)\n",
    "        predicted_class = target_names[int(pred)]\n",
    "        actual_class = target_names[actual]\n",
    "        status = '‚úÖ' if pred == actual else '‚ùå'\n",
    "        \n",
    "        print(f\"   Sample {i+1}: {predicted_class} (confidence: {confidence:.3f}) vs {actual_class} {status}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Could not load sklearn model directly: {e}\")\n",
    "\n",
    "# === MODEL METADATA AND INFORMATION ===\n",
    "print(\"\\\\nüìã Model Metadata and Information...\")\n",
    "\n",
    "try:\n",
    "    # Get model information\n",
    "    model_version_info = client.get_model_version(registered_model_name, latest_version)\n",
    "    \n",
    "    print(f\"\\\\nüè∑Ô∏è  Model Registry Information:\")\n",
    "    print(f\"   Name: {model_version_info.name}\")\n",
    "    print(f\"   Version: {model_version_info.version}\")\n",
    "    print(f\"   Stage: {model_version_info.current_stage}\")\n",
    "    print(f\"   Status: {model_version_info.status}\")\n",
    "    print(f\"   Description: {model_version_info.description}\")\n",
    "    print(f\"   Creation Time: {model_version_info.creation_timestamp}\")\n",
    "    print(f\"   Source Run: {model_version_info.run_id}\")\n",
    "    \n",
    "    # Show tags\n",
    "    if model_version_info.tags:\n",
    "        print(f\"   Tags:\")\n",
    "        for tag_key, tag_value in model_version_info.tags.items():\n",
    "            print(f\"      {tag_key}: {tag_value}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Could not retrieve model metadata: {e}\")\n",
    "\n",
    "print(\"\\\\n‚úÖ Model loading and inference demonstration complete!\")\n",
    "\n",
    "print(\"\\\\nüí° Model Loading Best Practices:\")\n",
    "print(\"   ‚úì Use stage-based loading for production systems\")\n",
    "print(\"   ‚úì Load by version for specific model comparisons\")\n",
    "print(\"   ‚úì Load by run ID for debugging and development\")\n",
    "print(\"   ‚úì Always validate loaded models before use\")\n",
    "print(\"   ‚úì Handle loading exceptions gracefully\")\n",
    "print(\"   ‚úì Log inference requests and results for monitoring\")\n",
    "print(\"   ‚úì Cache loaded models for better performance\")\n",
    "print(\"   ‚úì Implement model health checks in production\")\n",
    "\n",
    "print(\"\\\\nüéØ Model Serving Options:\")\n",
    "print(\"   üåê REST API: mlflow models serve\")\n",
    "print(\"   ‚òÅÔ∏è  Cloud: Deploy to AWS SageMaker, Azure ML, etc.\")\n",
    "print(\"   üê≥ Docker: mlflow models build-docker\")\n",
    "print(\"   ‚ö° Real-time: Apache Spark, Ray Serve\")\n",
    "print(\"   üì± Edge: Convert to TensorFlow Lite, ONNX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf93282",
   "metadata": {},
   "source": [
    "## 11. Compare Multiple Experiment Runs\n",
    "\n",
    "One of MLflow's most powerful features is the ability to compare multiple experiments systematically. This enables data-driven decision making and hypothesis testing in ML development.\n",
    "\n",
    "### What to Compare:\n",
    "- **Different algorithms**: Logistic Regression vs Random Forest vs SVM\n",
    "- **Hyperparameter variations**: Different learning rates, regularization values\n",
    "- **Feature engineering**: Different preprocessing techniques\n",
    "- **Data variations**: Different train/test splits, feature selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73471620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Comprehensive Experiment Comparison...\n",
      "\\nüß™ Experiment 1: Comparing Different Algorithms...\n",
      "\\n   üöÄ Training Logistic Regression...\n",
      "      ‚úÖ Logistic Regression - Accuracy: 0.9667, Time: 0.0990s\n",
      "\\n   üöÄ Training Random Forest...\n",
      "      ‚úÖ Logistic Regression - Accuracy: 0.9667, Time: 0.0990s\n",
      "\\n   üöÄ Training Random Forest...\n",
      "      ‚úÖ Random Forest - Accuracy: 0.9000, Time: 0.3229s\n",
      "\\nüéõÔ∏è  Experiment 2: Logistic Regression Hyperparameter Tuning...\n",
      "\\n   üîß Testing C = 0.1...\n",
      "      üìä C=0.1: Accuracy = 0.9667\n",
      "\\n   üîß Testing C = 0.5...\n",
      "      ‚úÖ Random Forest - Accuracy: 0.9000, Time: 0.3229s\n",
      "\\nüéõÔ∏è  Experiment 2: Logistic Regression Hyperparameter Tuning...\n",
      "\\n   üîß Testing C = 0.1...\n",
      "      üìä C=0.1: Accuracy = 0.9667\n",
      "\\n   üîß Testing C = 0.5...\n",
      "      üìä C=0.5: Accuracy = 0.9667\n",
      "\\n   üîß Testing C = 1.0...\n",
      "      üìä C=1.0: Accuracy = 0.9667\n",
      "\\n   üîß Testing C = 2.0...\n",
      "      üìä C=0.5: Accuracy = 0.9667\n",
      "\\n   üîß Testing C = 1.0...\n",
      "      üìä C=1.0: Accuracy = 0.9667\n",
      "\\n   üîß Testing C = 2.0...\n",
      "      üìä C=2.0: Accuracy = 0.9667\n",
      "\\n   üîß Testing C = 5.0...\n",
      "      üìä C=5.0: Accuracy = 1.0000\n",
      "\\n‚öôÔ∏è  Experiment 3: Feature Engineering Comparison...\n",
      "\\n   üî¨ Testing No Scaling...\n",
      "      üìä C=2.0: Accuracy = 0.9667\n",
      "\\n   üîß Testing C = 5.0...\n",
      "      üìä C=5.0: Accuracy = 1.0000\n",
      "\\n‚öôÔ∏è  Experiment 3: Feature Engineering Comparison...\n",
      "\\n   üî¨ Testing No Scaling...\n",
      "      üìà No Scaling: Accuracy = 0.9667\n",
      "\\n   üî¨ Testing Standard Scaling...\n",
      "      üìà Standard Scaling: Accuracy = 0.9333\n",
      "\\n   üî¨ Testing MinMax Scaling...\n",
      "      üìà MinMax Scaling: Accuracy = 0.9000\n",
      "\\nüìä Comprehensive Results Analysis...\n",
      "\\nüèÜ Algorithm Comparison Results:\n",
      "             algorithm                            run_id  accuracy  precision  \\\n",
      "0  Logistic Regression  e45821eeede84432b8c4a958d396e1df    0.9667     0.9697   \n",
      "1        Random Forest  80cb2877b19e4c16bd419e3a402034c2    0.9000     0.9024   \n",
      "\n",
      "   recall  f1_score  training_time  \n",
      "0  0.9667    0.9666         0.0990  \n",
      "1  0.9000    0.8997         0.3229  \n",
      "\\nüéõÔ∏è  Hyperparameter Tuning Results:\n",
      "     C                            run_id  accuracy  precision  recall  \\\n",
      "0  0.1  038dab9226f94bec9c47fd2cdf64bb21    0.9667     0.9697  0.9667   \n",
      "1  0.5  de21e7c389494cf29d36b92923fa95f5    0.9667     0.9697  0.9667   \n",
      "2  1.0  d3d4fe3cb16d490aa88c0ed52b18ea86    0.9667     0.9697  0.9667   \n",
      "3  2.0  efc8623045184b369bbb34adbe62f1f7    0.9667     0.9697  0.9667   \n",
      "4  5.0  23e0dfb86a6448d388d558d04bb31987    1.0000     1.0000  1.0000   \n",
      "\n",
      "   f1_score  \n",
      "0    0.9666  \n",
      "1    0.9666  \n",
      "2    0.9666  \n",
      "3    0.9666  \n",
      "4    1.0000  \n",
      "\\n‚öôÔ∏è  Feature Engineering Results:\n",
      "            scaling                            run_id  accuracy  precision  \\\n",
      "0        No Scaling  cb5b691d5f2a41738bcc42f548a5d881    0.9667     0.9697   \n",
      "1  Standard Scaling  0e19dbe1cf2a45bd953cc467e4e34605    0.9333     0.9333   \n",
      "2    MinMax Scaling  bee7eeaf35dd43c28bc697d90b745a4d    0.9000     0.9024   \n",
      "\n",
      "   recall  f1_score  \n",
      "0  0.9667    0.9666  \n",
      "1  0.9333    0.9333  \n",
      "2  0.9000    0.8997  \n",
      "\\nü•á Best Performers Summary:\n",
      "   üéØ Best Algorithm: Logistic Regression (Accuracy: 0.9667)\n",
      "   üéõÔ∏è  Best C Value: 5.0 (Accuracy: 1.0000)\n",
      "   ‚öôÔ∏è  Best Scaling: No Scaling (Accuracy: 0.9667)\n",
      "\\nüîç Programmatic Experiment Querying...\n",
      "      üìà No Scaling: Accuracy = 0.9667\n",
      "\\n   üî¨ Testing Standard Scaling...\n",
      "      üìà Standard Scaling: Accuracy = 0.9333\n",
      "\\n   üî¨ Testing MinMax Scaling...\n",
      "      üìà MinMax Scaling: Accuracy = 0.9000\n",
      "\\nüìä Comprehensive Results Analysis...\n",
      "\\nüèÜ Algorithm Comparison Results:\n",
      "             algorithm                            run_id  accuracy  precision  \\\n",
      "0  Logistic Regression  e45821eeede84432b8c4a958d396e1df    0.9667     0.9697   \n",
      "1        Random Forest  80cb2877b19e4c16bd419e3a402034c2    0.9000     0.9024   \n",
      "\n",
      "   recall  f1_score  training_time  \n",
      "0  0.9667    0.9666         0.0990  \n",
      "1  0.9000    0.8997         0.3229  \n",
      "\\nüéõÔ∏è  Hyperparameter Tuning Results:\n",
      "     C                            run_id  accuracy  precision  recall  \\\n",
      "0  0.1  038dab9226f94bec9c47fd2cdf64bb21    0.9667     0.9697  0.9667   \n",
      "1  0.5  de21e7c389494cf29d36b92923fa95f5    0.9667     0.9697  0.9667   \n",
      "2  1.0  d3d4fe3cb16d490aa88c0ed52b18ea86    0.9667     0.9697  0.9667   \n",
      "3  2.0  efc8623045184b369bbb34adbe62f1f7    0.9667     0.9697  0.9667   \n",
      "4  5.0  23e0dfb86a6448d388d558d04bb31987    1.0000     1.0000  1.0000   \n",
      "\n",
      "   f1_score  \n",
      "0    0.9666  \n",
      "1    0.9666  \n",
      "2    0.9666  \n",
      "3    0.9666  \n",
      "4    1.0000  \n",
      "\\n‚öôÔ∏è  Feature Engineering Results:\n",
      "            scaling                            run_id  accuracy  precision  \\\n",
      "0        No Scaling  cb5b691d5f2a41738bcc42f548a5d881    0.9667     0.9697   \n",
      "1  Standard Scaling  0e19dbe1cf2a45bd953cc467e4e34605    0.9333     0.9333   \n",
      "2    MinMax Scaling  bee7eeaf35dd43c28bc697d90b745a4d    0.9000     0.9024   \n",
      "\n",
      "   recall  f1_score  \n",
      "0  0.9667    0.9666  \n",
      "1  0.9333    0.9333  \n",
      "2  0.9000    0.8997  \n",
      "\\nü•á Best Performers Summary:\n",
      "   üéØ Best Algorithm: Logistic Regression (Accuracy: 0.9667)\n",
      "   üéõÔ∏è  Best C Value: 5.0 (Accuracy: 1.0000)\n",
      "   ‚öôÔ∏è  Best Scaling: No Scaling (Accuracy: 0.9667)\n",
      "\\nüîç Programmatic Experiment Querying...\n",
      "\\nüìã Querying experiment: iris-classification-tutorial (ID: 636605946707093340)\n",
      "\\nüìã Querying experiment: iris-classification-tutorial (ID: 636605946707093340)\n",
      "   üìà Total runs in experiment: 25\n",
      "   üìà Total runs in experiment: 25\n",
      "   üß™ Algorithm comparison runs: 2\n",
      "\\nüèÜ Overall Best Run:\n",
      "   Run ID: 23e0dfb86a6448d388d558d04bb31987\n",
      "   Accuracy: 1.0000\n",
      "   Algorithm: logistic_regression\n",
      "   Tags: hyperparameter_tuning\n",
      "\\nüìä Visualization and Analysis Recommendations:\n",
      "\\nüí° To visualize these results, you can:\n",
      "   üìà Use MLflow UI to compare runs side-by-side\n",
      "   üìä Create custom plots comparing metrics across runs\n",
      "   üéØ Use parallel coordinates plots for hyperparameter analysis\n",
      "   üìã Export run data to pandas for custom analysis\n",
      "   üîç Filter runs by tags and parameters for focused comparisons\n",
      "\\nüéØ Advanced Analysis Ideas:\n",
      "   üìä Statistical significance testing between models\n",
      "   üîÑ Cross-validation across multiple runs\n",
      "   üìà Learning curves for training progression\n",
      "   üéõÔ∏è  Hyperparameter optimization with tools like Optuna\n",
      "   üìã A/B testing frameworks for model comparison\n",
      "\\n‚úÖ Comprehensive experiment comparison complete!\n",
      "\\nüöÄ Next Steps:\n",
      "   1. Open MLflow UI to visually compare runs\n",
      "   2. Export best model to production\n",
      "   3. Set up automated hyperparameter tuning\n",
      "   4. Implement model monitoring and drift detection\n",
      "   5. Scale experiments with MLflow Projects\n",
      "   üß™ Algorithm comparison runs: 2\n",
      "\\nüèÜ Overall Best Run:\n",
      "   Run ID: 23e0dfb86a6448d388d558d04bb31987\n",
      "   Accuracy: 1.0000\n",
      "   Algorithm: logistic_regression\n",
      "   Tags: hyperparameter_tuning\n",
      "\\nüìä Visualization and Analysis Recommendations:\n",
      "\\nüí° To visualize these results, you can:\n",
      "   üìà Use MLflow UI to compare runs side-by-side\n",
      "   üìä Create custom plots comparing metrics across runs\n",
      "   üéØ Use parallel coordinates plots for hyperparameter analysis\n",
      "   üìã Export run data to pandas for custom analysis\n",
      "   üîç Filter runs by tags and parameters for focused comparisons\n",
      "\\nüéØ Advanced Analysis Ideas:\n",
      "   üìä Statistical significance testing between models\n",
      "   üîÑ Cross-validation across multiple runs\n",
      "   üìà Learning curves for training progression\n",
      "   üéõÔ∏è  Hyperparameter optimization with tools like Optuna\n",
      "   üìã A/B testing frameworks for model comparison\n",
      "\\n‚úÖ Comprehensive experiment comparison complete!\n",
      "\\nüöÄ Next Steps:\n",
      "   1. Open MLflow UI to visually compare runs\n",
      "   2. Export best model to production\n",
      "   3. Set up automated hyperparameter tuning\n",
      "   4. Implement model monitoring and drift detection\n",
      "   5. Scale experiments with MLflow Projects\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate comprehensive experiment comparison\n",
    "print(\"üî¨ Comprehensive Experiment Comparison...\")\n",
    "\n",
    "# === EXPERIMENT 1: DIFFERENT ALGORITHMS ===\n",
    "print(\"\\\\nüß™ Experiment 1: Comparing Different Algorithms...\")\n",
    "\n",
    "algorithms = [\n",
    "    {\n",
    "        \"name\": \"Logistic Regression\",\n",
    "        \"model\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "        \"params\": {\"algorithm\": \"logistic_regression\", \"solver\": \"lbfgs\", \"max_iter\": 1000}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Random Forest\",\n",
    "        \"model\": RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "        \"params\": {\"algorithm\": \"random_forest\", \"n_estimators\": 100, \"max_depth\": None}\n",
    "    }\n",
    "]\n",
    "\n",
    "algorithm_results = []\n",
    "\n",
    "for algo_config in algorithms:\n",
    "    with mlflow.start_run(run_name=f\"algorithm-comparison-{algo_config['name'].lower().replace(' ', '-')}\"):\n",
    "        \n",
    "        print(f\"\\\\n   üöÄ Training {algo_config['name']}...\")\n",
    "        \n",
    "        # Train the model\n",
    "        model = algo_config['model']\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred, average='weighted'),\n",
    "            \"recall\": recall_score(y_test, y_pred, average='weighted'),\n",
    "            \"f1_score\": f1_score(y_test, y_pred, average='weighted'),\n",
    "            \"training_time\": training_time\n",
    "        }\n",
    "        \n",
    "        # Log parameters and metrics\n",
    "        mlflow.log_params(algo_config['params'])\n",
    "        mlflow.log_metrics(metrics)\n",
    "        \n",
    "        # Add tags\n",
    "        mlflow.set_tag(\"experiment_type\", \"algorithm_comparison\")\n",
    "        mlflow.set_tag(\"algorithm\", algo_config['params']['algorithm'])\n",
    "        \n",
    "        # Store results for comparison\n",
    "        result = {\n",
    "            \"algorithm\": algo_config['name'],\n",
    "            \"run_id\": mlflow.active_run().info.run_id,\n",
    "            **metrics\n",
    "        }\n",
    "        algorithm_results.append(result)\n",
    "        \n",
    "        print(f\"      ‚úÖ {algo_config['name']} - Accuracy: {metrics['accuracy']:.4f}, Time: {training_time:.4f}s\")\n",
    "\n",
    "# === EXPERIMENT 2: HYPERPARAMETER TUNING ===\n",
    "print(\"\\\\nüéõÔ∏è  Experiment 2: Logistic Regression Hyperparameter Tuning...\")\n",
    "\n",
    "# Test different C values (regularization strength)\n",
    "c_values = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "hyperparameter_results = []\n",
    "\n",
    "for c_value in c_values:\n",
    "    with mlflow.start_run(run_name=f\"lr-hyperparameter-C-{c_value}\"):\n",
    "        \n",
    "        print(f\"\\\\n   üîß Testing C = {c_value}...\")\n",
    "        \n",
    "        # Train model with specific C value\n",
    "        model = LogisticRegression(C=c_value, random_state=42, max_iter=1000)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred, average='weighted'),\n",
    "            \"recall\": recall_score(y_test, y_pred, average='weighted'),\n",
    "            \"f1_score\": f1_score(y_test, y_pred, average='weighted')\n",
    "        }\n",
    "        \n",
    "        # Log parameters and metrics\n",
    "        mlflow.log_params({\n",
    "            \"algorithm\": \"logistic_regression\",\n",
    "            \"C\": c_value,\n",
    "            \"solver\": \"lbfgs\",\n",
    "            \"regularization\": \"l2\"\n",
    "        })\n",
    "        mlflow.log_metrics(metrics)\n",
    "        \n",
    "        # Tags for organization\n",
    "        mlflow.set_tag(\"experiment_type\", \"hyperparameter_tuning\")\n",
    "        mlflow.set_tag(\"parameter_tuned\", \"C\")\n",
    "        \n",
    "        result = {\n",
    "            \"C\": c_value,\n",
    "            \"run_id\": mlflow.active_run().info.run_id,\n",
    "            **metrics\n",
    "        }\n",
    "        hyperparameter_results.append(result)\n",
    "        \n",
    "        print(f\"      üìä C={c_value}: Accuracy = {metrics['accuracy']:.4f}\")\n",
    "\n",
    "# === EXPERIMENT 3: FEATURE ENGINEERING ===\n",
    "print(\"\\\\n‚öôÔ∏è  Experiment 3: Feature Engineering Comparison...\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "feature_configs = [\n",
    "    {\"name\": \"No Scaling\", \"scaler\": None},\n",
    "    {\"name\": \"Standard Scaling\", \"scaler\": StandardScaler()},\n",
    "    {\"name\": \"MinMax Scaling\", \"scaler\": MinMaxScaler()}\n",
    "]\n",
    "\n",
    "feature_results = []\n",
    "\n",
    "for config in feature_configs:\n",
    "    with mlflow.start_run(run_name=f\"feature-engineering-{config['name'].lower().replace(' ', '-')}\"):\n",
    "        \n",
    "        print(f\"\\\\n   üî¨ Testing {config['name']}...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        if config['scaler'] is not None:\n",
    "            X_train_scaled = config['scaler'].fit_transform(X_train)\n",
    "            X_test_scaled = config['scaler'].transform(X_test)\n",
    "        else:\n",
    "            X_train_scaled = X_train\n",
    "            X_test_scaled = X_test\n",
    "        \n",
    "        # Train model\n",
    "        model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred, average='weighted'),\n",
    "            \"recall\": recall_score(y_test, y_pred, average='weighted'),\n",
    "            \"f1_score\": f1_score(y_test, y_pred, average='weighted')\n",
    "        }\n",
    "        \n",
    "        # Log parameters and metrics\n",
    "        mlflow.log_params({\n",
    "            \"algorithm\": \"logistic_regression\",\n",
    "            \"feature_scaling\": config['name'].lower().replace(' ', '_'),\n",
    "            \"scaler_type\": type(config['scaler']).__name__ if config['scaler'] else \"none\"\n",
    "        })\n",
    "        mlflow.log_metrics(metrics)\n",
    "        \n",
    "        # Tags\n",
    "        mlflow.set_tag(\"experiment_type\", \"feature_engineering\")\n",
    "        mlflow.set_tag(\"scaling_method\", config['name'])\n",
    "        \n",
    "        result = {\n",
    "            \"scaling\": config['name'],\n",
    "            \"run_id\": mlflow.active_run().info.run_id,\n",
    "            **metrics\n",
    "        }\n",
    "        feature_results.append(result)\n",
    "        \n",
    "        print(f\"      üìà {config['name']}: Accuracy = {metrics['accuracy']:.4f}\")\n",
    "\n",
    "# === ANALYZE AND COMPARE RESULTS ===\n",
    "print(\"\\\\nüìä Comprehensive Results Analysis...\")\n",
    "\n",
    "# Create comparison DataFrames\n",
    "print(\"\\\\nüèÜ Algorithm Comparison Results:\")\n",
    "algo_df = pd.DataFrame(algorithm_results)\n",
    "print(algo_df.round(4))\n",
    "\n",
    "print(\"\\\\nüéõÔ∏è  Hyperparameter Tuning Results:\")\n",
    "hyperparam_df = pd.DataFrame(hyperparameter_results)\n",
    "print(hyperparam_df.round(4))\n",
    "\n",
    "print(\"\\\\n‚öôÔ∏è  Feature Engineering Results:\")\n",
    "feature_df = pd.DataFrame(feature_results)\n",
    "print(feature_df.round(4))\n",
    "\n",
    "# Find best performers\n",
    "print(\"\\\\nü•á Best Performers Summary:\")\n",
    "\n",
    "best_algorithm = algo_df.loc[algo_df['accuracy'].idxmax()]\n",
    "print(f\"   üéØ Best Algorithm: {best_algorithm['algorithm']} (Accuracy: {best_algorithm['accuracy']:.4f})\")\n",
    "\n",
    "best_hyperparameter = hyperparam_df.loc[hyperparam_df['accuracy'].idxmax()]\n",
    "print(f\"   üéõÔ∏è  Best C Value: {best_hyperparameter['C']} (Accuracy: {best_hyperparameter['accuracy']:.4f})\")\n",
    "\n",
    "best_feature = feature_df.loc[feature_df['accuracy'].idxmax()]\n",
    "print(f\"   ‚öôÔ∏è  Best Scaling: {best_feature['scaling']} (Accuracy: {best_feature['accuracy']:.4f})\")\n",
    "\n",
    "# === PROGRAMMATIC EXPERIMENT QUERYING ===\n",
    "print(\"\\\\nüîç Programmatic Experiment Querying...\")\n",
    "\n",
    "# Get experiment ID\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "experiment_id = experiment.experiment_id\n",
    "\n",
    "# Search runs programmatically\n",
    "print(f\"\\\\nüìã Querying experiment: {experiment_name} (ID: {experiment_id})\")\n",
    "\n",
    "# Query all runs in this experiment\n",
    "all_runs = mlflow.search_runs(experiment_ids=[experiment_id])\n",
    "print(f\"   üìà Total runs in experiment: {len(all_runs)}\")\n",
    "\n",
    "# Filter runs by tag\n",
    "algorithm_comparison_runs = mlflow.search_runs(\n",
    "    experiment_ids=[experiment_id],\n",
    "    filter_string=\"tags.experiment_type = 'algorithm_comparison'\"\n",
    ")\n",
    "print(f\"   üß™ Algorithm comparison runs: {len(algorithm_comparison_runs)}\")\n",
    "\n",
    "# Get best run by metric\n",
    "if len(all_runs) > 0:\n",
    "    best_run = all_runs.loc[all_runs['metrics.accuracy'].idxmax()]\n",
    "    print(f\"\\\\nüèÜ Overall Best Run:\")\n",
    "    print(f\"   Run ID: {best_run['run_id']}\")\n",
    "    print(f\"   Accuracy: {best_run['metrics.accuracy']:.4f}\")\n",
    "    print(f\"   Algorithm: {best_run.get('params.algorithm', 'Unknown')}\")\n",
    "    print(f\"   Tags: {best_run.get('tags.experiment_type', 'Unknown')}\")\n",
    "\n",
    "# === VISUALIZATION RECOMMENDATIONS ===\n",
    "print(\"\\\\nüìä Visualization and Analysis Recommendations:\")\n",
    "print(\"\\\\nüí° To visualize these results, you can:\")\n",
    "print(\"   üìà Use MLflow UI to compare runs side-by-side\")\n",
    "print(\"   üìä Create custom plots comparing metrics across runs\")\n",
    "print(\"   üéØ Use parallel coordinates plots for hyperparameter analysis\")\n",
    "print(\"   üìã Export run data to pandas for custom analysis\")\n",
    "print(\"   üîç Filter runs by tags and parameters for focused comparisons\")\n",
    "\n",
    "print(\"\\\\nüéØ Advanced Analysis Ideas:\")\n",
    "print(\"   üìä Statistical significance testing between models\")\n",
    "print(\"   üîÑ Cross-validation across multiple runs\")\n",
    "print(\"   üìà Learning curves for training progression\")\n",
    "print(\"   üéõÔ∏è  Hyperparameter optimization with tools like Optuna\")\n",
    "print(\"   üìã A/B testing frameworks for model comparison\")\n",
    "\n",
    "print(\"\\\\n‚úÖ Comprehensive experiment comparison complete!\")\n",
    "print(\"\\\\nüöÄ Next Steps:\")\n",
    "print(\"   1. Open MLflow UI to visually compare runs\")\n",
    "print(\"   2. Export best model to production\")\n",
    "print(\"   3. Set up automated hyperparameter tuning\")\n",
    "print(\"   4. Implement model monitoring and drift detection\")\n",
    "print(\"   5. Scale experiments with MLflow Projects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb395a2e",
   "metadata": {},
   "source": [
    "## üéâ Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You've completed a comprehensive MLflow tutorial covering all major aspects of experiment tracking and model management.\n",
    "\n",
    "### üìö What You've Learned:\n",
    "\n",
    "1. **‚úÖ MLflow Setup**: Configured tracking URI and experiment organization\n",
    "2. **‚úÖ Parameter Logging**: Tracked hyperparameters and configuration settings  \n",
    "3. **‚úÖ Metric Tracking**: Logged performance metrics and training progress\n",
    "4. **‚úÖ Artifact Management**: Saved models, plots, and data files\n",
    "5. **‚úÖ Model Registry**: Registered, versioned, and staged models\n",
    "6. **‚úÖ Model Loading**: Loaded models for inference and production use\n",
    "7. **‚úÖ Experiment Comparison**: Systematically compared multiple approaches\n",
    "\n",
    "### üöÄ Production Best Practices:\n",
    "\n",
    "- **üîß Set up MLflow Tracking Server** for team collaboration\n",
    "- **üìä Use consistent experiment naming** and tagging strategies  \n",
    "- **üéØ Implement automated model validation** before production deployment\n",
    "- **üìà Monitor model performance** in production with drift detection\n",
    "- **üîÑ Set up CI/CD pipelines** with MLflow for automated training and deployment\n",
    "- **üìã Document experiments** with detailed descriptions and metadata\n",
    "- **üõ°Ô∏è Implement model governance** with approval workflows\n",
    "\n",
    "### üéØ Next Learning Steps:\n",
    "\n",
    "1. **MLflow Projects**: Package your ML code for reproducible runs\n",
    "2. **MLflow Plugins**: Extend MLflow with custom tracking backends\n",
    "3. **Model Serving**: Deploy models with `mlflow models serve`\n",
    "4. **Docker Integration**: Containerize models with `mlflow models build-docker`\n",
    "5. **Cloud Deployment**: Deploy to AWS SageMaker, Azure ML, or GCP\n",
    "6. **Advanced Tracking**: Custom metrics, nested runs, and system metrics\n",
    "7. **Integration**: Connect with Apache Airflow, Kubernetes, and Spark\n",
    "\n",
    "### üîó Useful Resources:\n",
    "\n",
    "- **üìñ MLflow Documentation**: https://mlflow.org/docs/latest/\n",
    "- **üéì MLflow Tutorials**: https://mlflow.org/docs/latest/tutorials-and-examples/\n",
    "- **üíª GitHub Repository**: https://github.com/mlflow/mlflow\n",
    "- **üè™ MLflow Model Registry**: https://mlflow.org/docs/latest/model-registry.html\n",
    "- **üöÄ Deployment Guide**: https://mlflow.org/docs/latest/models.html#deployment\n",
    "\n",
    "### üí° Remember:\n",
    "\n",
    "> *\"The best MLflow setup is the one your team actually uses consistently.\"*\n",
    "\n",
    "Start simple, iterate, and gradually add complexity as your ML operations mature. Focus on creating reproducible experiments and maintainable ML workflows.\n",
    "\n",
    "**Happy experimenting! üß™‚ú®**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
